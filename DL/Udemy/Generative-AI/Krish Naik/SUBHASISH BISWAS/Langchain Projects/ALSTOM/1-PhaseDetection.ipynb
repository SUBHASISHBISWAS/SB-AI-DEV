{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-16T18:04:17.662254Z",
     "start_time": "2024-09-16T18:04:05.757230Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import  AzureChatOpenAI, OpenAIEmbeddings,AzureOpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import  RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from  langchain_community.embeddings import OllamaEmbeddings\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_TYPE']=os.getenv(\"AL_OPENAI_API_TYPE\")\n",
    "os.environ['OPENAI_API_VERSION']=os.getenv(\"AL_OPENAI_API_VERSION\")\n",
    "os.environ['AZURE_OPENAI_ENDPOINT']=os.getenv(\"AL_AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"AL_OPENAI_API_KEY\")\n",
    "os.environ['DEPLOYMENT_NAME']=os.getenv(\"AL_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"AL_LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:04:28.300599Z",
     "start_time": "2024-09-16T18:04:28.204249Z"
    }
   },
   "cell_type": "code",
   "source": "llm=AzureChatOpenAI()",
   "id": "dcdc5069d90c20ae",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:04:32.718024Z",
     "start_time": "2024-09-16T18:04:32.146125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract the text from the PDF file.\n",
    "\n",
    "pdfReader = PdfReader('./Requirements/SET MSA Schedule 23_updated.pdf')\n",
    "\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfReader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "        \n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ],
   "id": "c977f0c16951e1f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:04.096485Z",
     "start_time": "2024-09-16T18:04:38.619128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vector_store_db=FAISS.from_texts(texts,embeddings)\n",
    "retriever=vector_store_db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":1})\n",
    "vector_store_db.save_local(\"faiss_index\")"
   ],
   "id": "a09594aa8dd1bc57",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:09.466114Z",
     "start_time": "2024-09-16T18:05:09.461372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vector_store_db_from_local=FAISS.load_local('faiss_index',embeddings,allow_dangerous_deserialization=True)\n",
    "retriever=vector_store_db_from_local.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":1})"
   ],
   "id": "32557217627f801f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T12:17:09.960681Z",
     "start_time": "2024-09-13T12:17:07.285502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "embeddings=AzureOpenAIEmbeddings()\n",
    "\n",
    "loader=PyPDFDirectoryLoader(\"Requirements\")\n",
    "docs=loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunksDocuments=text_splitter.split_documents(docs)\n",
    "##result = [chunksDocument.dict()['page_content'] for chunksDocument in chunksDocuments]\n",
    "vector_store_db=FAISS.from_documents(chunksDocuments,embeddings)\n",
    "##retriever=vector_store_db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":1})\n",
    "##retriever_tool=create_retriever_tool(retriever,\"PhaseFinder\",\"Search phases in the document\")"
   ],
   "id": "e615f299ed8bbafe",
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Unsupported data type",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m chunksDocuments\u001B[38;5;241m=\u001B[39mtext_splitter\u001B[38;5;241m.\u001B[39msplit_documents(docs)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m##result = [chunksDocument.dict()['page_content'] for chunksDocument in chunksDocuments]\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m vector_store_db\u001B[38;5;241m=\u001B[39mFAISS\u001B[38;5;241m.\u001B[39mfrom_documents(chunksDocuments,embeddings)\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:835\u001B[0m, in \u001B[0;36mVectorStore.from_documents\u001B[1;34m(cls, documents, embedding, **kwargs)\u001B[0m\n\u001B[0;32m    833\u001B[0m texts \u001B[38;5;241m=\u001B[39m [d\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[0;32m    834\u001B[0m metadatas \u001B[38;5;241m=\u001B[39m [d\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m--> 835\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_texts(texts, embedding, metadatas\u001B[38;5;241m=\u001B[39mmetadatas, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1041\u001B[0m, in \u001B[0;36mFAISS.from_texts\u001B[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001B[0m\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_texts\u001B[39m(\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1021\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m   1022\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FAISS:\n\u001B[0;32m   1023\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001B[39;00m\n\u001B[0;32m   1024\u001B[0m \n\u001B[0;32m   1025\u001B[0m \u001B[38;5;124;03m    This is a user friendly interface that:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1041\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m embedding\u001B[38;5;241m.\u001B[39membed_documents(texts)\n\u001B[0;32m   1042\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__from(\n\u001B[0;32m   1043\u001B[0m         texts,\n\u001B[0;32m   1044\u001B[0m         embeddings,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1048\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1049\u001B[0m     )\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:616\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[1;34m(self, texts, chunk_size)\u001B[0m\n\u001B[0;32m    613\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[0;32m    614\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[0;32m    615\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[1;32m--> 616\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_len_safe_embeddings(texts, engine\u001B[38;5;241m=\u001B[39mengine)\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:514\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[1;34m(self, texts, engine, chunk_size)\u001B[0m\n\u001B[0;32m    512\u001B[0m batched_embeddings: List[List[\u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    513\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[1;32m--> 514\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m    515\u001B[0m         \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtokens[i : i \u001B[38;5;241m+\u001B[39m _chunk_size], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invocation_params\n\u001B[0;32m    516\u001B[0m     )\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    518\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:125\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    119\u001B[0m         embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[0;32m    120\u001B[0m             base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    121\u001B[0m         )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[1;32m--> 125\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    127\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(params, embedding_create_params\u001B[38;5;241m.\u001B[39mEmbeddingCreateParams),\n\u001B[0;32m    128\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    129\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers,\n\u001B[0;32m    130\u001B[0m         extra_query\u001B[38;5;241m=\u001B[39mextra_query,\n\u001B[0;32m    131\u001B[0m         extra_body\u001B[38;5;241m=\u001B[39mextra_body,\n\u001B[0;32m    132\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    133\u001B[0m         post_parser\u001B[38;5;241m=\u001B[39mparser,\n\u001B[0;32m    134\u001B[0m     ),\n\u001B[0;32m    135\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mCreateEmbeddingResponse,\n\u001B[0;32m    136\u001B[0m )\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\openai\\_base_client.py:1260\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1248\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1255\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1256\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1257\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1258\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1259\u001B[0m     )\n\u001B[1;32m-> 1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\openai\\_base_client.py:937\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    929\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    930\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    935\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    936\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 937\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m    938\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    939\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    940\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    941\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    942\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[0;32m    943\u001B[0m     )\n",
      "File \u001B[1;32mC:\\GIT\\SB-AI-DEV\\DL\\Udemy\\Generative-AI\\Krish Naik\\SUBHASISH BISWAS\\Langchain Projects\\venv\\Lib\\site-packages\\openai\\_base_client.py:1041\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1038\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1040\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1044\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1045\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1049\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[0;32m   1050\u001B[0m )\n",
      "\u001B[1;31mBadRequestError\u001B[0m: Unsupported data type"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T11:38:05.828279Z",
     "start_time": "2024-09-13T11:38:05.823774Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7eb963c279da1f8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:13.125256Z",
     "start_time": "2024-09-16T18:05:13.120124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "d0b5b4856bd62682",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:15.329354Z",
     "start_time": "2024-09-16T18:05:15.324780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ],
   "id": "d32ea8ece0e314b1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:16.888396Z",
     "start_time": "2024-09-16T18:05:16.878395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if  session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "        \n",
    "with_message_history=RunnableWithMessageHistory(llm,get_session_history)"
   ],
   "id": "17832426a728e32c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:18.456514Z",
     "start_time": "2024-09-16T18:05:18.452858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "id": "e73060bb4226037b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:23.229513Z",
     "start_time": "2024-09-16T18:05:20.927813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
    "chat_history=[]\n",
    "\n",
    "\n",
    "question=\"tell about Security Policy in this document \"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={ \"configurable\": {\"session_id\": \"session-1\"} },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['answer'])\n"
   ],
   "id": "291c5cf630de3ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Security Policy in this document requires that all IT and OT Systems, including associated systems, hardware, and firmware, adhere to its requirements. It also mandates limiting access to network and information systems to authorized personnel who need access for their job duties, with regular reviews of access rights. Additionally, all personnel with access to these systems must be under a contractual duty of confidence.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:05:27.358717Z",
     "start_time": "2024-09-16T18:05:25.219072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question=\"The Manufacturer shall:\"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\"configurable\": {\"session_id\": \"session-1\"}},\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(response['answer'])"
   ],
   "id": "8fe236713b0f211c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Manufacturer shall have sole responsibility for the maintenance of the IT and OT Systems, ensuring timely updates, patches, and upgrades to maintain service availability, integrity, and protection of networks, communications systems, and data. The Manufacturer must comply with legal and regulatory requirements, best industry practices, the latest technological developments, threat intelligence, specific sections of the Functional Specification, and any reasonable security guidelines or instructions provided by the Operator.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:58:31.698205Z",
     "start_time": "2024-09-13T10:58:31.691989Z"
    }
   },
   "cell_type": "code",
   "source": "chat_history.clear()",
   "id": "c3eacdd9a4edbb8e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:58:31.813127Z",
     "start_time": "2024-09-13T10:58:31.809709Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a0ae6d49ab564bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
