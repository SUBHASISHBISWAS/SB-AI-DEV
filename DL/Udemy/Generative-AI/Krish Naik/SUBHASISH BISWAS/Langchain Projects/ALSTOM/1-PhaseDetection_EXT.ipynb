{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:03.642800Z",
     "start_time": "2024-09-20T12:22:03.629230Z"
    }
   },
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import  AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever\n",
    "from  langchain_community.embeddings import OllamaEmbeddings\n",
    "import hashlib\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_TYPE']=os.getenv(\"AL_OPENAI_API_TYPE\")\n",
    "os.environ['OPENAI_API_VERSION']=os.getenv(\"AL_OPENAI_API_VERSION\")\n",
    "os.environ['AZURE_OPENAI_ENDPOINT']=os.getenv(\"AL_AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"AL_OPENAI_API_KEY\")\n",
    "os.environ['DEPLOYMENT_NAME']=os.getenv(\"AL_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"AL_LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T13:02:13.235492Z",
     "start_time": "2024-09-20T13:02:13.193891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_data(sentences, tokenizer, model):\n",
    "    try:\n",
    "       import torch\n",
    "       import torch.nn.functional as F\n",
    "       \n",
    "       def mean_pooling(model_output, attention_mask):\n",
    "           token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "           input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "           return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1),min=1e-9)\n",
    "\n",
    "       encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "       with torch.no_grad():\n",
    "           model_output = model(**encoded_input)\n",
    "       sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "       sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "       return torch.squeeze(sentence_embeddings) .numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print (sentences)"
   ],
   "id": "df0eb9d71ac1cebc",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:04.736294Z",
     "start_time": "2024-09-20T12:22:04.636670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm=AzureChatOpenAI()\n",
    "# Initialize the embedding model\n",
    "embedding=OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ],
   "id": "dcdc5069d90c20ae",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:07.003827Z",
     "start_time": "2024-09-20T12:22:06.999606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to split text into manageable chunks using a Recursive Text Splitter\n",
    "def split_document_into_chunks(document, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.create_documents([document.page_content])\n",
    "    return chunks\n"
   ],
   "id": "461c555818b4cd8e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:07.973638Z",
     "start_time": "2024-09-20T12:22:07.970486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to generate a unique document ID based on the hash of the content\n",
    "def generate_doc_id(content):\n",
    "    normalized_content = content.strip().lower()\n",
    "    return hashlib.sha256(normalized_content.encode('utf-8')).hexdigest()"
   ],
   "id": "b4a234b9f72ffbe2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:09.165896Z",
     "start_time": "2024-09-20T12:22:09.161541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load the FAISS index from disk if it exists\n",
    "def load_faiss_index(index_path, embedding):\n",
    "    index_file = os.path.join(index_path, \"index.faiss\")\n",
    "    pkl_file = os.path.join(index_path, \"index.pkl\")\n",
    "    \n",
    "    if os.path.exists(index_file) and os.path.exists(pkl_file):\n",
    "        print(f\"Loading FAISS index and metadata from {index_path}\")\n",
    "        return FAISS.load_local(index_path, embedding,allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        print(f\"No FAISS index found at {index_path}, creating a new one.\")\n",
    "        return None\n"
   ],
   "id": "10cdaf63414c2565",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:10.456781Z",
     "start_time": "2024-09-20T12:22:10.453384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to save the FAISS index to disk\n",
    "def save_faiss_index(vector_store, index_path):\n",
    "    os.makedirs(index_path, exist_ok=True)\n",
    "    vector_store.save_local(index_path)\n",
    "    print(f\"FAISS index and metadata saved to {index_path}\")"
   ],
   "id": "a2a24ec77e4bcc20",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:55.699394Z",
     "start_time": "2024-09-20T12:22:55.691963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to add PDF document to FAISS store with consistent doc_id generation\n",
    "def add_pdf_to_faiss(pdf_path, vector_store=None, index_path=\"faiss_index\"):\n",
    "    if vector_store is None:\n",
    "        vector_store = load_faiss_index(index_path, embedding)\n",
    "\n",
    "    # Load the PDF document\n",
    "    pdf_loader = PyPDFLoader(pdf_path)\n",
    "    documents = pdf_loader.load()\n",
    "    \n",
    "    new_documents = []\n",
    "    new_embeddings = []\n",
    "\n",
    "    # Retrieve existing document IDs from FAISS and normalize them\n",
    "    existing_ids = set()\n",
    "    if vector_store is not None:\n",
    "        existing_ids = set(\n",
    "            generate_doc_id(doc.page_content)  # Re-generate hash-based doc_ids for existing documents\n",
    "            for doc_id, doc in vector_store.docstore._dict.items()\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Iterate through the documents, chunk them, and check if they are already embedded\n",
    "    for document in documents:\n",
    "        chunks = split_document_into_chunks(document)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            doc_id = generate_doc_id(chunk.page_content)\n",
    "            \n",
    "\n",
    "            if doc_id not in existing_ids:\n",
    "                new_documents.append(Document(page_content=chunk.page_content, metadata={\"id\": doc_id}))\n",
    "                new_embeddings.append(embedding.embed_documents([chunk.page_content])[0])\n",
    "                print(f\"Embedding new document chunk with doc_id: {doc_id}\")\n",
    "            else:\n",
    "                print(f\"Document chunk {doc_id} already exists in FAISS, skipping.\")\n",
    "\n",
    "    if new_documents:\n",
    "        if vector_store is None:\n",
    "            vector_store = FAISS.from_documents(new_documents, embedding)\n",
    "            print(f\"Created new FAISS index for {pdf_path}.\")\n",
    "        else:\n",
    "            for i, document in enumerate(new_documents):\n",
    "                vector_store.add_documents([document], embeddings=[new_embeddings[i]])\n",
    "                # Map the generated doc_id to the FAISS index's internal mapping\n",
    "                vector_store.index_to_docstore_id[vector_store.index.ntotal - 1] = document.metadata[\"id\"]\n",
    "            print(f\"Added {len(new_documents)} new chunks to FAISS index.\")\n",
    "    else:\n",
    "        print(\"No new chunks to add to FAISS.\")\n",
    "\n",
    "    save_faiss_index(vector_store, index_path)\n",
    "\n",
    "    return vector_store"
   ],
   "id": "22da39c48b606b1a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:56.177968Z",
     "start_time": "2024-09-20T12:22:56.174811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage with a PDF file\n",
    "pdf_path = \"./Requirements/SET MSA Schedule 23_updated.pdf\"  # Path to the PDF file\n",
    "# Specify where the FAISS index should be stored\n",
    "index_path = \"faiss_index\""
   ],
   "id": "ced7c1cd3327d1c4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:22:57.439986Z",
     "start_time": "2024-09-20T12:22:56.836498Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store = add_pdf_to_faiss(pdf_path, vector_store=None, index_path=index_path)",
   "id": "e0157dc7522b4c5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index and metadata from faiss_index\n",
      "Document chunk 9210a3f803bbce8a9de86aa23fe0259034da5f3295b67c1e43df146319ba73f9 already exists in FAISS, skipping.\n",
      "Document chunk 9fea60f03bbd1cd49949fa0d19a0ab599eb240433500364a13139ef90b99f854 already exists in FAISS, skipping.\n",
      "Document chunk b0f8b8bd3f42e49cb309d763d97b9b543e50bcb7638883b2d57ec07cff5c1c4c already exists in FAISS, skipping.\n",
      "Document chunk 94c4d33bff81a106758ebf7b7581bb9f431a3d74e6ae8dff635dc158e7d12edc already exists in FAISS, skipping.\n",
      "Document chunk 81119c46edc025a2e334e347defc2dee98eb3a607db6569e47bb310dfbe7ebd3 already exists in FAISS, skipping.\n",
      "Document chunk 917f707ba9989c6ad59f6cc5f39107f1241ae22ae0db117af1aeb6e276c779f4 already exists in FAISS, skipping.\n",
      "Document chunk 6e84932888188d8d5fa39a1d237f364c250b3cb9059a1af6fa06fd146a8494cb already exists in FAISS, skipping.\n",
      "Document chunk 7d6aa675be3a1f273d3dbd1eaedca26658d23ffa584d078ea4a7039809563487 already exists in FAISS, skipping.\n",
      "Document chunk 19892e3d9a30d21299914d13d8a0b19d4c5e538814bb722076e6e0a5f4825807 already exists in FAISS, skipping.\n",
      "Document chunk 45c6568ab9e60709a6028129da9367b3200c4a82fc7ba23fe05ed61088b224d8 already exists in FAISS, skipping.\n",
      "Document chunk ef7eee2afa8b21a162c5ca6e40b876474c1ea759928cd797e9979802848e2dea already exists in FAISS, skipping.\n",
      "Document chunk cda2b63e3b57303b6b11fcf95ca92d04623e774903bedf99a29441daf1029077 already exists in FAISS, skipping.\n",
      "Document chunk 334e98aa7f5dc16d04f9da8403f7504c0972345dda438e823c01262f74497ae1 already exists in FAISS, skipping.\n",
      "Document chunk e01fd51711ea015b28cae131207a3a95756a581f4fdb4bb1318cd19c36f31b8b already exists in FAISS, skipping.\n",
      "Document chunk bf3d189551b4acc26e49110a6b39a1ead93915c5c6b82b122ed8a9328bc3cac2 already exists in FAISS, skipping.\n",
      "Document chunk 8aa4c7be92e9c618bee553059b882fb89b04b4a95f242dc68f39c1d632b67627 already exists in FAISS, skipping.\n",
      "Document chunk 4c84e15d7bc4776f05a0e59d3e4be45a3f183dccf5f0bf154f9aee8d7335b052 already exists in FAISS, skipping.\n",
      "Document chunk ef9376d1862af1debba0a85ce78967753a8f73e55ce45b302b17b24f5d733e88 already exists in FAISS, skipping.\n",
      "Document chunk cf743952ebec89ca7264bd2cff10ab347541757bbdbaf7f362ece7df842d8755 already exists in FAISS, skipping.\n",
      "Document chunk 0040624fadaeb3f676bd565fb57180c44a0ec12fed8353073a5d97a3ecec2b73 already exists in FAISS, skipping.\n",
      "Document chunk 4659b8f2c8affc17b161f9a8383793ea213623c44e4715ce8ab63efb7c8118a1 already exists in FAISS, skipping.\n",
      "Document chunk dbabbe0ea71a0863d0311affb014e5c23c9cd9346f843346acb3ceeb8f2047a3 already exists in FAISS, skipping.\n",
      "Document chunk f1d591018291ce082fd95c03bfe33b733c4a7de628e08f20ef7d8c6886991a2c already exists in FAISS, skipping.\n",
      "Document chunk f1845ed43d84518a521d6b07284c12a2ff2444223c410df1039ab674e811ec62 already exists in FAISS, skipping.\n",
      "Document chunk 49c228859bd6dc75c91f512a00bbf6f83883517f33503dcede5e6ac4cfb5db4e already exists in FAISS, skipping.\n",
      "Document chunk 693cd3a789b106c54aa1651660d18315ce7c3fd4d809f76df6859694875c168d already exists in FAISS, skipping.\n",
      "Document chunk d865b73f18dbd44e00127cc8e3280daabb3dc45fef425276cc7f095a79505d67 already exists in FAISS, skipping.\n",
      "Document chunk 3408949c7cba2afc6cc19d6359ba67e906b76c86006ea0f5fb1f062b0c490077 already exists in FAISS, skipping.\n",
      "Document chunk acf30d9b29eab1d2ad7df90ffac20098a45a58a41aaf76a1f8cc74b95e8ad7fa already exists in FAISS, skipping.\n",
      "No new chunks to add to FAISS.\n",
      "FAISS index and metadata saved to faiss_index\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:26:58.624085Z",
     "start_time": "2024-09-20T12:26:58.618872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})"
   ],
   "id": "32557217627f801f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:26:59.324866Z",
     "start_time": "2024-09-20T12:26:59.321479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "# embeddings=AzureOpenAIEmbeddings()\n",
    "# \n",
    "# loader=PyPDFDirectoryLoader(\"Requirements\")\n",
    "# docs=loader.load()\n",
    "# text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "# chunksDocuments=text_splitter.split_documents(docs)\n",
    "# ##result = [chunksDocument.dict()['page_content'] for chunksDocument in chunksDocuments]\n",
    "# vector_store_db=FAISS.from_documents(chunksDocuments,embeddings)\n",
    "# ##retriever=vector_store_db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":1})\n",
    "# ##retriever_tool=create_retriever_tool(retriever,\"PhaseFinder\",\"Search phases in the document\")"
   ],
   "id": "e615f299ed8bbafe",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:00.213028Z",
     "start_time": "2024-09-20T12:27:00.210482Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7eb963c279da1f8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:00.780177Z",
     "start_time": "2024-09-20T12:27:00.775174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "d0b5b4856bd62682",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:01.425655Z",
     "start_time": "2024-09-20T12:27:01.420991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ],
   "id": "d32ea8ece0e314b1",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:01.773466Z",
     "start_time": "2024-09-20T12:27:01.767656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if  session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "        \n",
    "with_message_history=RunnableWithMessageHistory(llm,get_session_history)"
   ],
   "id": "17832426a728e32c",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:02.011429Z",
     "start_time": "2024-09-20T12:27:02.007396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "id": "e73060bb4226037b",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:05.538003Z",
     "start_time": "2024-09-20T12:27:03.374759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
    "chat_history=[]\n",
    "\n",
    "\n",
    "question=\"tell about Security Policy in this document \"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={ \"configurable\": {\"session_id\": \"session-1\"} },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['answer'])\n"
   ],
   "id": "291c5cf630de3ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Security Policy in this document requires the Manufacturer to ensure that all IT Systems, OT Systems, and associated systems, hardware, and firmware comply with its requirements. The Manufacturer must limit access to these systems to authorized personnel who need access solely for performing obligations under the MSA, with access rights reviewed regularly and personnel appropriately screened in accordance with best industry practice. Additionally, the Manufacturer must comply with the Security Policy and any other cyber security policies and procedures notified by the Operator.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T12:27:43.532248Z",
     "start_time": "2024-09-20T12:27:39.475636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question=\"The Manufacturer shall:\"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\"configurable\": {\"session_id\": \"session-1\"}},\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(response['answer'])"
   ],
   "id": "8fe236713b0f211c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Manufacturer shall:\n",
      "\n",
      "1. Comply with legal and regulatory requirements, best industry practice, latest technological developments, threat intelligence, sections [A35 to A40] of the Functional Specification, security requirements set out in this Schedule, and any reasonable security guidelines or instructions provided by the Operator.\n",
      "2. Continually measure, review, provide evidence of, and document compliance with all security requirements, reporting such compliance to the Operator upon request.\n",
      "3. Allow the Operator and/or the Owner to carry out necessary audits or inspections.\n",
      "4. Provide full details of any Security Incident to the Operator within five working days of its resolution, including root cause analysis.\n",
      "5. Assist the Operator with information provision in connection with incident notifications to authorities and contribute to any subsequent investigations or inspections.\n",
      "6. Certify annually that it has complied with the requirements of this Schedule. \n",
      "7. Take all reasonable steps to minimize the extent of harm caused by any Security Incident and comply with any relevant instructions.\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:58:31.698205Z",
     "start_time": "2024-09-13T10:58:31.691989Z"
    }
   },
   "cell_type": "code",
   "source": "chat_history.clear()",
   "id": "c3eacdd9a4edbb8e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T10:58:31.813127Z",
     "start_time": "2024-09-13T10:58:31.809709Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a0ae6d49ab564bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
