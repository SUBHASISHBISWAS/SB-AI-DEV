{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-25T06:55:32.872852Z",
     "start_time": "2024-09-25T06:55:32.862404Z"
    }
   },
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import  AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever\n",
    "from  langchain_community.embeddings import OllamaEmbeddings,HuggingFaceEmbeddings\n",
    "import hashlib\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_TYPE']=os.getenv(\"AL_OPENAI_API_TYPE\")\n",
    "os.environ['OPENAI_API_VERSION']=os.getenv(\"AL_OPENAI_API_VERSION\")\n",
    "os.environ['AZURE_OPENAI_ENDPOINT']=os.getenv(\"AL_AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"AL_OPENAI_API_KEY\")\n",
    "os.environ['DEPLOYMENT_NAME']=os.getenv(\"AL_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"AL_LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:57:34.828951Z",
     "start_time": "2024-09-25T06:57:34.818492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class CustomLangChainEmbedding(Embeddings):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", use_gpu=False):\n",
    "        \"\"\"\n",
    "        Initialize the embedding class with a specific transformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained transformer model.\n",
    "            use_gpu (bool): If True, use GPU (CUDA) for inference; otherwise, use CPU.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,clean_up_tokenization_spaces=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Use GPU if available and requested\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to compute sentence embeddings from token embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element is token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    \n",
    "        \n",
    "    def encode_data(self, sentences):\n",
    "        \"\"\"\n",
    "        Encode the input sentences into sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentences (list of str): List of sentences to encode.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Sentence embeddings as a numpy array.\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "                                         \n",
    "        \n",
    "        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "        print(sentence_embeddings.shape)\n",
    "        return torch.squeeze(sentence_embeddings).numpy() # Convert to numpy for FAISS or other downstream tasks\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        LangChain-compatible method to create embeddings for documents.\n",
    "        \n",
    "        Args:\n",
    "            texts (list of str): List of documents (text) to create embeddings for.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Document embeddings as numpy arrays.\n",
    "        \"\"\"\n",
    "        return self.encode_data(texts)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        LangChain-compatible method to create embedding for a single query.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Query to create embedding for.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Query embedding as a numpy array.\n",
    "        \"\"\"\n",
    "        return self.encode_data(text)\n",
    "\n"
   ],
   "id": "bf1505461f961cb4",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:57:35.281600Z",
     "start_time": "2024-09-25T06:57:35.261772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import hashlib\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class FaissIndexManager:\n",
    "    def __init__(self, embedding, index_path=\"faiss_index\"):\n",
    "        self.embedding = embedding\n",
    "        self.index_path = index_path\n",
    "        self.vector_store = self.load_faiss_index()\n",
    "    \n",
    "    # Function to save the FAISS index to disk\n",
    "    def save_faiss_index(self):\n",
    "        os.makedirs(self.index_path, exist_ok=True)\n",
    "        self.vector_store.save_local(self.index_path)\n",
    "        print(f\"FAISS index and metadata saved to {self.index_path}\")\n",
    "    \n",
    "    # Function to load FAISS index from disk\n",
    "    def load_faiss_index(self):\n",
    "        index_file = os.path.join(self.index_path, \"index.faiss\")\n",
    "        pkl_file = os.path.join(self.index_path, \"index.pkl\")\n",
    "        \n",
    "        if os.path.exists(index_file) and os.path.exists(pkl_file):\n",
    "            print(f\"Loading FAISS index and metadata from {self.index_path}\")\n",
    "            return FAISS.load_local(self.index_path, self.embedding, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(f\"No FAISS index found at {self.index_path}, creating a new one.\")\n",
    "            return None\n",
    "    \n",
    "    # Function to split a document into chunks\n",
    "    @staticmethod\n",
    "    def split_document_into_chunks(document, chunk_size=1000, chunk_overlap=200):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.create_documents([document.page_content])\n",
    "        return chunks\n",
    "    \n",
    "    # Function to generate a consistent document ID using a hash\n",
    "    @staticmethod\n",
    "    def generate_doc_id(content):\n",
    "        normalized_content = content.strip().lower()\n",
    "        return hashlib.sha256(normalized_content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Function to add a PDF document to the FAISS store\n",
    "    def add_pdf_to_faiss(self, pdf_path):\n",
    "        if self.vector_store is None:\n",
    "            # Load or create a new FAISS index\n",
    "            self.vector_store = self.load_faiss_index()\n",
    "\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        documents = pdf_loader.load()\n",
    "\n",
    "        new_documents = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Check for existing documents in vector store\n",
    "        existing_ids = set(\n",
    "            self.generate_doc_id(doc.page_content)\n",
    "            for doc_id, doc in self.vector_store.docstore._dict.items()\n",
    "        ) if self.vector_store is not None else set()\n",
    "\n",
    "        for document in documents:\n",
    "            chunks = self.split_document_into_chunks(document)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                doc_id = self.generate_doc_id(chunk.page_content)\n",
    "                if doc_id not in existing_ids:\n",
    "                    new_embedding = self.embedding.encode_data(chunk.page_content).reshape(1, -1)\n",
    "                    new_documents.append(Document(page_content=chunk.page_content, metadata={\"id\": doc_id}))\n",
    "                    print(f\"Embedding new document chunk with doc_id: {doc_id}\")\n",
    "                    embeddings_list.append(new_embedding)\n",
    "\n",
    "        # Debugging information\n",
    "        print(f\"Total new documents: {len(new_documents)}\")\n",
    "        print(f\"Total embeddings created: {len(embeddings_list)}\")\n",
    "\n",
    "        if new_documents:\n",
    "            if self.vector_store is None:\n",
    "                # Initialize FAISS index manually, passing in precomputed embeddings\n",
    "                self.vector_store = FAISS.from_documents(new_documents, self.embedding)\n",
    "                print(f\"Created new FAISS index for {pdf_path}.\")\n",
    "            else:\n",
    "                # Add the new documents and embeddings to the existing FAISS index\n",
    "                self.vector_store.add_documents(new_documents, embeddings=embeddings_list)\n",
    "                for idx, doc in enumerate(new_documents):\n",
    "                    self.vector_store.index_to_docstore_id[self.vector_store.index.ntotal - len(new_documents) + idx] = doc.metadata[\"id\"]\n",
    "                print(f\"Added {len(new_documents)} new chunks to FAISS index.\")\n",
    "        else:\n",
    "            print(\"No new chunks to add to FAISS.\")\n",
    "\n",
    "        # Save the updated FAISS index\n",
    "        self.save_faiss_index()\n",
    "        return self.vector_store\n",
    "    \n",
    "    # Function to inspect the FAISS store\n",
    "    def inspect_faiss_store(self):\n",
    "        if self.vector_store is None:\n",
    "            print(\"FAISS store is empty or not loaded.\")\n",
    "            return\n",
    "        \n",
    "        # Check number of vectors stored\n",
    "        num_vectors = self.vector_store.index.ntotal\n",
    "        print(f\"Number of vectors stored: {num_vectors}\")\n",
    "        \n",
    "        # Check stored documents and metadata\n",
    "        print(\"Stored documents:\")\n",
    "        for doc_id, document in self.vector_store.docstore._dict.items():\n",
    "            print(f\"Document ID: {doc_id}\")\n",
    "            print(f\"Content: {document.page_content[:200]}\")  # Print first 200 characters of content\n",
    "            print(f\"Metadata: {document.metadata}\")\n",
    "        \n",
    "        # Retrieve and check stored embeddings\n",
    "        if num_vectors > 0:\n",
    "            for i in range(min(5, num_vectors)):  # Print embeddings of first 5 documents\n",
    "                vector = self.vector_store.index.reconstruct(i)\n",
    "                print(f\"Vector Shape: {vector.shape}...\")\n",
    "                print(f\"Embedding {i}: {vector[:10]}...\")  # Print first 10 dimensions of the embedding\n",
    "        else:\n",
    "            print(\"No embeddings stored.\")\n"
   ],
   "id": "61ffea0f2b6debc8",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:57:35.857670Z",
     "start_time": "2024-09-25T06:57:35.724459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the embedding model\n",
    "embedding = CustomLangChainEmbedding(model_name=\"./Models/all-MiniLM-L6-v2\", use_gpu=False)\n",
    "#embedding=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "llm=AzureChatOpenAI()\n",
    "faiss_manager = FaissIndexManager(embedding)"
   ],
   "id": "dbd2388bcff0a9b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Loading FAISS index and metadata from faiss_index\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:57:37.507633Z",
     "start_time": "2024-09-25T06:57:36.987470Z"
    }
   },
   "cell_type": "code",
   "source": "vector_store=faiss_manager.add_pdf_to_faiss(\"./Requirements/SET MSA Schedule 23_updated.pdf\")",
   "id": "dcdc5069d90c20ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new documents: 0\n",
      "Total embeddings created: 0\n",
      "No new chunks to add to FAISS.\n",
      "FAISS index and metadata saved to faiss_index\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:57:38.565253Z",
     "start_time": "2024-09-25T06:57:38.561952Z"
    }
   },
   "cell_type": "code",
   "source": "retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})",
   "id": "34496ec908bb526f",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:57:40.456352Z",
     "start_time": "2024-09-25T06:57:40.431651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "retriever.invoke(\"Security Policy\")"
   ],
   "id": "568d79d4e1580db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '94c4d33bff81a106758ebf7b7581bb9f431a3d74e6ae8dff635dc158e7d12edc'}, page_content=\"(b) the occurrence of one or more events which, either \\nindividually or collectively, have an adverse effect on the \\nconfidentiality, integrity, availability or security of the railway, \\nany data and/or the IT Systems and/or the OT Systems; \\nSecurity Policy means the Operator's IT/Information Security Policy as may be \\nnotified to the Manufacturer from time to time; and \\nSecurity, Continuity & has the meaning given in paragraph 3.1 and as updated from time\"),\n",
       " Document(metadata={'id': '6e84932888188d8d5fa39a1d237f364c250b3cb9059a1af6fa06fd146a8494cb'}, page_content='Operator, and shall comply with the following in respect thereof:  \\n2.3.1 legal and regulatory requirements;  \\n2.3.2 best industry practice; \\n2.3.3 latest technological developments;  \\n2.3.4 threat intelligence (e.g. from National Cyber Security Centre alerts); \\n2.3.5 sections [A35 to A40]53 of the Functional Specification (on an ongoing basis); \\n2.3.6 the security requirements set out in this Schedule; and  \\n2.3.7 any reasonable security guidelines and/or instructions provided by the Operator to the \\nManufacturer from time to time. \\n2.4 The Manufacturer shall continually measure, review, provide evidence of and document its \\ncompliance with all security requirements (including details of the version of anti-virus software \\nand cyber security measures) as set out in this Schedule, and shall report such compliance to the \\nOperator on request. The Manufacturer shall allow the Operator and/or the Owner to carry out'),\n",
       " Document(metadata={'id': 'f1845ed43d84518a521d6b07284c12a2ff2444223c410df1039ab674e811ec62'}, page_content='(i) where such Security Incident relates to Personal Data, the categories and \\napproximate number of data subjects and Personal Data records affected or \\nconcerned; and/or \\n(ii) where such Security Incident relates to critical infrastructure and requires \\nreporting under the NIS Regulations, the types of digital services provided, \\nthe approximate time and duration of the Security Incident, the nature of the \\nincident and any cross-border impact; \\n(b) a description of the likely consequences of the Security Incident;   \\n(c) a description of the measures taken or proposed to be taken by the Manufacturer \\nto address the Security Incident, including measures to mitigate its possible \\nadverse effects; and \\n(d) any other information the Operator reasonably requests;  \\n4.1.2 immediately take all reasonable steps necessary to minimise the extent of actual or \\npotential harm caused by any Security Incident and, in doing so, comply with any'),\n",
       " Document(metadata={'id': 'bf3d189551b4acc26e49110a6b39a1ead93915c5c6b82b122ed8a9328bc3cac2'}, page_content='respect of OT Systems) and conducts its activities in compliance with the principles of \\napplicable international standards and the requirements of all Applicable Laws \\n(including Information Laws); \\n2.7.9 comply with the Security Policy and ensure that all IT Systems and OT Systems and all \\nconnecting or associated systems, hardware and firmware adhere to the requirements \\nof the Security Policy; \\n2.7.10 limit access to network and information systems to such of its authorised personnel \\nthat require access to such systems solely for the purpose of performing its obligations \\npursuant to this MSA (with access rights being reviewed on a regular basis) and that \\nhave been appropriately screened in accordance with best industry practice (provided \\nalways that such action would not put the Manufacturer in breach of Applicable Laws), \\nthe Security Policy and any other Operator standards (as notified to the Manufacturer \\nfrom time to time);'),\n",
       " Document(metadata={'id': '49c228859bd6dc75c91f512a00bbf6f83883517f33503dcede5e6ac4cfb5db4e'}, page_content=\"4.1.2 immediately take all reasonable steps necessary to minimise the extent of actual or \\npotential harm caused by any Security Incident and, in doing so, comply with any \\ninstructions issued by the Operator (acting reasonably);  \\n4.1.3 implement the Security, Continuity & Recovery Plan and continue, in so far as is \\npossible, to provide the affected IT Systems and/or OT Systems to the Operator and \\ntake all action necessary to restore the affected IT Systems and/or OT Systems to \\nnormal as soon as possible and in accordance with the Security, Continuity & \\nRecovery Plan; \\n4.1.4 comply with any of the Operator's cyber security policies and procedures (as notified to \\nthe Manufacturer from time to time);\")]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:56:25.520670Z",
     "start_time": "2024-09-25T06:56:25.515128Z"
    }
   },
   "cell_type": "code",
   "source": [
    " \n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "d0b5b4856bd62682",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:56:26.457546Z",
     "start_time": "2024-09-25T06:56:26.453643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ],
   "id": "d32ea8ece0e314b1",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:56:27.122398Z",
     "start_time": "2024-09-25T06:56:27.116213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if  session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "        \n",
    "with_message_history=RunnableWithMessageHistory(llm,get_session_history)"
   ],
   "id": "17832426a728e32c",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:56:27.744382Z",
     "start_time": "2024-09-25T06:56:27.740509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "id": "e73060bb4226037b",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:56:31.439638Z",
     "start_time": "2024-09-25T06:56:28.904599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
    "chat_history=[]\n",
    "\n",
    "\n",
    "question=\"tell about Security Policy in this document \"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={ \"configurable\": {\"session_id\": \"session-1\"} },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['answer'])\n"
   ],
   "id": "291c5cf630de3ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n",
      "The Security Policy refers to the Operator's IT/Information Security Policy, which may be notified to the Manufacturer from time to time. The Manufacturer must comply with this Security Policy and ensure that all IT Systems, OT Systems, and associated systems, hardware, and firmware adhere to its requirements. Compliance includes implementing and maintaining appropriate security measures, limiting access to authorized personnel, and adhering to industry standards and legal requirements.\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:43:58.510565Z",
     "start_time": "2024-09-25T06:43:53.903916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question=\"The Manufacturer shall:\"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\"configurable\": {\"session_id\": \"session-1\"}},\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(response['answer'])"
   ],
   "id": "8fe236713b0f211c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n",
      "The Manufacturer shall:\n",
      "\n",
      "1. Comply with legal and regulatory requirements, best industry practice, latest technological developments, threat intelligence, sections [A35 to A40] of the Functional Specification, the security requirements set out in the Schedule, and any reasonable security guidelines/instructions provided by the Operator.\n",
      "2. Continually measure, review, provide evidence of, and document its compliance with all security requirements, and report such compliance to the Operator on request.\n",
      "3. Ensure that all personnel with access to data, network, information systems, IT Systems, and OT Systems are subject to a contractual duty of confidence and comply with the Manufacturer's obligations.\n",
      "4. Comply with the Rail Cyber Security Guidance to Industry and ensure network segmentation between safety integrity levels (SILs).\n",
      "5. Comply at all times with the Security, Continuity & Recovery Plan.\n",
      "6. Notify and assist the Operator in the event of any security incidents, including providing details and root cause analysis.\n",
      "7. Certify on an annual basis compliance with the Schedule's requirements.\n",
      "8. Ensure subcontractors adhere to cyber security obligations that provide no less protection than those set out in the Schedule.\n",
      "9. Fully indemnify and hold harmless the Operator against any costs, liabilities, and losses arising from breaches of the Schedule or security incidents caused by the Manufacturer or its subcontractors.\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:43:58.548414Z",
     "start_time": "2024-09-25T06:43:58.542527Z"
    }
   },
   "cell_type": "code",
   "source": "chat_history.clear()",
   "id": "c3eacdd9a4edbb8e",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:43:58.579115Z",
     "start_time": "2024-09-25T06:43:58.576536Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a0ae6d49ab564bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
