{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPw7AI6FI0weK7Hyc0WI/9P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/nlp-and-transformers/blob/main/module4/NLPTransformers_Mod4Demo3_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization with T5\n",
        "\n",
        "Â© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "Author: Axel Sirota"
      ],
      "metadata": {
        "id": "gzf37X_XizQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization is one of those tasks that is quite challenging. However the T5 multi-task model can take it! We will show you how to finetune T5 to your needs, however I highly recommend tuning the ammount of traning data depending on your GPU power avaiable."
      ],
      "metadata": {
        "id": "ivtdRrRwz8JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "mfchytx40OuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate rouge_score transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "C9BTEOu0PerV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fpgYwAtNO2T"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "import sys\n",
        "import keras.backend as K\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import time\n",
        "import nltk\n",
        "\n",
        "TRACE = False\n",
        "PATIENCE = 1\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 32\n",
        "DIVISION_FACTOR = 6\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset and tokenize it"
      ],
      "metadata": {
        "id": "7q-Gv3LX1HtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the dataset and metric used for Summarization"
      ],
      "metadata": {
        "id": "84-5h38J0UfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"xsum\")\n",
        "metric = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "gkzGKLjN-T9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "Ai8pidMJ5wfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can tell we have 200k training datapoints, if you have a less powerful GPU in Colab, just use maybe 5% of that and it should suffice."
      ],
      "metadata": {
        "id": "z2z3xXTW0a0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "6m9PFCN8vCuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can tell, we have a document, a summary and an ID. Let's see if we can make this work!"
      ],
      "metadata": {
        "id": "ep3Dvi6y21TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the T5 model (a small version of it) which is a **multi task model**\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The T5 architecture</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.dropbox.com/s/5mcfpwuahy3yivg/t5_arch.webp?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Ihpsu4Oc20jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it is quite a simple vanilla Encoder-Decoder model, but it was trained on a variety of situations with multiple targets and tasks.\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>T5 training and finetuning</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.dropbox.com/s/r8c64dvgmz6jom2/t5-fnietuning.png?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "MKSJEBl96OTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So in the end we are able to use it simple for all the tasks, as the following:\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>T5 usage</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.dropbox.com/s/4i71mp3axb4x8gy/t5_tasks.png?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "uBByLUUm6Y99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before remember to use the `AutoTokenizer` to download the correct tokenization"
      ],
      "metadata": {
        "id": "mKSqEMrL0lge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "sp2tlU-YvEmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5 is a multitask model, so it needs a prefix to know what we want to do (summary, q&a, grammatical checks, you name it). The prefix for our tasks is `summarize: `"
      ],
      "metadata": {
        "id": "YvyFshDC0rXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
      ],
      "metadata": {
        "id": "Z0wcBZ_wxKI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\""
      ],
      "metadata": {
        "id": "HEJ0CMmy21x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 1024  # This comes from the model card\n",
        "max_target_length = 128  # This comes from the model card\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"summary\"], max_length=max_target_length, truncation=True\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "j4r0tmll267Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the preprocessing just applies the tokenizer to both the document and the summary and sets the summary input_ids as the label"
      ],
      "metadata": {
        "id": "xpnTcTD9v9ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(raw_datasets[\"train\"][:2])"
      ],
      "metadata": {
        "id": "I-sr4E8G2-g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we filter the training set to fit in RAM, here you can adapt it to your needs"
      ],
      "metadata": {
        "id": "p2RayeoswQca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIVISION_FACTOR = 20\n",
        "even_dataset = raw_datasets.filter(lambda example, index: index % DIVISION_FACTOR == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "hmr9JhtM9ndB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = even_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "Om0HyL6Y3BPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the model and Training"
      ],
      "metadata": {
        "id": "XHKTDGY88SND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can download the model"
      ],
      "metadata": {
        "id": "GVDGy1fdwyXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFT5ForConditionalGeneration\n",
        "\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "hhN2DScZ-J9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we could specify a maximum length and pad based on that, we used to do this a lot with RNNs; however that is not optimal. It would be better if we can pad up to the maximum *of each batch*. And that is what a **DataCollator** do"
      ],
      "metadata": {
        "id": "diVzXSF91zfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\")  # By default it returns PyTorch tensors which would fail, better to specify Numpy\n",
        "\n",
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)"
      ],
      "metadata": {
        "id": "DnQwxnac3ciK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "JoRnI8pP3f0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamWeightDecay\n",
        "import tensorflow as tf\n",
        "\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "OmboZwdl3k_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "euJdXSCS3oPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sadly, we cannot freeze some layers of this model in particular since it was uploaded as a simple encoder decoder, so we will just finetune it."
      ],
      "metadata": {
        "id": "GCWryQMH4quH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "P-Gf2FQG3xhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing it out"
      ],
      "metadata": {
        "id": "1WKF0tAl-aOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled'\n",
        "if 't5' in model_checkpoint:\n",
        "    document = \"summarize: \" + document\n",
        "tokenized = tokenizer([document], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)"
      ],
      "metadata": {
        "id": "YRvK3ZEC4B6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document)"
      ],
      "metadata": {
        "id": "XV1ufToG-GWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0]))"
      ],
      "metadata": {
        "id": "0V_cVqel4VPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_lTSlDTF4Xzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}