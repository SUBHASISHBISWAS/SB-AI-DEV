{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"FCz0cvWF_wbA","executionInfo":{"status":"ok","timestamp":1706850721306,"user_tz":360,"elapsed":29730,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.4 openai==1.10.0 langchain-openai\n","!pip install -q -U faiss-cpu tiktoken"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gnISdj8lMqKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706850726844,"user_tz":360,"elapsed":5542,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"caafbbae-7dc9-4a8c-cee9-f6e63bf80077"},"outputs":[{"name":"stdout","output_type":"stream","text":["Open AI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]}],"source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"]},{"cell_type":"markdown","metadata":{"id":"CiCAlO7L-VlD"},"source":["# Retrieval Augmented Generation (RAG)\n","\n","[Meta AI introduced the RAG method](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/), emphasizing its potential for knowledge-intensive tasks.\n","\n","## üîç **1. What is RAG?**\n","\n","- RAG, or Retrieval Augmented Generation, boosts large language models (LLMs) by tapping into external knowledge sources.\n","\n","- Meta AI pioneered RAG to tackle knowledge-heavy tasks efficiently.\n","\n","- Combines info retrieval with text generation, enabling LLMs to access fresh, reliable info.\n","\n","- Ideal for tasks needing accurate, current data.\n","\n","## ü§î **2. Why RAG was developed?**\n","\n","- LLMs excel in mimicking human text but face limitations.\n","\n","- High training/fine-tuning costs.\n","\n","- Knowledge is static, outdated post-training.\n","\n","- \"Hallucinating\" issue: confidently giving wrong info.\n","\n","- RAG overcomes these by merging LLM prowess with real-time data access.\n","\n","## 3. üõ†Ô∏è **3. How RAG Works?**\n","\n","- On receiving a query (like a question), RAG fetches relvant documents/passages from external sources (like Wikipedia).\n","\n","- Blends these retrieved docs with the query to create an enriched context. This is then processed by a text generator (e.g., GPT-3) to generate the final answer.\n","\n","<img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg\">\n","\n","## üåü **4. Key Features of RAG:**\n","\n","- RAG stays current, accessing the latest info, unlike static-knowledge LLMs.\n","\n","- Integrates fresh info without the high cost of retraining the whole LLM.\n","\n","- Sources reliable info, reducing wrong answers or \"hallucinations.\"\n","\n","## üõ† **5. Practical Implementations:**\n","\n","- Answering evolving topic questions.\n","\n","- Useful in domains needing real-time accuracy (e.g., medical, legal).\n","\n","- Boosts chatbots/virtual assistants with factual, updated replies.\n","\n","## üìå **In Summary:**\n","\n","- RAG: Marrying vast LLM knowledge with the latest real-world info.\n","\n","- Ensures models knowledgeable, up-to-date, and accurate.\n","\n","# üõ†Ô∏è **RAG Implementation in LangChain**\n","\n","1. üß† **LLM**: The brain of the system, generating human-like text.\n","\n","2. üåê **Vector Store**: The heart of retrieval - stores text embeddings for quick, efficient access.\n","\n","3. üîç **Vector Store Retriever**: The system's \"search engine,\" finding relevant documents via vector similarities.\n","\n","4. üîÑ **Embedder**: Transforms text into vectors, making it readable for the system.\n","\n","5. üí¨ **Prompt**: Captures the initial user query or statement, kicking off the process.\n","\n","6. üìö **Document Loader**: Manages the import and preparation of documents for processing.\n","\n","7. üß© **Document Chunker**: Breaks down large documents into smaller segments for better efficiency.\n","\n","8. üë§ **User Input**: The starting point, where the user's query activates the RAG workflow.\n","\n","\n","# üåê **The RAG System and Its Subsystems**\n","\n","### 1. üóÇÔ∏è **Index Subsystem**\n","<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\">\n","\n","   - **Components**: Embedder, Vector Store, Document Loader, Document Chunker.\n","\n","   - **Function**: Processes and organizes data into an accessible format.\n","\n","   - **Role**: Creates a searchable database of vectorized information.\n","\n","### 2. üîé **Retrieval Subsystem**:\n","\n","   - **Components**: User Input, Prompt, Vector Store Retriever.\n","\n","   - **Function**: Matches user queries with relevant data.\n","\n","   - **Role**: Fetches the most pertinent information from the index based on user input.\n","\n","### 3. ü§ñ **Augment Subsystem**:\n","\n","   - **Components**: LLM, User Input, Retrieved Data.\n","\n","   - **Function**: Integrates user queries with retrieved data.\n","\n","   - **Role**: Generates accurate and context-rich responses, blending human-like text generation with factually correct information.\n","\n","<img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\">\n","\n","Together, these subsystems form a seamless flow, transforming user queries into comprehensive and reliable responses."]},{"cell_type":"markdown","metadata":{"id":"iK3gf7TDH9GS"},"source":["# Load documents\n","There are SO MANY document loaders in LangChain\n","\n","I won't go every single one in this notebook. But, you can check out [the documentation](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/document_loaders) to see jusy how many are available to you.\n","\n","## üìÇ **Understanding Document Loaders in LangChain**\n","\n","- üìö LangChain document loaders load data from various sources into Document objects.\n","\n","- üìÑ A Document is text with metadata.\n","\n","- üåê Loaders fetch data from text files, web pages, video transcripts, etc.\n","\n","- üîÑ Main role: Retrieve data for further processing.\n","\n","- üõ†Ô∏è Method: Use `load` to fetch data and return it as a Document.\n","\n","- üß† Some loaders support lazy loading (data loads into memory only when needed).\n","\n","## üîß **How to Use Document Loaders**\n","\n","1. üì• Import the loader class from `langchain.document_loaders`.\n","\n","2. üèóÔ∏è Create an instance of your chosen class with the directory path.\n","\n","3. üöÄ Use `load()` to load files in the directory into Document format.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"nmKgqy5X_vhA","executionInfo":{"status":"ok","timestamp":1706851609574,"user_tz":360,"elapsed":5672,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["from langchain_community.document_loaders import WebBaseLoader\n","\n","yolo_nas_loader = WebBaseLoader(\"https://deci.ai/blog/pose-estimation-yolo-nas-pose/\").load()\n","\n","decicoder_loader = WebBaseLoader(\"https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class/\").load()\n","\n","yolo_newsletter_loader = WebBaseLoader(\"https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas\").load()"]},{"cell_type":"code","source":["yolo_newsletter_loader[0]"],"metadata":{"id":"fhwMj0CTSLli"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1shOpId_vqb"},"source":["# Chunk documents\n","\n","üî¢ **Exploring Text Splitters in LangChain**\n","\n","- üìñ Text splitters divide long texts into\n","smaller, meaningful parts.\n","\n","- üß© Aim: Make large texts easier to handle for analysis or processing.\n","\n","### How Text Splitters Work:\n","\n","1. ‚úÇÔ∏è Split text into small, meaningful chunks (like sentences).\n","\n","2. üìè Combine these chunks into a larger one until a certain size is reached.\n","\n","3. üìå Once the size is reached, start a new chunk with some overlap for context.\n","\n","### Customization Axes:\n","\n","1. üõ†Ô∏è How the text is split.\n","\n","2. üìê How chunk size is measured.\n","\n","## Getting Started with Text Splitters\n","\n","- üöÄ Default choice: `RecursiveCharacterTextSplitter`.\n","\n","- üìã Works by: Splitting text based on a list of characters.\n","\n","- üîÑ If chunks are too large, it moves to the next character.\n","\n","- üìå Default split characters: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n","\n","### Additional Controls:\n","\n","- üìè `length_function`: Defines how chunk length is calculated (default: character count, token counter is common).\n","\n","- üîç `chunk_size`: Sets the maximum chunk size.\n","\n","- üîÄ `chunk_overlap`: Determines overlap between chunks for continuity.\n","\n","- üìä `add_start_index`: Option to include each chunk's start position in the original document in metadata."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"fuXm06J1IDs7","executionInfo":{"status":"ok","timestamp":1706851855300,"user_tz":360,"elapsed":207,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 500,\n","    chunk_overlap = 50,\n","    length_function = len\n",")\n","\n","yolo_nas_chunks = text_splitter.transform_documents(yolo_nas_loader)\n","\n","decicoder_chunks = text_splitter.transform_documents(decicoder_loader)\n","\n","yolo_newsletter_chunks = text_splitter.transform_documents(yolo_newsletter_loader)"]},{"cell_type":"markdown","metadata":{"id":"cGzRpURwJqHM"},"source":["# Index System\n","\n","- üéØ **Purpose:** Efficiently organize data for easy retrieval.\n","\n","### Steps in the Index System:\n","1. üìö **Load Documents (Document Loader):**\n","   - Import and read large amounts of data.\n","2. üß© **Chunk Documents (Document Chunker):**\n","   - Break down documents into smaller parts for better handling.\n","3. üåê **Embed Documents (Embedder):**\n","   - Convert text chunks into vector formats for searchability.\n","4. üíæ **Store Embeddings (Vector Store):**\n","   - Keep embeddings and their textual counterparts for retrieval."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"iGtfN_hiJzVB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706851952815,"user_tz":360,"elapsed":4930,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"c1973467-55ba-4da5-dde4-f3b36d875b53"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['f64ae3b1-a7c7-48a9-a84a-2add6f8d0ae6',\n"," 'cb86207c-3999-4cbe-9846-1e9abdd19f15',\n"," 'cfcaa710-ce6d-4f16-aeb8-2c717c002c61',\n"," '33c2d641-7aac-4f0a-8d56-562c85dd6632',\n"," 'cef97655-7bb1-4971-a750-85b8c8c93ba5',\n"," 'dd7c1d67-0987-4a1d-8694-2ea087085aca',\n"," 'ee20aae5-3ef3-429c-87a4-cfb9b3de2e49',\n"," '848fba7b-847d-4f5c-8701-f5500fb4adf2',\n"," '9814ae5f-7aac-4202-b1a5-7ef8f2ac58a4',\n"," '671a8a62-3326-479d-aeab-c931b01a43fb',\n"," '7681c144-1cb0-4334-89e0-2edefceefe76',\n"," '053625f7-ac80-48ad-88f0-cdb99592f3fb',\n"," '65ac8931-3efd-434b-a508-ab679b563648',\n"," 'e763594f-47eb-4502-88fa-7863b48080d8',\n"," '52121261-e918-4632-bb6d-549de89050a8',\n"," 'a0dec906-0cb6-4087-9b6c-10aa7bc604e8',\n"," '2892a8da-a4b6-40e8-91be-92be2703c090',\n"," '290ba25b-9746-4f74-8e35-f9c920733418',\n"," '7cbecb0c-682e-414d-ab66-b45f24712d34']"]},"metadata":{},"execution_count":13}],"source":["from langchain_openai import OpenAIEmbeddings\n","from langchain.embeddings import CacheBackedEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.storage import LocalFileStore\n","\n","store = LocalFileStore(\"./cachce/\")\n","\n","# create an embedder\n","core_embeddings_model = OpenAIEmbeddings()\n","\n","embedder = CacheBackedEmbeddings.from_bytes_store(\n","    core_embeddings_model,\n","    store,\n","    namespace = core_embeddings_model.model\n",")\n","\n","# store embeddings in vector store\n","vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)\n","\n","vectorstore.add_documents(decicoder_chunks)\n","\n","vectorstore.add_documents(yolo_newsletter_chunks)"]},{"cell_type":"markdown","metadata":{"id":"r4HNkh6pmQ61"},"source":["# üîç **Retrieval System**\n","\n","- üéØ **Purpose:** Fetch relevant information based on user queries.\n","\n","### Steps in the Retrieval System:\n","\n","1. üí¨ **Obtain User Query (User Input):**\n","   - Capture the user's question or statement.\n","\n","2. üîÑ **Embed User Query (Embedder):**\n","   - Convert the user's query into a vector format, aligning with indexed documents.\n","\n","3. üîç **Vector Search (Vector Store Retriever):**\n","   - Search for document embeddings in the Vector Store that closely match the user query.\n","\n","4. üìÑ **Return Relevant Documents:**\n","   - Provide the top matching documents, ensuring pertinence to the query.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Eq3nKNJ2mRDK","executionInfo":{"status":"ok","timestamp":1706852050480,"user_tz":360,"elapsed":150,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain import hub"]},{"cell_type":"code","source":["# instantiate a retriever\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"4mDjaXw_6V8B","executionInfo":{"status":"ok","timestamp":1706852051785,"user_tz":360,"elapsed":142,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"D9FGhpMWORwa","executionInfo":{"status":"ok","timestamp":1706852066059,"user_tz":360,"elapsed":176,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["llm = ChatOpenAI(model=\"gpt-4-0125-preview\")"]},{"cell_type":"markdown","source":["# üîç **Augment System**\n","\n","- üöÄ **Purpose:** Improve LLM's input with additional context.\n","\n","### Steps in the Augment System:\n","\n","1. üåü **Create Initial Prompt (Prompt):**\n","   - Begin with the user's initial question or statement.\n","\n","2. üß© **Augment Prompt with Retrieved Context (Context Integration):**\n","   - Blend the initial prompt with context from the Vector Store for a richer input.\n","\n","3. ‚ö° **Send Augmented Prompt to LLM (Input Enhancement):**\n","   - Pass the enhanced prompt to the LLM.\n","\n","4. üì¨ **Receive LLM's Response (Output Reception):**\n","   - Obtain the LLM's comprehensive response after processing the augmented prompt."],"metadata":{"id":"0UgXQzj56xED"}},{"cell_type":"code","source":["%%capture\n","!pip install langchainhub"],"metadata":{"id":"5Gu31lNAUHJN","executionInfo":{"status":"ok","timestamp":1706852144511,"user_tz":360,"elapsed":7986,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"uBhaA_nkpU_2","executionInfo":{"status":"ok","timestamp":1706852169131,"user_tz":360,"elapsed":1357,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","source":["prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7Sqnd-lUwS8","executionInfo":{"status":"ok","timestamp":1706852290207,"user_tz":360,"elapsed":2,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"772ba1b8-d919-4f23-81fa-7d5666e1b023"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["prompt.input_variables"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSbOl4EbUXln","executionInfo":{"status":"ok","timestamp":1706852251409,"user_tz":360,"elapsed":2,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"1bd90cb8-5ff4-430c-9415-a083c870370b"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['context', 'question']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Q-NO6i0YS6nY","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1706852476310,"user_tz":360,"elapsed":24741,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"e425054e-8a71-4b95-9dc9-401f49b2a90d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Neural Architecture Search (NAS) is integral to how Deci creates its models because it uses Deci's advanced NAS engine, AutoNAC, to automate the process of finding optimal neural network architectures. This automation allows Deci to efficiently search through a vast space of possible architectures, significantly reducing the computational resources required. As a result, Deci can develop highly efficient and effective neural network models, such as DeciCoder-6B and YOLO-NAS, by finding novel architectures that promise outstanding results with less computational demand.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["# This is the entire augment system!\n","rag_chain.invoke(\"What does Neural Architecture Search have to do with how Deci creates its models?\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"jGBkR1ewxHWY","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1706852483397,"user_tz":360,"elapsed":7088,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"bbf6f4d5-6aa7-43b2-c366-0385fe7858ed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"DeciCoder is an advanced code generation application known for its cost-effectiveness and superior accuracy, particularly with its DeciCoder-6B model. It outperforms competitors on the HumanEval evaluation benchmark, surpassing models like CodeGen 2.5 7B and StarCoder 7B in nearly all supported languages, especially in Python. DeciCoder's optimized architecture enables exceptional performance on cost-effective hardware, making it an efficient and accurate choice for multi-language code generation.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}],"source":["rag_chain.invoke(\"What is DeciCoder\")"]},{"cell_type":"markdown","source":["## Return sources"],"metadata":{"id":"H9DNus0R7mSP"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableParallel\n","\n","rag_chain_from_docs = (\n","    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","rag_chain_with_source = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}).assign(answer=rag_chain_from_docs)"],"metadata":{"id":"sDTWLOfk7mXX","executionInfo":{"status":"ok","timestamp":1706852504106,"user_tz":360,"elapsed":139,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["rag_chain_with_source.invoke(\"What does Neural Architecture Search have to do with how Deci creates its models?\")"],"metadata":{"id":"7MTFGSfI7mbm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706852513627,"user_tz":360,"elapsed":7860,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"06b43560-3799-4f5d-d448-7c39bd60148e"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(page_content='which is a product of Deci‚Äôs cutting-edge Neural Architecture Search-based AutoNAC engine.', metadata={'source': 'https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class/', 'title': 'Introducing DeciCoder-6B: Code LLM Engineered for Accuracy & Cost Efficiency At Scale', 'description': 'DeciCoder-6B, a multi-language code LLM in the 7B parameter class that supports a sequence length of up to 4096 tokens and excels in 8 code languages.', 'language': 'en-US'}),\n","  Document(page_content='Neural Architecture Search is define the architecture search space. For YOLO-NAS, our researchers took inspiration from the basic blocks of YOLOv6 and YOLOv8. With the architecture and training regime in place, our researchers harnessed the power of AutoNAC. It intelligently searched a vast space of ~10^14 possible architectures, ultimately zeroing in on three final networks that promised outstanding results. The result is a family of architectures with a novel quantization-friendly basic', metadata={'source': 'https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas', 'title': 'Unleashing the Power of YOLO-NAS: A New Era in Object Detection and Computer Vision', 'description': 'The Future of Computer Vision is Here', 'language': 'en'}),\n","  Document(page_content='The NAS Engine Behind DeciCoder-6B: AutoNAC\\nThe architecture of DeciCoder-6B was developed using Deci‚Äôs advanced Neural Architecture Search (NAS) engine, AutoNAC. Traditional NAS methods, while promising, typically require extensive computational resources. AutoNAC circumvents this challenge by automating the search process in a more compute-efficient manner.', metadata={'source': 'https://deci.ai/blog/decicoder-6b-the-best-multi-language-code-generation-llm-in-its-class/', 'title': 'Introducing DeciCoder-6B: Code LLM Engineered for Accuracy & Cost Efficiency At Scale', 'description': 'DeciCoder-6B, a multi-language code LLM in the 7B parameter class that supports a sequence length of up to 4096 tokens and excels in 8 code languages.', 'language': 'en-US'}),\n","  Document(page_content='Harnessing AutoNAC for novel head design\\nFinding an optimal architecture requires navigating the vast architecture search space. But doing so manually is not only tedious but often inefficient. The traditional Neural Architecture Search showed promise in this arena, automating the development of superior neural networks. However, its insatiable thirst for computational resources rendered it impractical for many, confining its usage to a select few with vast computational power.', metadata={'source': 'https://deci.ai/blog/pose-estimation-yolo-nas-pose/', 'title': 'YOLO-NAS Pose - SOTA Pose Estimation Model Using NAS | Deci', 'description': \"Building on YOLO-NAS, we've unveiled its pose estimation sibling: YOLO-NAS Pose. This model offers a superior latency-accuracy balance compared to YOLOv8 Pose.\", 'language': 'en-US'})],\n"," 'question': 'What does Neural Architecture Search have to do with how Deci creates its models?',\n"," 'answer': \"Deci creates its models using Neural Architecture Search (NAS) through its AutoNAC engine, which automates the search for optimal neural network architectures in a compute-efficient manner. This process involves exploring a vast space of possible architectures to find the most promising ones without the extensive computational resources typically required by traditional NAS methods. AutoNAC's ability to streamline this search and development process is central to Deci's approach to creating high-performance and efficient neural network models.\"}"]},"metadata":{},"execution_count":27}]}],"metadata":{"colab":{"provenance":[{"file_id":"1XfUn-YcMT1gAC4K6qUlkKiEubj0Tj0Pu","timestamp":1697209253839},{"file_id":"14VRXcGHu6u0R15AE8cjefERSgdszBYmW","timestamp":1697207548768}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}