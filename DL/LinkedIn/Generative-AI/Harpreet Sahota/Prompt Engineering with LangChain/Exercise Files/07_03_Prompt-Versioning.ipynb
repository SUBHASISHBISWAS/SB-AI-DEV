{"cells":[{"cell_type":"markdown","metadata":{"id":"hyGWZ3FH9d4s"},"source":["# Using Version-Specific Prompts in Production with LangChain:\n","\n","• **Stability and Consistency:**\n","   - Utilizing the \"latest\" version of a prompt can lead to unpredictable issues in production environments.\n","   - For more stable and consistent deployments, it's advisable to use specific prompt versions.\n","\n","• **Version-Specific Prompt Pulling:**\n","   - LangChain hub supports pulling prompts by their specific commit hash.\n","   - This ensures you're using the exact version of the prompt needed for your application.\n","\n","• **Specifying the Version:**\n","   - To specify a prompt's version, append the 'version' tag to the prompt ID in the pull command.\n","   - Here's a Python snippet for reference:\n","     ```python\n","     from langchain import hub\n","\n","     hub.pull(f\"{handle}/{prompt-repo}:{version}\")\n","     ```\n","\n","•. **Prerequisites:**\n","   - Having a LangSmith account and an API key for your organization is essential.\n","   - If you're starting, the [LangSmith documentation](https://docs.smith.langchain.com/hub/quickstart) provides a comprehensive guide for setup.\n","\n","By following these guidelines, you can achieve more reliable and controlled deployments in your production environments using LangChain."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2hQDslu9d4u"},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.1 openai==1.8.0 langchainhub langchain-openai"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"jn4z_Rdb9d4u"},"outputs":[],"source":["import os\n","import getpass"]},{"cell_type":"code","source":["os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key:\")"],"metadata":{"id":"dUTa8jPC_W3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700260005664,"user_tz":360,"elapsed":1957,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"b32d54e5-bc31-4862-803a-89e1c8204da4"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["LangSmith API Key:··········\n"]}]},{"cell_type":"code","source":["os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woEpcBEUg7GK","executionInfo":{"status":"ok","timestamp":1700260013690,"user_tz":360,"elapsed":1874,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"0efab7ba-5d0b-4a06-d2b8-39d8c4e52c1c"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["OpenAI API Key:··········\n"]}]},{"cell_type":"code","source":["# Update with your API URL if using a hosted instance of Langsmith.\n","os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n","# Update with your API URL if using a hosted instance of Langsmith.\n","os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\""],"metadata":{"id":"T0eUeeXf_bj8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOD1o5Cb9d4v"},"source":["### Loading a Specific Version of a Prompt:\n","\n","1. **Version Tracking in Repositories:**\n","   - Each push to a prompt repository saves a new version, identified by a unique commit hash.\n","\n","2. **Loading the Latest Version:**\n","   - By default, accessing the repo will load the most recent version of a given prompt.\n","\n","3. **Loading a Specific Version:**\n","   - To load a specific version, include its commit hash with the prompt name.\n","   - Example: For loading the \"rag-prompt\" with version `c9839f14`, append this hash to the prompt name in your loading command."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"z0VO8UjV9d4v"},"outputs":[],"source":["from typing import Any, Optional\n","\n","from langchain import hub\n","\n","def pull_prompt_from_langchain(handle: str, prompt_name: str, version: Optional[str] = None, api_url: str = \"https://api.hub.langchain.com\") -> Any:\n","    \"\"\"\n","    Pulls a specified prompt from the Langchain AI Hub.\n","\n","    Args:\n","        handle (str): The handle of the repository in the Langchain AI Hub.\n","        prompt_name (str): The name of the prompt to be pulled.\n","        version (Optional[str]): The specific version of the prompt, if any.\n","        api_url (str): The API URL for the Langchain AI Hub. Defaults to 'https://api.hub.langchain.com'.\n","\n","    Returns:\n","        Any: The prompt object retrieved from the Langchain AI Hub.\n","    \"\"\"\n","    request_str = f\"{handle}/{prompt_name}\"\n","    if version:\n","        request_str += f\":{version}\"\n","    prompt = hub.pull(request_str, api_url=api_url)\n","    return prompt\n","\n","def push_prompt_to_langchain(handle: str, prompt_name: str, prompt_to_push, api_url: str = \"https://api.hub.langchain.com\") -> Any:\n","    \"\"\"\n","    Pulls a specified prompt from the Langchain AI Hub.\n","\n","    Args:\n","        handle (str): The handle of the repository in the Langchain AI Hub.\n","        prompt_name (str): The name of the prompt to be pulled.\n","        version (Optional[str]): The specific version of the prompt, if any.\n","        api_url (str): The API URL for the Langchain AI Hub. Defaults to 'https://api.hub.langchain.com'.\n","\n","    Returns:\n","        Any: The prompt object retrieved from the Langchain AI Hub.\n","    \"\"\"\n","    request_str = f\"{handle}/{prompt_name}\"\n","    prompt = hub.push(request_str, prompt_to_push, api_url=api_url)\n","    return prompt"]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n","\n","prompt = pull_prompt_from_langchain(\"datascienceharp\", \"lczc\")"],"metadata":{"id":"kozrON1YgU_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmIkXsPblFCg","executionInfo":{"status":"ok","timestamp":1700260098969,"user_tz":360,"elapsed":124,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"a5d84d7b-e037-4003-ff59-e601d6d1cf13"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert in Deep Learning and Language Modeling. You are also an expert in PyTorch. You are brief in your reply and straight to the point. ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["question = \"How do I generate text with a HuggingFace transformers model?\"\n","\n","chain = prompt | llm"],"metadata":{"id":"gNHtJtcalD_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = chain.invoke({\"question\":question})"],"metadata":{"id":"mq8uDFJsjkT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def m_print(message: str) -> str:\n","    display(Markdown(message))\n","\n","m_print(result.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"id":"8MwRHGTWmrwb","executionInfo":{"status":"ok","timestamp":1700260126259,"user_tz":360,"elapsed":149,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"5ff87ec8-97a1-4c63-b503-9686725a7d1d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"First, ensure you have the `transformers` library installed:\n\n```bash\npip install transformers\n```\n\nThen, use the following Python code snippet to generate text with a HuggingFace transformers model:\n\n```python\nfrom transformers import pipeline\n\n# Load a text generation pipeline\ngenerator = pipeline('text-generation', model='gpt2') # Replace 'gpt2' with your model of choice\n\n# Generate text\nprompt = \"Today is a beautiful day\"  # Your text prompt\ngenerated_texts = generator(prompt, max_length=50, num_return_sequences=1)\n\n# Print generated text\nfor i, text in enumerate(generated_texts):\n    print(f\"{i}: {text['generated_text']}\")\n```\n\nModify `max_length` for longer or shorter outputs and `num_return_sequences` for more generated samples. Adjust the `model` parameter as needed to use other models."},"metadata":{}}]},{"cell_type":"markdown","source":["### Uploading Prompts to the LangChain Hub:\n","\n","1. **Introduction:**\n","   - Retrieving prompts from the LangChain Hub is user-friendly, and so is uploading your own prompts to the hub.\n","\n","   - This process allows you to share and manage your custom prompts efficiently.\n","\n","2. **Creating a Simple Prompt:**\n","\n","   - Start by crafting a prompt that suits your specific application needs.\n","\n","   - Ensure that it meets the criteria and standards expected by the LangChain Hub.\n","\n","3. **Uploading Process:**\n","\n","   - The snippet consists of two key components:\n","     - **Account Handle:** This is your unique identifier within the LangChain Hub, such as `myfirstlangchain-hub1`.\n","     - **Prompt Name:** A descriptive name for your prompt, indicating its purpose or functionality.\n","\n","4. **Code Snippet for Uploading:**\n","   - Here's a generic format for the code snippet used for uploading:\n","     ```python\n","     from langchain import hub\n","\n","     # Define your prompt\n","     my_prompt = \"...\"  # Your prompt content goes here\n","\n","     # Upload the prompt to LangChain Hub\n","     hub.push(f\"{account_handle}/{prompt_name}\", my_prompt)\n","     ```\n","     - Replace `account_handle` with your handle and `my_prompt` with the name of your prompt template.\n","     \n","     - Ensure that `my_prompt` is a LangChain `PromptTemplate`.\n","\n","5. **Usage and Accessibility:**\n","   - Once uploaded, your prompt becomes accessible for retrieval and use in various applications through the LangChain Hub.\n","   - This facilitates easy sharing and collaboration, as well as version control and management of your custom prompts.\n","\n"],"metadata":{"id":"4uZRc93smr1L"}},{"cell_type":"code","source":["prompt.messages[0].prompt.template += \"You write code that follows PEP8 standards, and you are a Principal Level Engineer who is a great mentor.\""],"metadata":{"id":"X_vXUp_BqfQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["push_prompt_to_langchain(\"datascienceharp\", \"lczc\", prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"RoHywq6xnFNF","executionInfo":{"status":"ok","timestamp":1700260200294,"user_tz":360,"elapsed":785,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"c1e9fd9d-1586-4e31-f471-db0da5c0d97b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'https://smith.langchain.com/hub/datascienceharp/lczc/1c725cd1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"ykwZGbO79d4x"},"source":["#The Importance of Prompt Versioning in LangChain Workflows:\n","\n","1. **Stable Deployments:**\n","   - Loading specific versions of prompts ensures stability in your deployments. This approach safeguards against the unintended consequences of ongoing updates.\n","\n","2. **Continuous Collaboration and Experimentation:**\n","   - With prompt versioning, you and your team can freely commit new versions to the prompt repo. This flexibility fosters a dynamic environment for experimentation and improvement.\n","\n","3. **Preventing Deployment of Under-Validated Components:**\n","   - Version control plays a crucial role in preventing the accidental deployment of untested or under-validated components. By selecting proven versions of prompts, you maintain the integrity and reliability of your systems.\n","\n","4. **Workflow Efficiency:**\n","   - This feature streamlines the development process, allowing for agile development and iteration without risking your production environment.\n","\n","5. **Conclusion:**\n","   - Incorporating prompt versioning into your LangChain workflow is not just a best practice for stability and reliability; it's also a strategic approach to fostering innovation and collaboration in a controlled and efficient manner.\n","\n","In essence, prompt versioning is a key tool in the LangChain arsenal, enabling developers to balance the need for ongoing development and experimentation with the necessity of maintaining stable and reliable deployments."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}