{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOU3qWDrSbZk/WGusT5u7Pp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sk4lfY0zCsa5"},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.4 openai==1.10.0 langchain-openai"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"id":"_moU87efUosB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompt Pipelining Overview\n","\n","Prompt pipelining is a powerful tool designed for those who seek a modular and efficient approach to prompt design.\n","\n","\n","## Here are some common cases where prompt pipelining can be useful:\n","\n"," - Reusing prompt components like introductions, instructions, examples, etc. across multiple prompts. Prompt pipelining allows easily reusing these modular blocks.\n","\n"," - Splitting up a long prompt into smaller logical chunks. This can make prompts more readable and maintainable.\n","\n"," - Dynamically assembling prompts based on conditionals or other logic. You can build prompts on the fly from reusable parts.\n","\n"," - Creating prompts in a loop by appending to an existing base prompt. Useful for things like few-shot learning prompts.\n","\n"," - Mixing static text with templates containing variables. The static text can provide structure while the templates inject dynamic content.\n","\n"," - Composing chat prompts from message templates and static messages. Each piece gets appended as a new message.\n","\n"," - Building up prompts from user provided components for customization. Prompt pipelining allows prompts to be assembled from modular parts.\n","\n"," - Any situation where you want to build up prompts in a reusable composable way, prompt pipelining provides a clean interface for that.\n","\n","## String Prompt Pipelining\n","\n","For string prompts, templates are interconnected in sequence.\n","\n","You have the liberty to utilize either direct prompts or strings.\n","\n","However, it's essential to note that the initial element in the sequence should always be a prompt."],"metadata":{"id":"0ed8MD_LUp6I"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","prompt = (\n","    PromptTemplate.from_template(\"I'm heading to {destination}. \")\n","    + \"Recommend a great {activity} spot!\")\n","\n","prompt\n"],"metadata":{"id":"rXzIc8jCXiob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = prompt + \"\\n\\nAlso, any local delicacies I should try?\"\n","prompt"],"metadata":{"id":"z1wpoDv7l3Wg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt.format(destination=\"Punjab\", activity=\"dining\")"],"metadata":{"id":"XVbMS46yYIwO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The key differences between prompt pipelining and multi-input prompts are:\n","\n"," - Prompt pipelining composes multiple prompts/strings into a single final prompt. Multi-input prompts allow passing multiple inputs to a single prompt template.\n","\n"," - With pipelining, each component can be formatted independently before joining. Multi-input just formats a single template.\n","\n"," - Pipelining joins components into a linear sequence. Multi-input prompts are a single template handling multiple inputs.\n","\n"," - Pipelining allows reuse of components like introductions, examples, etc. Multi-input is focused on handling multiple query inputs.\n","\n"," - Pipelining output is a single prompt. Multi-input output is the completion based on multiple inputs.\n","\n"," - Pipelining lets you build up prompts modularly. Multi-input is one template handling multiple inputs.\n","\n","In summary, prompt pipelining composes multiple prompt components into one final prompt in a linear sequence. Multi-input prompts allow a single template to handle multiple input variables in parallel. Pipelining focuses on modular prompt building, while multi-input handles multiple query inputs."],"metadata":{"id":"kCuU15Zsb6-E"}},{"cell_type":"markdown","source":["# Use in a chain"],"metadata":{"id":"vWKXtrGIYPjX"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser"],"metadata":{"id":"sgGCg2s8YtqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n","\n","output_parser = StrOutputParser()\n","\n","chain = prompt | llm | output_parser\n","\n","chain.invoke({\"destination\":\"Punjab\", \"activity\":\"dining\"})"],"metadata":{"id":"F8-qiXIAYu7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = prompt +\" How should I greet the locals in a jolly, informal manner?\"\n","\n","chain = prompt | llm | output_parser\n","\n","chain.invoke({\"destination\":\"Punjab\", \"activity\":\"dining\"})"],"metadata":{"id":"X7COZydQl2gz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example usecase"],"metadata":{"id":"V4WgevnuoYIO"}},{"cell_type":"code","source":["class TravelChatbot:\n","    def __init__(self, base_template):\n","        self.model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n","        self.base_prompt = PromptTemplate.from_template(base_template)\n","\n","    def append_to_prompt(self, additional_text):\n","        self.base_prompt += additional_text\n","\n","    def run_chain(self, destination, activity):\n","        output_parser = StrOutputParser()\n","        chain = self.base_prompt | self.model | output_parser\n","        return chain.invoke({\"destination\":destination, \"activity\":activity})\n","\n","# Usage\n","base_template = \"I'm heading to {destination}. Recommend a great {activity} spot!\"\n","\n","chatbot = TravelChatbot(base_template)\n","# Basic prompt\n","chatbot.run_chain(destination=\"Punjab\", activity=\"dining\")"],"metadata":{"id":"_BdVa1z2l2X0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Append more to the prompt and run again\n","chatbot.append_to_prompt(\"\\n\\nAlso, any local delicacies I should try?\")\n","print(chatbot.run_chain(destination=\"Punjab\", activity=\"dining\"))"],"metadata":{"id":"9PvagxTLl2PF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chatbot.append_to_prompt(\" How should I greet the locals in a friendly, informal, jolly colloquial manner?\")\n","chatbot.run_chain(destination=\"Punjab\", activity=\"dining\")"],"metadata":{"id":"ufMFRyBnltpk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chat Prompt Pipeline\n","\n","A chat prompt is made up a of a list of messages. Purely for developer experience, we've added a convinient way to create these prompts. In this pipeline, each new element is a new message in the final prompt.\n","\n","### Key Insights into Chat Prompt Pipelining in Langchain:\n","\n","Composition: Chat prompt pipelining facilitates the creation of chat prompts using reusable message components. Each addition to the pipeline translates to a new message in the final chat prompt.\n","\n","Versatility: You can seamlessly integrate static message objects, such as HumanMessage and AIMessage, with variable-containing message templates.\n","\n","End Result: The culmination is a unified ChatPromptTemplate, primed for formatting and integration into a chain. This structure promotes the effortless reuse of components, including instructions and examples.\n","\n","Modularity: This approach champions the dynamic construction of chat prompts using modular blocks, ensuring flexibility and efficiency.\n","\n","So in summary, chat prompt pipelining composes chat prompts from reusable message templates and static messages. It allows dynamically constructing prompts from logical blocks in a user friendly way. The end result is a single reusable ChatPromptTemplate.\n","\n"],"metadata":{"id":"3ySTTr9VYx1f"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","\n","# Setting the scene with a Cockney-themed system message\n","prompt = SystemMessage(content=\"Welcome to the East End Cockney Chat! ðŸ‡¬ðŸ‡§\")\n","\n","# Constructing a chat flow with dry humour\n","new_prompt = (\n","    prompt\n","    + HumanMessage(content=\"Alright, guv'nor?\")\n","    + AIMessage(content=\"Not too shabby. Did you hear about the London fog?\")\n","    + \"{input}\"\n",")\n","\n","# Formatting the chat with the user's response\n","new_prompt.format_messages(input=\"No, what about it?\")\n","print(new_prompt)\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import LLMChain\n","\n","model = ChatOpenAI()\n","\n","chain = LLMChain(llm=model, prompt=new_prompt)\n","\n","# Running the chatbot to get the punchline\n","response = chain.run(\"No, what about it?\")\n","response"],"metadata":{"id":"RK4T0u4Io4S6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompt Composition\n","\n","This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt. A PipelinePrompt consists of two main parts:\n","\n","1. Final prompt: The final prompt that is returned\n","\n","2. Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name."],"metadata":{"id":"rl5mldDcqzOp"}},{"cell_type":"code","source":["from langchain.prompts.pipeline import PipelinePromptTemplate\n","from langchain.prompts.prompt import PromptTemplate"],"metadata":{"id":"328V_Y-Qq1UH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_template = \"\"\"{introduction}\n","\n","{example}\n","\n","{start}\"\"\"\n","\n","full_prompt = PromptTemplate.from_template(full_template)"],"metadata":{"id":"4IUN1EfirhVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n","introduction_prompt = PromptTemplate.from_template(introduction_template)"],"metadata":{"id":"j2w6SdMmnXDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_template = \"\"\"Here's an example of an interaction:\n","\n","Q: {example_q}\n","A: {example_a}\"\"\"\n","example_prompt = PromptTemplate.from_template(example_template)"],"metadata":{"id":"3JbuG8wEnYe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_template = \"\"\"Now, do this for real!\n","\n","Q: {input}\n","A:\"\"\"\n","start_prompt = PromptTemplate.from_template(start_template)"],"metadata":{"id":"pHAMVyaGnZ9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_prompts = [\n","    (\"introduction\", introduction_prompt),\n","    (\"example\", example_prompt),\n","    (\"start\", start_prompt)\n","]\n","pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"],"metadata":{"id":"Drwq7Z4cnbN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline_prompt.input_variables"],"metadata":{"id":"WYgcG7BAncnT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_prompt = pipeline_prompt.format(\n","    person=\"Elon Musk\",\n","    example_q=\"What's your favorite car?\",\n","    example_a=\"Tesla\",\n","    input=\"What's your favorite social media site?\"\n",")\n","\n","print(last_prompt)"],"metadata":{"id":"lsNGuKCXnhpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.75)\n","\n","llm.invoke(last_prompt)"],"metadata":{"id":"ahbbsNgSoUtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate, PipelinePromptTemplate\n","\n","class CookingShowChatbot:\n","    def __init__(self):\n","        # Base template for the cooking show scenario\n","        self.full_template = \"\"\"{introduction}\n","        {example_dish}\n","\n","        {present_dish}\"\"\"\n","        self.full_prompt = PromptTemplate.from_template(self.full_template)\n","\n","        # Introduction where the user impersonates a famous chef\n","        self.introduction_template = \"\"\"Welcome to the cooking show! Today, you're channeling the spirit of Chef {chef_name}.\"\"\"\n","        self.introduction_prompt = PromptTemplate.from_template(self.introduction_template)\n","\n","        # Example dish made by the famous chef\n","        self.example_dish_template = \"\"\"Remember when Chef {chef_name} made that delicious {example_dish_name}? It was a hit!\"\"\"\n","        self.example_dish_prompt = PromptTemplate.from_template(self.example_dish_template)\n","\n","        # User's turn to present their dish\n","        self.present_dish_template = \"\"\"Now, it's your turn! Show us how you make your {user_dish_name}. Let's get cooking!\"\"\"\n","        self.present_dish_prompt = PromptTemplate.from_template(self.present_dish_template)\n","\n","        # Combining the prompts into a pipeline\n","        self.input_prompts = [\n","            (\"introduction\", self.introduction_prompt),\n","            (\"example_dish\", self.example_dish_prompt),\n","            (\"present_dish\", self.present_dish_prompt)\n","        ]\n","        self.pipeline_prompt = PipelinePromptTemplate(final_prompt=self.full_prompt,\n","                                                      pipeline_prompts=self.input_prompts\n","                                                      )\n","\n","    def run_scenario(self, chef_name, example_dish_name, user_dish_name):\n","        return self.pipeline_prompt.format(\n","            chef_name=chef_name,\n","            example_dish_name=example_dish_name,\n","            user_dish_name=user_dish_name\n","        )\n","\n","# Usage\n","chatbot = CookingShowChatbot()\n","scenario = chatbot.run_scenario(chef_name=\"Gordon Ramsay\", example_dish_name=\"Beef Wellington\", user_dish_name=\"Vegan Lasagna\")\n","print(scenario)"],"metadata":{"id":"dFIiHk-FnjVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm.invoke(scenario)\n","\n","print(response.content)"],"metadata":{"id":"CjGCHwc1oRz9"},"execution_count":null,"outputs":[]}]}