{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJBQr3FE+tEPGQlyUlHjSQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QhbzFH3vVgy6"},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.4 openai==1.10.0 langchain-openai"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"id":"_vWgLxI2V5qO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Structure of a Prompt\n","\n","A prompt can consist of multiple components:\n","\n","* Instructions\n","* External information or context\n","* User input or query\n","* Output indicator\n","\n","Not all prompts require all of these components. However, a good prompt will use two or more of them.\n","\n","Let's define each component more precisely.\n","\n"," - **Instructions** tell the model what to do, typically how to use inputs and/or external information to produce the desired output.\n","\n"," - **External information or context** Additional information can be manually inserted into the prompt, retrieved from long-term memory, or pulled in through API calls or calculations.\n","\n"," - **User input or query** is typically a query directly input by the system user.\n","\n"," - **Output indicator** is the *beginning* of the generated text.\n","\n","# What is a prompt template?\n","\n","A prompt template is a tool used to produce a prompt in a consistent and repeatable manner.\n","\n","It consists of a text string, also known as \"the template\", that can take input from users and generate a prompt.\n","\n","The template may include instructions for the language model, examples for the model to learn from, or specific questions to guide the model's response.\n","\n","It helps to create a more accurate and efficient prompt generation process.\n","\n","A prompt template will take some input (here we're using `query`} and format the template string to include that input."],"metadata":{"id":"r8cH681iWAyu"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","template = \"\"\"You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n","\n","You answer queries by being brief, bright, and concise.\n","\n","Query: {query}\n","\"\"\""],"metadata":{"id":"E7ttt5EFnpzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# instantiate using the initializer\n","prompt_template = PromptTemplate(input_variables = ['query'],template = template)\n","prompt_template.pretty_print()"],"metadata":{"id":"WwaUaEFuoZZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_template.format(query=\"Give me the outline of a PyTorch training loop.\")"],"metadata":{"id":"i3A46gKtLU1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# recommended to instantiate using `from_template`\n","prompt_template = PromptTemplate.from_template(template)\n","prompt_template.pretty_print()"],"metadata":{"id":"t1INkk2ULXZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_template.format(query=\"Give me the outline of a PyTorch training loop.\")"],"metadata":{"id":"mKduHp0_LaCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n","\n","llm_chain = prompt_template | llm | StrOutputParser()"],"metadata":{"id":"RS2VrlNNofqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm_chain.invoke({\"query\":\"Give me the outline of a PyTorch training loop.\"})"],"metadata":{"id":"MmyZlHUyLbZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in llm_chain.stream({\"query\":\"Give me the outline of a PyTorch training loop.\"}):\n","    print(chunk, end=\"\", flush=True)"],"metadata":{"id":"C2uF2Kz0Lbdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in llm_chain.stream({\"query\":\"Why is the SoftMax function used in NNs?\"}):\n","    print(chunk, end=\"\", flush=True)"],"metadata":{"id":"1lakJLM5LbiW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You could use Python string manipulation to create a prompt, but PromptTemplate is more legible and works with any number of input variables."],"metadata":{"id":"S_B2GmUPth6v"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","def get_advice(topic: str) -> str:\n","    \"\"\"\n","    Generate advice for a given topic using the OpenAI model.\n","\n","    Args:\n","    - topic (str): The subject on which advice is needed.\n","\n","    Returns:\n","    - str: Advice from the OpenAI model.\n","    \"\"\"\n","    # Initialize the OpenAI model with a temperature setting of 0.9.\n","    llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.9)\n","\n","    # Define the template for generating the prompt.\n","    prompt = PromptTemplate.from_template(template=\"Can you give me some advice on {topic}?\")\n","\n","    chain = prompt | llm | StrOutputParser()\n","\n","    for chunk in chain.stream({\"topic\":topic}):\n","      print(chunk, end=\"\", flush=True)\n","\n","# Test the get_advice function with a couple of topics.\n","print(get_advice(\"Balancing so many priorities that I don't have any free time\"))"],"metadata":{"id":"QNQ0Yzf-qv4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_advice(\"Getting over my addiction to learning new things\"))"],"metadata":{"id":"W8m1ib2trg80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multi-input prompts\n","\n"],"metadata":{"id":"4JJKDd61-OHD"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Initialize the OpenAI model with a temperature setting of 0.9.\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.9)\n","\n","def get_movie_information(movie_title: str, main_actor:str) -> str:\n","    \"\"\"\n","    Predict the genre and synopsis of a given movie using the OpenAI model.\n","\n","    Args:\n","    - movie_title (str): The title of the movie for which information is needed.\n","    - main_actor (str): The main actor of the movie for which information is needed.\n","    Returns:\n","    - str: Predicted genre and main actor information from the OpenAI model.\n","    \"\"\"\n","\n","    # Define the template for generating the prompt.\n","    prompt = PromptTemplate(\n","        input_variables=[\"movie_title\", \"main_actor\"],\n","        template=\"\"\"\n","        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n","\n","        Movie: {movie_title}\n","        Actor: {main_actor}\n","        \"\"\"\n","        )\n","\n","    # Format the prompt using the provided movie title.\n","    prompt_text = prompt.format(movie_title=movie_title, main_actor=main_actor)\n","\n","    # Print the generated prompt.\n","    print(prompt_text)\n","\n","    response = llm.invoke(prompt_text)\n","\n","    # Get the movie information from the OpenAI model and return it.\n","    return response.content"],"metadata":{"id":"vQ-XOH39pIRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_movie_information(movie_title=\"Jatt da Pajama Uuchaa Ho Gayaa\", main_actor=\"AP Dhillon\"))"],"metadata":{"id":"33MSVfitpIXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_movie_information(movie_title=\"Amritsar:1984\", main_actor=\"Gurdaas Mann\"))"],"metadata":{"id":"GcvbhcmJpIcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_movie_information(movie_title=\"Amritsar: 1984\", main_actor=\"Diljit Dosanjh\"))"],"metadata":{"id":"WcPqMVSX-BQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_movie_information(movie_title=\"Chandighar:Sector 17\", main_actor=\"Diljit Dosanjh\"))"],"metadata":{"id":"iV7Ns5RR66UZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chat prompt templates\n","\n","The prompt to chat models is a **list** of chat messages.\n","\n","Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role.\n","\n","`ChatPromptTemplate.from_messages`  accepts a list as the argument, and each element in that list can be a message representation like:\n","\n"," - A tuple with `(role, content)` - For example: (\"system\", \"You are a helpful assistant\")\n","\n"," - An instance of a `MessagePromptTemplate` subclass like `SystemMessagePromptTemplate` or `HumanMessagePromptTemplate`.\n","\n","\n","So in summary:\n","\n"," - `ChatPromptTemplate.from_messages` accepts a list\n","\n"," - Each element in the list can be a message representation\n","\n"," - One option for the message representation is a `MessagePromptTemplate` subclass instance like `SystemMessagePromptTemplate`\n"],"metadata":{"id":"wy70tkyF__n9"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n","from langchain_core.messages import SystemMessage\n","from langchain_openai import ChatOpenAI"],"metadata":{"id":"YZxYjfIFDIdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.8)\n","\n","template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.\"),\n","    (\"human\", \"Yo! Wassup nephew.\"),\n","    (\"ai\", \"As an AI language model, I am incapable of being your nephew.\"),\n","    (\"human\", \"{user_input}\"),\n","])\n","\n","messages = template.format_messages(\n","    name=\"Robotalker\",\n","    user_input=\"Talk robo to me!\"\n",")"],"metadata":{"id":"7p8RWSS-__tW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm.invoke(messages)\n","\n","print(response.content)"],"metadata":{"id":"vSZxR7ma__zE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use LCEL\n","chain = template | llm | StrOutputParser()"],"metadata":{"id":"IgNEhI3HOVB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"})"],"metadata":{"id":"M-86UYWVOVJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in chain.stream({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"}):\n","  print(chunk, end=\"\", flush=True)"],"metadata":{"id":"CeVOF04yOX2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["system_message = SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\")\n","\n","human_message = HumanMessagePromptTemplate.from_template(\"{text}\")\n","\n","template = ChatPromptTemplate.from_messages([system_message, human_message])"],"metadata":{"id":"19-0OEca__-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm.invoke(template.format_messages(text=\"That Sam I Am, I do not like that Sam I Am...\"))\n","\n","print(response.content)"],"metadata":{"id":"BkiuvUpvAAFY"},"execution_count":null,"outputs":[]}]}