{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangSmith Deep Dive\n",
        "\n",
        "This notebook is co-authored with my friends at [AI MakerSpace](https://aimakerspace.io/). Check out their [YouTube channel](https://www.youtube.com/@AI-Makerspace/featured) for, hands down, the best educational content for all things LLMs.\n",
        "\n",
        "Be sure to connect with [Chris Alexiuk](https://ca.linkedin.com/in/csalexiuk) and [Greg Loughnane](https://www.linkedin.com/in/gregloughnane) on LinkedIn!"
      ],
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depenedencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ],
      "metadata": {
        "id": "tw5ok9p-XuUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhSjB1O6-Y0J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.1.4 openai==1.10.0 langchain_openai langsmith  tiktoken cohere -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "ADl8-whIAUHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ],
      "metadata": {
        "id": "T_NpPwk1YAgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ],
      "metadata": {
        "id": "CUWXhsNVYLTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
      ],
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ],
      "metadata": {
        "id": "iiagvgVDYTPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ],
      "metadata": {
        "id": "PDO0XJqbYabb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ],
      "metadata": {
        "id": "sAS3QBQSARiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "j0lnrY14zC0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata[\"source\"]"
      ],
      "metadata": {
        "id": "_s_x87H0BYmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ],
      "metadata": {
        "id": "F79PdFcaYfBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 256,\n",
        "    chunk_overlap = 16\n",
        ").split_documents(documents)"
      ],
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_documents)"
      ],
      "metadata": {
        "id": "yLA5-LNBBVM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_documents[42]"
      ],
      "metadata": {
        "id": "MkFUuWVOzRXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's Embeddings Models today!"
      ],
      "metadata": {
        "id": "EUsEc07iYnwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAISS VectorStore Retriever\n",
        "\n",
        "Now we can use a FAISS VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ],
      "metadata": {
        "id": "NLoO_2MaY0TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install faiss-cpu -qU"
      ],
      "metadata": {
        "id": "HoZFgJB-CAdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_documents, base_embeddings_model)"
      ],
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ],
      "metadata": {
        "id": "U2GPhHPAY5yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ],
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ],
      "metadata": {
        "id": "xmT5VyLmZAAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})"
      ],
      "metadata": {
        "id": "6Dq9rCScDfBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ],
      "metadata": {
        "id": "fJtSdDsXZXam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Langsmith_RAG_{unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
      ],
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangSmith API\n"
      ],
      "metadata": {
        "id": "Ms4msyKLaIr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "id": "MVq1EYngEMhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our our first generation!"
      ],
      "metadata": {
        "id": "6qy0MMBLacXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ],
      "metadata": {
        "id": "3eoqBtBQERXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ],
      "metadata": {
        "id": "fLxh0-thanXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "test_inputs = [\n",
        "    \"What is LangSmith?\",\n",
        "    \"What is LangServe?\",\n",
        "    \"How could I benchmark RAG on tables?\",\n",
        "    \"What was exciting about LangChain's first birthday?\",\n",
        "    \"What features were released for LangChain on August 7th?\",\n",
        "    \"What is a conversational retrieval agent?\"\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v1\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for input in test_inputs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : input},\n",
        "      outputs={\"answer\" : base_rag_chain.invoke({\"question\" : input})[\"context\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ],
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!"
      ],
      "metadata": {
        "id": "QXgi14vSbFIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "  evaluators=[\n",
        "    RunEvalConfig.CoTQA(llm=eval_llm, prediction_key=\"response\"),\n",
        "    RunEvalConfig.Criteria(\"harmfulness\", prediction_key=\"response\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "base_rag_base_run = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=base_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "CENtd4K_IQa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Reranking\n",
        "\n",
        "We'll add reranking to our RAG application to confirm the claim made by [Cohere](https://cohere.com/rerank)!\n",
        "\n",
        "`Improve search performance with a single line of code`\n",
        "\n",
        "We'll put that to the test today!"
      ],
      "metadata": {
        "id": "fOB0u8-RemQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")"
      ],
      "metadata": {
        "id": "CIgP810vPGdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever_expander = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\" : 10}\n",
        ")"
      ],
      "metadata": {
        "id": "Z3VTNGXlO-m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "reranker = CohereRerank()\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker, base_retriever=base_retriever_expander\n",
        ")"
      ],
      "metadata": {
        "id": "Uk7EPsa3PiUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recreating our Chain with Reranker\n",
        "\n",
        "Now we can recreate our chain using the reranker."
      ],
      "metadata": {
        "id": "MBN0h0Zbe7up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | rerank_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "rerank_rag_chain = rerank_rag_chain.with_config({\"tags\" : [\"cohere-rerank\"]})"
      ],
      "metadata": {
        "id": "kfckjK3QPqhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved Evaluation\n",
        "\n",
        "Now we can leverage the full suite of LangSmith's evaluation to evaluate our chains on multiple metrics, including custom metrics!"
      ],
      "metadata": {
        "id": "6qEHGMLAfISS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_config = RunEvalConfig(\n",
        "  evaluators=[\n",
        "    RunEvalConfig.CoTQA(llm=eval_llm, prediction_key=\"response\"),\n",
        "    RunEvalConfig.Criteria(\"harmfulness\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\n",
        "        {\n",
        "            \"helpfulness\" : (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \"taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        prediction_key=\"response\"\n",
        "    ),\n",
        "    RunEvalConfig.LabeledCriteria(\n",
        "        {\n",
        "            \"litness\" : (\n",
        "                \"Is this submission lit, dope, or cool?\"\n",
        "            )\n",
        "        },\n",
        "        prediction_key=\"response\"\n",
        "    ),\n",
        "    RunEvalConfig.LabeledCriteria(\"conciseness\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\"coherence\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\"relevance\", prediction_key=\"response\")\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "nQQXpFg2SV5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Eval on Each Chain\n",
        "\n",
        "Now we can evaluate each of our chains!"
      ],
      "metadata": {
        "id": "DPxlhXLmft0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_chain_results = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=base_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "IuVnTrncU9LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_chain_results = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=rerank_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rS0m4cunQ1m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5o_glU-0zy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}