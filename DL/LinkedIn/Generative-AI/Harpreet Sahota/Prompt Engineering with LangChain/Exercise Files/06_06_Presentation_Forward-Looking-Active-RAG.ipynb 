{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JDuyfKY67dnYM5IUlxZgUA0m4u7cFdst","timestamp":1697211613393}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"5NpLUx0bgw57","executionInfo":{"status":"ok","timestamp":1706853267193,"user_tz":360,"elapsed":29161,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.4 openai==1.10.0 langchain-openai langchainhub faiss-cpu arxiv pymupdf tiktoken"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"id":"sHgv4EQ7hBxA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706853315663,"user_tz":360,"elapsed":48475,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"065980d4-73f6-4bef-decf-be5a1f981317"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Your OpenAI API Key:路路路路路路路路路路\n"]}]},{"cell_type":"markdown","source":["# FLARE\n","\n","### What is FLARE?\n","\n","[FLARE](https://arxiv.org/pdf/2305.06983.pdf) stands for `Forward-Looking Active REtrieval` augmented generation.  This is a method that was introduced in 2023 in a paper titled \"Active Retrieval Augmented Generation\" by Zhengbao Jiang, et al. The original implementation can be found in [the paper's GitHub repo](https://github.com/jzbjyb/FLARE), which the LangChain implementation draws heavy inspiration from.\n","\n","\n","##  **Why FLARE Matters**\n","\n","- Enhances large language models by fetching info from external sources while generating content.\n","\n","- Tackles the issue of models making stuff up or being factually wrong.\n","\n","- Aims for outputs that are more accurate and well-informed.\n","\n","##  **FLARE's Approach:**\n","- Starts by answering a question.\n","\n","- Looks up relevant documents when hitting \"uncertain\" tokens (based on log probs).\n","\n","- Uses these documents to keep generating.\n","\n","- Repeats the process until done.\n","\n","- Highlights uncertain tokens and forms questions for LLM to find answers.\n","\n","\n","\n","\n","<img src=\"https://raw.githubusercontent.com/jzbjyb/FLARE/main/res/flare.gif\">\n","\n","##  **How FLARE Stands Out from Traditional RAG:**\n","\n","- Traditional RAG fetches info once based on the input, then generates an output.\n","\n","- Can be restrictive for long texts.\n","\n","- FLARE keeps deciding **when and what to fetch** during the generation.\n","\n","- Predicts the upcoming sentence to guide future content queries.\n","\n","- If a sentence has low-confidence tokens, it fetches relevant info and rewrites the sentence.\n","\n","\n","###  **FLARE Prompt Structure:**\n","\n","- Guides language models to create search queries during answer generation.\n","\n","- For example: \"The colors on the flag of Ghana mean... Red stands for [Search(Ghana flag red meaning)] the blood of martyrs...\"\n","\n","###  **Direct FLARE Method:**\n","\n","- Uses the next sentence generated by the language model for retrieval decisions.\n","\n","- Accepts the sentence if the model is confident.\n","\n","- If unsure, transforms the sentence into search queries to get more info, then revises the sentence.\n","\n","###  **Setting Up the FLARE Chain:**\n","\n","1. **LLM for Answers**: Needs to return log probabilities to spot uncertain tokens. Recommended: OpenAI wrapper (not ChatOpenAI wrapper, as it doesn't return logprobs).\n","\n","2. **LLM for Hypothetical Questions**: Can be any model. In this case, we use ChatOpenAI for its speed and cost-effectiveness.\n","\n","3. **Retriever**: Any retriever works. Here, the SERPER search engine is used for cost efficiency.\n","\n","####  **Key Parameters in FLARE:**\n","\n","1. **`max_generation_len`**: Caps the number of tokens generated before checking for uncertainties.\n","\n","2. **`min_prob`**: Tokens generated with a probability below this threshold are marked as uncertain."],"metadata":{"id":"BvaIosUctu5H"}},{"cell_type":"code","source":["from langchain_community.document_loaders import ArxivLoader\n","from langchain_community.vectorstores import FAISS\n","\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_openai import ChatOpenAI\n","\n","from langchain.chains import FlareChain\n","\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","from langchain.callbacks.manager import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n","\n","# We set this so we can see what exactly is going on\n","from langchain.globals import set_verbose\n","\n","set_verbose(True)"],"metadata":{"id":"mEoXA-kQpdGz","executionInfo":{"status":"ok","timestamp":1706853695804,"user_tz":360,"elapsed":2903,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Same pattern as we saw in the RAG lecture."],"metadata":{"id":"KoNiUTL8ahup"}},{"cell_type":"code","source":["# instantiate llm\n","llm = ChatOpenAI(model=\"gpt-4-0125-preview\")\n","\n","#instantiate embeddings model\n","embeddings = OpenAIEmbeddings()\n","\n","# fetch docs from arxiv, in this case it's the FLARE paper\n","docs = ArxivLoader(query=\"2305.06983\", load_max_docs=2).load()\n","\n","# instantiate text splitter\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=16)\n","\n","# split the document into chunks\n","doc_chunks = text_splitter.split_documents(docs)\n","\n","# create vector store consisting of embeddigs for document\n","vector_store = FAISS.from_documents(doc_chunks, embedding=embeddings)\n","\n","vector_store_retriever = vector_store.as_retriever()"],"metadata":{"id":"QA126pbpu0UK","executionInfo":{"status":"ok","timestamp":1706853732867,"user_tz":360,"elapsed":6238,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Set up the FLARE chain"],"metadata":{"id":"xtSrDBBSa0i5"}},{"cell_type":"code","source":["flare = FlareChain.from_llm(\n","    llm=llm,\n","    retriever=vector_store_retriever,\n","    max_generation_len=1024,\n","    min_prob=0.7,\n","    verbose=True,\n","    handle_parsing_errors = True\n",")"],"metadata":{"id":"4DZGInhUwgp_","executionInfo":{"status":"ok","timestamp":1706853752284,"user_tz":360,"elapsed":533,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Start generation"],"metadata":{"id":"OE-f2-xma2q0"}},{"cell_type":"code","source":["flare.invoke(\"What is FLARE?\")"],"metadata":{"id":"DQ1oAcawL3RM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706853871805,"user_tz":360,"elapsed":8074,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"d6bf7f8e-c389-4d2d-be3f-1b5b7a5c1d37"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new FlareChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What is FLARE?\n",">>> EXISTING PARTIAL RESPONSE:  \n","FLARE stands for Fast Light Attack and Reconnaissance Aircraft. It is a type of military aircraft designed for quick response and reconnaissance missions. It is often used for close air support, surveillance, and intelligence gathering.\n","\n","The question to which the answer is the term/entity/phrase \" stands for Fast Light Attack and Reconnaissance\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What is FLARE?\n",">>> EXISTING PARTIAL RESPONSE:  \n","FLARE stands for Fast Light Attack and Reconnaissance Aircraft. It is a type of military aircraft designed for quick response and reconnaissance missions. It is often used for close air support, surveillance, and intelligence gathering.\n","\n","The question to which the answer is the term/entity/phrase \" type of military aircraft designed for quick response and reconnaissance missions. It is often used for close air support, surveillance, and intelligence gathering.\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What does FLARE stand for in the context of military aircraft?', 'What kind of missions is a Fast Light Attack and Reconnaissance Aircraft (FLARE) designed for, and how is it typically utilized?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: 3\n","FLARE: Forward-Looking Active\n","REtrieval Augmented Generation\n","Our intuition is that (1) LMs should only retrieve\n","information when they do not have the necessary\n","knowledge to avoid unnecessary or inappropriate\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","3\n","FLARE: Forward-Looking Active\n","REtrieval Augmented Generation\n","Our intuition is that (1) LMs should only retrieve\n","information when they do not have the necessary\n","knowledge to avoid unnecessary or inappropriate\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tive retrieval augmented generation, methods\n","that actively decide when and what to retrieve\n","across the course of the generation. We propose\n","Forward-Looking Active REtrieval augmented\n","generation (FLARE), a generic method which\n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response:  FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets.\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE:  FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What is FLARE?\n",">>> RESPONSE: FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'user_input': 'What is FLARE?',\n"," 'response': 'FLARE is a method for forward-looking active retrieval augmented generation, which actively decides when and what to retrieve during the generation process. It has been shown to achieve superior or competitive performance on various long-form knowledge-intensive generation tasks and datasets. '}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["flare.invoke(\"What are the steps in a FLARE pipeline?\")"],"metadata":{"id":"KvaLunCFL4ug","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706854258903,"user_tz":360,"elapsed":354683,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"5073742a-00f7-4caa-a081-44c8f6d678d0"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new FlareChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" include data preprocessing, feature\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" model evaluation. Data preprocessing involves\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" formatting the data to make it suitable for analysis. Feature\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" and transforming relevant features from\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" using the extracted features to\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" Finally, model\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" trained model on a separate dataset to assess\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" Is there anything specific you would\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE:  \n","The steps in a FLARE pipeline typically include data preprocessing, feature extraction, model training, and model evaluation. Data preprocessing involves cleaning and formatting the data to make it suitable for analysis. Feature extraction involves selecting and transforming relevant features from the data. Model training involves using the extracted features to train a machine learning model. Finally, model evaluation involves testing the trained model on a separate dataset to assess its performance. Is there anything specific you would like to know about these steps?\n","\n","The question to which the answer is the term/entity/phrase \" these steps?\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are you referring to when you mention the sequence of procedures followed within a FLARE pipeline?', 'What are some of the initial steps involved in a FLARE pipeline?', 'What step follows model training in a FLARE pipeline, and what is the initial step in this process?', 'What is involved in the initial stages of data preprocessing in a FLARE pipeline, particularly after cleaning the data?', 'What does the feature extraction step in a FLARE pipeline involve in terms of handling the data?', 'What does the model training step in a FLARE pipeline involve?', 'What step in the FLARE pipeline involves testing the trained machine learning model on a separate dataset to assess its performance?', 'What is involved in the model evaluation step of a FLARE pipeline?', 'After detailing the general steps involved in a FLARE pipeline, what question could be asked to invite further inquiries on specific aspects of the process?', 'What details would you like to learn more about regarding the processes involved in a FLARE pipeline?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","and UniEval (Zhong et al., 2022) which measures\n","factual consistency.\n","6\n","Experimental Results\n","We first report overall results across 4 tasks/datasets\n","and compare the performance of FLARE with all\n","the baselines introduced in section 4. We then\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","Question decomposition approaches require task-\n","specific prompt engineering, which restricts their\n","generalizability in new tasks.\n","5\n","Experimental Setup\n","We evaluate the effectiveness of FLARE on 4 di-\n","verse knowledge-intensive tasks using few-shot in-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response:  FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training.\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE:  FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training.\u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training.  FINISHED\n","\n","The question to which the answer is the term/entity/phrase \" FINISHED\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What is the status of the FLARE pipeline once it has completed its iterative generation process and no further sentences are being generated?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE:  FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are you detailing when you describe the sequence of actions taken by FLARE starting from using user input for document retrieval to the iterative sentence generation and improvement process, including the potential enhancement through a browser and applicability to existing LMs?', 'What does the initial step of the FLARE pipeline process entail?', 'What does FLARE start with using the user input x for, in order to bootstrap the iterative generation process?', 'What is the step that follows the initial generation of the first sentence in a FLARE pipeline?', 'What triggers the retrieval of documents during the sentence generation phase in a FLARE pipeline?', 'What marks the point at which the iterative sentence generation and document retrieval process in a FLARE pipeline concludes?', 'At what stage can FLARE be applied to existing language models to enhance their performance without requiring further training?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are you describing when you outline the sequence of actions or operations involved in the FLARE pipeline?', 'What does the beginning of the FLARE pipeline specifically entail in terms of processing the initial input?', 'What does the FLARE pipeline initially use the user input for?', 'What happens after the first sentence is generated in a FLARE pipeline?', 'What determines whether FLARE retrieves documents to regenerate the next sentence during its iterative generation process?', 'What marks the completion point of the iterative sentence generation and checking process in a FLARE pipeline?', 'At what stage can FLARE be utilized with existing language models without requiring further training?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","Retriever\n","$\n","%$\n","%%\n","Figure 2: An illustration of forward-looking active re-\n","trieval augmented generation with retrieval instructions\n","(FLAREinstruct). It iteratively generates search queries\n","(shown in gray italic) to retrieve relevant information to\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What do you need to outline to understand the sequence of actions or processes involved in the FLARE pipeline?', 'What does the initial step in a FLARE pipeline entail?', 'What does FLARE use the user input for in its initial step to begin the iterative generation process?', 'After generating the first sentence in a FLARE pipeline, what is the next step in the iterative generation process?', 'What triggers the retrieval of documents to regenerate the next sentence in a FLARE pipeline?', 'What marks the completion point of the iterative generation process in a FLARE pipeline?', 'At what stage can FLARE be combined with any existing language models without the need for additional training?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","Retriever\n","$\n","%$\n","%%\n","Figure 2: An illustration of forward-looking active re-\n","trieval augmented generation with retrieval instructions\n","(FLAREinstruct). It iteratively generates search queries\n","(shown in gray italic) to retrieve relevant information to\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are you describing when you outline the sequence involving the use of user input for document retrieval, sentence generation with checks for low-probability tokens, and the potential integration with a browser to enhance retrieval quality in the context of FLARE?', 'What does the initial step of a FLARE pipeline entail?', 'What does FLARE use the user input for initially in its pipeline?', 'After generating the first sentence in a FLARE pipeline, what is the next step in the iterative generation process?', 'What triggers the retrieval of documents to regenerate the next sentence in the FLARE pipeline?', 'What marks the point at which the iterative sentence generation and document retrieval process in a FLARE pipeline is concluded?', 'At which stage can FLARE be used with existing language models without requiring additional training?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What part of the FLARE pipeline description outlines the sequence of operations or actions taken?', 'What does the initial step of the FLARE pipeline specifically entail?', 'What does the FLARE pipeline use the user input for initially to bootstrap the iterative generation process?', 'After generating the first sentence in a FLARE pipeline, what is the next step in the iterative generation process?', 'What triggers the retrieval of relevant documents during the sentence generation process in a FLARE pipeline?', 'What marks the completion point of the iterative sentence generation process in a FLARE pipeline?', 'What can FLARE be used with without requiring additional training to enhance its performance during the generation process?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What does the process of FLARE involve from start to finish?', 'What does the initial step of the FLARE pipeline specifically entail?', 'What does the FLARE pipeline initially use the user input for?', 'What is the step that follows the generation of the first sentence in a FLARE pipeline?', 'What triggers the retrieval of new documents during the sentence generation in a FLARE pipeline?', 'What marks the point at which the iterative generation process stops in a FLARE pipeline?', 'What can FLARE be used with without requiring additional training, specifically during the stage when models are applied to new data?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","Retrieved\n","documents\n","LM\n","Generation\n","$\n","%$\n","%%\n","Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","next search query is generated or reaches the end.\n","Additional implementation details are included in\n","Appendix A.\n","3.2\n","Direct FLARE\n","Since we cannot fine-tune black-box LMs, we\n","found queries generated by FLAREinstruct through\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are you describing when you outline the process involving initial query retrieval, sentence generation, and iterative improvement in the context of FLARE?', 'What does the initial step of the FLARE pipeline encompass?', 'What does the FLARE pipeline initially use the user input for?', 'What immediately follows the generation of the first sentence in a FLARE pipeline?', 'What triggers the retrieval of new documents during the sentence generation process in a FLARE pipeline?', 'What marks the termination point of the iterative generation process in a FLARE pipeline?', 'At what stage can FLARE be used with any existing language models without the need for additional training?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tasks, we examine the effectiveness of our meth-\n","ods on text-davinci-003. We evaluate FLARE\n","on 4 diverse tasks/datasets involving generating\n","long outputs, including multihop QA (2WikiMul-\n","tihopQA), commonsense reasoning (StrategyQA),\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","reported in Figure 4. FLARE outperforms all base-\n","line on all tasks/datasets, indicating that FLARE\n","is a generic method that can effectively retrieve\n","additional information throughout the generation.\n","Among various tasks, multihop QA shows the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","next search query is generated or reaches the end.\n","Additional implementation details are included in\n","Appendix A.\n","3.2\n","Direct FLARE\n","Since we cannot fine-tune black-box LMs, we\n","found queries generated by FLAREinstruct through\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[36;1m\u001b[1;3mCurrent Response: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: \n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\n","\u001b[1m> Entering new QuestionGeneratorChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \"The steps in\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" involve using the user input as the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" documents and generate the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" Then, the next sentence is\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" any are present, relevant\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" end of the\" is:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven a user input and an existing partial response as context, ask a question to which the answer is the given term/entity/phrase:\n","\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> EXISTING PARTIAL RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \n","\n","The steps in a FLARE pipeline involve using the user input as the initial query to retrieve documents and generate the first sentence. Then, the next sentence is generated and checked for low-probability tokens. If any are present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can also be combined with a browser to potentially improve retrieval quality and can be used with any existing LMs at inference time without additional training.\n","\n","The question to which the answer is the term/entity/phrase \" LMs at inference time\" is:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\u001b[33;1m\u001b[1;3mGenerated Questions: ['What are you describing when you outline the sequence of actions taken from the initial query using user input to the iterative generation and checking process in a FLARE pipeline?', 'What does the initial step of a FLARE pipeline entail?', 'What is the first action taken in the FLARE pipeline after using the user input as the initial query?', 'After generating the first sentence in a FLARE pipeline, what happens next in the sequence?', 'What triggers the retrieval of new documents during the iterative sentence generation process in a FLARE pipeline?', 'At what point does the iterative sentence generation and checking process in a FLARE pipeline conclude?', 'At what stage can FLARE be used with any existing language models without requiring additional training?']\u001b[0m\n","\n","\n","\u001b[1m> Entering new _OpenAIResponseChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mRespond to the user message using any relevant context. If context is provided, you should ground your answer in that context. Once you're done responding return FINISHED.\n","\n",">>> CONTEXT: FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","Iterative re-\n","trieval and refinement has been studied in both\n","text and code generation tasks (Peng et al., 2023;\n","Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu\n","et al., 2023). FLARE differs from these methods in\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","to retrieve relevant information, promptly remove\n","it from the generation, and generate the next few\n","tokens while forbidding [ by adding a large\n","negative value to the logit of [.\n","The initial query of FLARE.\n","FLARE starts\n","\n","next search query is generated or reaches the end.\n","Additional implementation details are included in\n","Appendix A.\n","3.2\n","Direct FLARE\n","Since we cannot fine-tune black-box LMs, we\n","found queries generated by FLAREinstruct through\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","Looking Active REtrieval augmented generation\n","(FLARE), as illustrated in Figure 1. FLARE iter-\n","atively generates a temporary next sentence, use\n","it as the query to retrieve relevant documents if it\n","contains low-probability tokens and regenerate the\n","\n","FLARE starts\n","with the user input x as the initial query to re-\n","trieve documents to generate the first sentence\n","s1 = LM([Dx, x]) to bootstrap the iterative gener-\n","ation process. For the following steps, the tempo-\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\n","italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\n","\n","as FLAREdirect, which iteratively generates the next\n","sentence to gain insight into the future topic, and\n","if uncertain tokens are present, retrieves relevant\n","documents to regenerate the next sentence.\n","3.1\n","FLARE with Retrieval Instructions\n","\n","iteratively uses a prediction of the upcoming\n","sentence to anticipate future content, which is\n","then utilized as a query to retrieve relevant doc-\n","uments to regenerate the sentence if it contains\n","low-confidence tokens. We test FLARE along\n","\n","next sentence until reaches the end.\n","FLARE is applicable to any existing LMs at\n","inference time without additional training. Con-\n","sidering the impressive performance achieved by\n","GPT-3.5 (Ouyang et al., 2022) on a variety of\n","\n","using reinforcement learning or supervised train-\n","ing where multiple queries can be triggered before\n","generation. FLARE is built on text-based retrievers\n","but can be combined with a browser to potentially\n","improve retrieval quality.\n","8\n","Conclusion\n","\n","tent of future generations. FLARE outperforms\n","this baseline, indicating that manual exemplar an-\n","notation is not necessary for effective future-aware\n","retrieval. The gap between FLAREinstruct and ques-\n","tion decomposition is large, indicating that teaching\n","\n","with baselines comprehensively over 4 long-\n","form knowledge-intensive generation tasks/-\n","datasets. FLARE achieves superior or compet-\n","itive performance on all tasks, demonstrating\n","the effectiveness of our method.1\n","1\n","Introduction\n",">>> USER INPUT: What are the steps in a FLARE pipeline?\n",">>> RESPONSE: FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. \u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'user_input': 'What are the steps in a FLARE pipeline?',\n"," 'response': 'FLARE starts with the user input x as the initial query to retrieve documents to generate the first sentence s1 = LM([Dx, x]) to bootstrap the iterative generation process. For the following steps, the tempo-rary next sentence is generated and checked for low-probability tokens. If present, relevant documents are retrieved to regenerate the next sentence. This process continues until the end of the generation. FLARE can be combined with a browser to potentially improve retrieval quality and is applicable to any existing LMs at inference time without additional training. '}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["flare.invoke(\"What does a typical retrieval instruction look like in FLARE?\")"],"metadata":{"id":"a9K3HpCxMIJY"},"execution_count":null,"outputs":[]}]}