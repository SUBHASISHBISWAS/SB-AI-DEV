{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMMn/oKGGFErJ8fiFkcVqhP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jbtKCUddukKp"},"outputs":[],"source":["%%capture\n","!pip install langchain==0.1.1 openai==1.8.0 langchain-openai datasets faiss-gpu sentence_transformers"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"id":"lYso775DvQdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Self-Consistency Prompting in Language Models\n","\n","Self-consistency prompting was introduced in the March 2022 paper [\"Self-Consistency Improves Chain of Thought Reasoning in Language Models\"](https://arxiv.org/pdf/2203.11171.pdf) by Xuezhi Wang, et. al.\n","\n","### **What is Self-Consistency Prompting?**\n","The idea behind self-consistency is that complex reasoning problems often have multiple valid ways of thinking that lead to the same correct answer.\n","\n","By exploring these diverse reasoning paths, one can achieve a more reliable and consistent answer.\n","\n","### **How is it Different from Other Prompting Techniques?**\n","\n","- Traditional chain-of-thought prompting prompts a language model to generate a series of short sentences that mimic the reasoning process a person might use to solve a task.\n","\n","- Self-consistency, instead of just taking the most probable reasoning path, samples multiple paths and then determines the most consistent answer among them.\n","\n","- Unlike other methods that require additional training, human annotations, or auxiliary models, self-consistency is unsupervised and works directly with pre-trained language models.\n","\n","### **How to Construct a Prompt Using Self-Consistency?**\n","\n","1. **Chain-of-Thought Prompting**: Start by prompting the language model using chain-of-thought prompting.\n","\n","2. **Sampling Diverse Reasoning Paths**: Instead of greedily decoding the optimal reasoning path, use a \"sample-and-marginalize\" decoding procedure. This involves sampling from the language model's decoder to generate a diverse set of reasoning paths.\n","\n","3. **Marginalizing Reasoning Paths**: Each reasoning path might lead to a different final answer. Determine the optimal answer by marginalizing out the sampled reasoning paths to find the most consistent answer in the final answer set.\n","\n","### **Examples**:\n","- **Chain-of-Thought**: For the question \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\", instead of directly answering \"5\", the model might respond with the reasoning: \"There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\"\n","\n","- **Self-Consistency**: For a question about how much Janet makes from selling eggs, the model might generate multiple reasoning paths like:\n","  1. \"She has 16 - 3 - 4 = 9 eggs left. So she makes $2 * 9 = $18 per day.\"\n","\n","  2. \"This means she sells the remainder for $2 * (16 - 4 - 3) = $26 per day.\"\n","  \n","  3. \"She eats 3 for breakfast, so she has 16 - 3 = 13 left. Then she bakes muffins, so she has 13 - 4 = 9 eggs left. So she has 9 eggs * $2 = $18.\"\n","\n","  The model then aggregates these paths to determine the most consistent answer, which in this case is \"$18 per day.\"\n","\n","Self-consistency prompting improves the reasoning capabilities of language models by exploring diverse reasoning paths and selecting the most consistent answer.\n","\n","This method has shown significant performance boosts on various arithmetic and commonsense reasoning benchmarks.\n","\n","# Begin by setting up CoT prompts\n","\n","We're following the same pattern from the Chain of Thought lesson.\n","\n","1. Downloading CoT prompt datasets from HuggingFace\n","\n","2. Downloading embeddings model from HuggingFace\n","\n","3. Creating prompt template for CoT\n","\n","4. Creating an example selector\n","\n","5. Construct the prompt"],"metadata":{"id":"M-za_e1Ovs8O"}},{"cell_type":"code","source":["%%capture\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"kaist-ai/CoT-Collection\", split=\"train\", trust_remote_code=True)\n","\n","dataset = dataset.remove_columns(['task', 'type'])\n","\n","dataset = dataset.shuffle(seed=42)\n","\n","dataset = dataset.select(range(10_000))\n","\n","dataset = dataset.to_pandas()\n","\n","selected_examples = dataset.to_dict(orient='records')"],"metadata":{"id":"hQTIOjPeVTCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","\n","model_name = \"BAAI/bge-base-en-v1.5\"\n","\n","encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n","\n","embeddings = HuggingFaceBgeEmbeddings(\n","    model_name=model_name,\n","    model_kwargs={'device': 'cuda'},\n","    encode_kwargs=encode_kwargs\n",")"],"metadata":{"id":"fZW_bkcHWsIj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n","from langchain_community.vectorstores import FAISS\n","from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n","\n","prefix = \"Consider the following as examples of how to reason:\"\n","\n","examples_template = \"\"\"Query: {source}\n","\n","Rationale: {rationale}\n","\n","Response: {target}\n","\"\"\"\n","\n","suffix = \"\"\"Using a similar reasoning approach, answer the users question which is delimited by triple backticks.\n","\n","User question: ```{input}```\n","\n","Take a deep breath, break down the user's query step-by-step, and provide a clear chain of thought in your response.\"\n","\"\"\"\n","\n","examples_prompt = PromptTemplate(\n","    input_variables=[\"source\", \"rationale\", \"target\"],\n","    template=examples_template\n",")"],"metadata":{"id":"OU_F3tjCWtrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n","    selected_examples,\n","    embeddings,\n","    FAISS,\n","    k=5,\n",")\n","\n","mmr_prompt = FewShotPromptTemplate(\n","    example_selector=example_selector,\n","    example_prompt=examples_prompt,\n","    prefix=prefix,\n","    suffix=suffix,\n","    input_variables=[\"input\"]\n",")"],"metadata":{"id":"OrBTG72YXnF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"\"\"There were nine computers in the server room. Five more computers were installed each day, from\n","monday to thursday. How many computers are now in the server room?\n","\"\"\"\n","prompt = mmr_prompt.format(input=query)"],"metadata":{"id":"Uk_MsWVrYyAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prompt)"],"metadata":{"id":"Tfa1Joqbi0ig"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Self-Consistency Prompt"],"metadata":{"id":"Gpf4qBTGayyu"}},{"cell_type":"code","source":["sc_template = \"\"\"Based on the responses (delimited by < >) to the following query, \\\n","(delimited by triple backticks) return the response that occurs most frequently.\n","\n","Query: ```{query}```\n","\n","Responses: <{responses}>\n","\"\"\"\n","\n","sc_prompt = PromptTemplate.from_template(sc_template)"],"metadata":{"id":"AYl6DmB2ay3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating multiple responses\n","\n","\n","Use the `n` parameter to generate alternative responses. Increase `n` to explore different variations.\n","\n"],"metadata":{"id":"0HiZ4ruMXtVx"}},{"cell_type":"code","source":["from langchain_openai import OpenAI\n","\n","# we'll use the default model here, gpt-3.5-turbo-instruct\n","llm = OpenAI(n=5)\n","\n","generations = llm.generate([prompt])"],"metadata":{"id":"VE0qVJOKaXhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generations.generations"],"metadata":{"id":"etiUjc8GfA8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["responses = []\n","for item in generations.generations[0]:\n","    response_index = item.text.find(\"Response: \")\n","    if response_index != -1:\n","        response = item.text[response_index + len(\"Response: \"):].strip()\n","        responses.append(response)"],"metadata":{"id":"pSww4BDffMLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["responses"],"metadata":{"id":"DhXPkzdggX0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = OpenAI()\n","\n","final_prompt = sc_prompt.format(query=query, responses=str(responses))\n","\n","print(final_prompt)"],"metadata":{"id":"6mNnbPddhGK6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(llm.invoke(final_prompt))"],"metadata":{"id":"gjT78-f5jOk8"},"execution_count":null,"outputs":[]}]}