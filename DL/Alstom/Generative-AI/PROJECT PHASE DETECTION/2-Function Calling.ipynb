{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:03:51.197291Z",
     "start_time": "2024-10-09T07:03:51.165571Z"
    }
   },
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import pathlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "import jinja2\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 286
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:03:52.837633Z",
     "start_time": "2024-10-09T07:03:52.826036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomLangChainEmbedding(Embeddings):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", use_gpu=False):\n",
    "        \"\"\"\n",
    "        Initialize the embedding class with a specific transformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained transformer model.\n",
    "            use_gpu (bool): If True, use GPU (CUDA) for inference; otherwise, use CPU.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,clean_up_tokenization_spaces=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Use GPU if available and requested\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to compute sentence embeddings from token embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element is token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    \n",
    "        \n",
    "    def encode_data(self, sentences):\n",
    "        \"\"\"\n",
    "        \n",
    "        In summary, the encode_data method tokenizes the input sentences, computes their embeddings using a pre-trained transformer model, \n",
    "        normalizes the embeddings, and returns them as a NumPy array\n",
    "         \n",
    "        Encode the input sentences into sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentences (list of str): List of sentences to encode.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Sentence embeddings as a numpy array.\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "                                         \n",
    "        \n",
    "        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "        return torch.squeeze(sentence_embeddings).numpy() # Convert to numpy for FAISS or other downstream tasks\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        LangChain-compatible method to create embeddings for documents.\n",
    "        \n",
    "        Args:\n",
    "            texts (list of str): List of documents (text) to create embeddings for.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Document embeddings as numpy arrays.\n",
    "        \"\"\"\n",
    "        return self.encode_data(texts)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        LangChain-compatible method to create embedding for a single query.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Query to create embedding for.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Query embedding as a numpy array.\n",
    "        \"\"\"\n",
    "        return self.encode_data(text)"
   ],
   "id": "65d6b4f562ea06f4",
   "outputs": [],
   "execution_count": 288
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:03:52.904383Z",
     "start_time": "2024-10-09T07:03:52.853449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FaissIndexManager:\n",
    "    def __init__(self, embedding, index_path=\"faiss_index\"):\n",
    "        self.embedding = embedding\n",
    "        self.index_path = index_path\n",
    "        self.vector_store = self.load_faiss_index()\n",
    "    \n",
    "    # Function to save the FAISS index to disk\n",
    "    def save_faiss_index(self):\n",
    "        os.makedirs(self.index_path, exist_ok=True)\n",
    "        self.vector_store.save_local(self.index_path)\n",
    "        print(f\"FAISS index and metadata saved to {self.index_path}\")\n",
    "    \n",
    "    # Function to load FAISS index from disk\n",
    "    def load_faiss_index(self):\n",
    "        index_file = os.path.join(self.index_path, \"index.faiss\")\n",
    "        pkl_file = os.path.join(self.index_path, \"index.pkl\")\n",
    "        \n",
    "        if os.path.exists(index_file) and os.path.exists(pkl_file):\n",
    "            print(f\"Loading FAISS index and metadata from {self.index_path}\")\n",
    "            return FAISS.load_local(self.index_path, self.embedding, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(f\"No FAISS index found at {self.index_path}, creating a new one.\")\n",
    "            return None\n",
    "    \n",
    "    # Function to split a document into chunks\n",
    "    @staticmethod\n",
    "    def split_document_into_chunks(document, chunk_size=1000, chunk_overlap=200):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.create_documents([document.page_content])\n",
    "        return chunks\n",
    "    \n",
    "    # Function to generate a consistent document ID using a hash\n",
    "    @staticmethod\n",
    "    def generate_doc_id(content):\n",
    "        normalized_content = content.strip().lower()\n",
    "        return hashlib.sha256(normalized_content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Function to add a PDF document to the FAISS store\n",
    "    def add_pdf_to_faiss(self, pdf_path):\n",
    "        if self.vector_store is None:\n",
    "            # Load or create a new FAISS index\n",
    "            self.vector_store = self.load_faiss_index()\n",
    "\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        documents = pdf_loader.load()\n",
    "\n",
    "        new_documents = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Check for existing documents in vector store\n",
    "        existing_ids = set(\n",
    "            self.generate_doc_id(doc.page_content)\n",
    "            for doc_id, doc in self.vector_store.docstore._dict.items()\n",
    "        ) if self.vector_store is not None else set()\n",
    "\n",
    "        for document in documents:\n",
    "            chunks = self.split_document_into_chunks(document)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                doc_id = self.generate_doc_id(chunk.page_content)\n",
    "                if doc_id not in existing_ids:\n",
    "                    new_embedding = self.embedding.encode_data(chunk.page_content).reshape(1, -1)\n",
    "                    new_documents.append(Document(page_content=chunk.page_content, metadata={\"id\": doc_id}))\n",
    "                    print(f\"Embedding new document chunk with doc_id: {doc_id}\")\n",
    "                    embeddings_list.append(new_embedding)\n",
    "\n",
    "        # Debugging information\n",
    "        print(f\"Total new documents: {len(new_documents)}\")\n",
    "        print(f\"Total embeddings created: {len(embeddings_list)}\")\n",
    "\n",
    "        if new_documents:\n",
    "            if self.vector_store is None:\n",
    "                # Initialize FAISS index manually, passing in precomputed embeddings\n",
    "                self.vector_store = FAISS.from_documents(new_documents, self.embedding)\n",
    "                print(f\"Created new FAISS index for {pdf_path}.\")\n",
    "            else:\n",
    "                # Add the new documents and embeddings to the existing FAISS index\n",
    "                self.vector_store.add_documents(new_documents, embeddings=embeddings_list)\n",
    "                for idx, doc in enumerate(new_documents):\n",
    "                    self.vector_store.index_to_docstore_id[self.vector_store.index.ntotal - len(new_documents) + idx] = doc.metadata[\"id\"]\n",
    "                print(f\"Added {len(new_documents)} new chunks to FAISS index.\")\n",
    "        else:\n",
    "            print(\"No new chunks to add to FAISS.\")\n",
    "\n",
    "        # Save the updated FAISS index\n",
    "        self.save_faiss_index()\n",
    "        return self.vector_store\n",
    "    \n",
    "    # Function to add an Excel document to the FAISS store, using content from the 'description' column\n",
    "    def add_excel_to_faiss(self, excel_path, sheet_name=0):\n",
    "        if self.vector_store is None:\n",
    "            # Load or create a new FAISS index\n",
    "            self.vector_store = self.load_faiss_index()\n",
    "\n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "\n",
    "        # Make sure the 'description' column exists\n",
    "        if 'requirements' not in df.columns:\n",
    "            print(f\"The column 'requirements' was not found in the Excel file.\")\n",
    "            return\n",
    "\n",
    "        new_documents = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Check for existing documents in vector store\n",
    "        existing_ids = set(\n",
    "            self.generate_doc_id(doc.page_content)\n",
    "            for doc_id, doc in self.vector_store.docstore._dict.items()\n",
    "        ) if self.vector_store is not None else set()\n",
    "\n",
    "        # Iterate through the 'description' column and treat each cell as a document chunk\n",
    "        for _, row in df.iterrows():\n",
    "            content = str(row['requirements'])  # Extract content from the 'description' column\n",
    "\n",
    "            if pd.isna(content) or not content.strip():\n",
    "                continue  # Skip empty or NaN entries\n",
    "\n",
    "            doc_id = self.generate_doc_id(content)\n",
    "            if doc_id not in existing_ids:\n",
    "                # Split the content into chunks if necessary\n",
    "                chunks = self.split_document_into_chunks(Document(page_content=content))\n",
    "                for chunk in chunks:\n",
    "                    doc_id = self.generate_doc_id(chunk.page_content)\n",
    "                    if doc_id not in existing_ids:\n",
    "                        new_embedding = self.embedding.embed_documents(chunk.page_content)\n",
    "                        new_documents.append(Document(page_content=chunk.page_content, metadata={\"id\": doc_id}))\n",
    "                        print(f\"Embedding new document chunk with doc_id: {doc_id}\")\n",
    "                        embeddings_list.append(new_embedding)\n",
    "\n",
    "        # Debugging information\n",
    "        print(f\"Total new documents: {len(new_documents)}\")\n",
    "        print(f\"Total embeddings created: {len(embeddings_list)}\")\n",
    "\n",
    "        if new_documents:\n",
    "            if self.vector_store is None:\n",
    "                # Initialize FAISS index manually, passing in precomputed embeddings\n",
    "                self.vector_store = FAISS.from_documents(new_documents, self.embedding)\n",
    "                print(f\"Created new FAISS index for {excel_path}.\")\n",
    "            else:\n",
    "                # Add the new documents and embeddings to the existing FAISS index\n",
    "                self.vector_store.add_documents(new_documents, embeddings=embeddings_list)\n",
    "                for idx, doc in enumerate(new_documents):\n",
    "                    self.vector_store.index_to_docstore_id[self.vector_store.index.ntotal - len(new_documents) + idx] = doc.metadata[\"id\"]\n",
    "                print(f\"Added {len(new_documents)} new chunks to FAISS index.\")\n",
    "        else:\n",
    "            print(\"No new chunks to add to FAISS.\")\n",
    "\n",
    "        # Save the updated FAISS index\n",
    "        self.save_faiss_index()\n",
    "        return self.vector_store\n",
    "    \n",
    "    # Function to inspect the FAISS store\n",
    "    def inspect_faiss_store(self):\n",
    "        if self.vector_store is None:\n",
    "            print(\"FAISS store is empty or not loaded.\")\n",
    "            return\n",
    "        \n",
    "        # Check number of vectors stored\n",
    "        num_vectors = self.vector_store.index.ntotal\n",
    "        print(f\"Number of vectors stored: {num_vectors}\")\n",
    "        \n",
    "        # Check stored documents and metadata\n",
    "        print(\"Stored documents:\")\n",
    "        for doc_id, document in self.vector_store.docstore._dict.items():\n",
    "            print(f\"Document ID: {doc_id}\")\n",
    "            print(f\"Content: {document.page_content[:200]}\")  # Print first 200 characters of content\n",
    "            print(f\"Metadata: {document.metadata}\")\n",
    "        \n",
    "        # Retrieve and check stored embeddings\n",
    "        if num_vectors > 0:\n",
    "            for i in range(min(5, num_vectors)):  # Print embeddings of first 5 documents\n",
    "                vector = self.vector_store.index.reconstruct(i)\n",
    "                print(f\"Vector Shape: {vector.shape}...\")\n",
    "                print(f\"Embedding {i}: {vector[:10]}...\")  # Print first 10 dimensions of the embedding\n",
    "        else:\n",
    "            print(\"No embeddings stored.\")"
   ],
   "id": "5949f99d6c522a31",
   "outputs": [],
   "execution_count": 289
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T09:02:04.532345Z",
     "start_time": "2024-10-09T09:02:04.312432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding = CustomLangChainEmbedding(model_name=\"./Models/all-MiniLM-L6-v2\", use_gpu=False)\n",
    "faiss_manager = FaissIndexManager(embedding)\n",
    "vector_store=faiss_manager.add_excel_to_faiss(f\"{os.getcwd()}/Requirements/Phases.xlsx\")\n",
    "retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})"
   ],
   "id": "77acf8eff5251ed6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Loading FAISS index and metadata from faiss_index\n",
      "Total new documents: 0\n",
      "Total embeddings created: 0\n",
      "No new chunks to add to FAISS.\n",
      "FAISS index and metadata saved to faiss_index\n"
     ]
    }
   ],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T10:28:46.430503Z",
     "start_time": "2024-10-09T10:28:46.425595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    " \n",
    " #print(prompt_template.render(keyword=\"phases\"))   "
   ],
   "id": "a9efb2f76addc0c7",
   "outputs": [],
   "execution_count": 404
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T10:32:14.935916Z",
     "start_time": "2024-10-09T10:32:09.401029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AL_AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AL_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AL_OPENAI_API_VERSION\"))\n",
    "\n",
    "def get_context_for_phase_detection_with_keyword(keyword):\n",
    "    return retriever.invoke(f\"Find the {keyword}\")\n",
    "\n",
    "functions=[{\"name\": \"get_context_for_phase_detection_with_keyword\",\n",
    "                \"description\":\"get correct context from list of documents related to keyword for metro line\", \n",
    "                \"parameters\": \n",
    "                {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": \n",
    "                        {\n",
    "                        \"keyword\":{\"type\":\"string\",\"description\":\"get the keyword\"}\n",
    "                        },\n",
    "                    \"required\": [\"keyword\"],\n",
    "                },\n",
    "            }\n",
    "]\n",
    "\n",
    "path = pathlib.Path(f\"{os.getcwd()}/Templates/PhaseDetectionTemplate.jinja2\")\n",
    "with path.open() as f:\n",
    "    prompt_template = jinja2.Template(f.read())\n",
    "prompt = prompt_template.render(keyword=\"phases\")\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant to find out relevant information from given context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={'name': 'get_context_for_phase_detection_with_keyword'}\n",
    "    \n",
    ")\n",
    "available_functions={\"get_context_for_phase_detection_with_keyword\": get_context_for_phase_detection_with_keyword}\n",
    "\n",
    "response_message=response.choices[0].message\n",
    "function_name=response_message.function_call.name\n",
    "function_args=json.loads(response_message.function_call.arguments)\n",
    "\n",
    "function_to_call=available_functions.get(function_name)\n",
    "function_response=function_to_call(function_args.get(\"keyword\"))\n",
    "\n",
    "\n",
    "messages.append(response_message)\n",
    "messages.append({\"role\": \"function\", \"name\" : function_name, \"content\": f\"{function_response}\"})\n",
    "\n",
    "second_response = client.chat.completions.create(model=\"gpt-4o\",temperature=0,messages=messages)\n",
    "print(second_response.choices[0].message.content)"
   ],
   "id": "c731a23064444d15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the relevant phases mentioned are:\n",
      "\n",
      "1. **Phases of the Bhopal Metro**:\n",
      "   - From AIIMS to Karond Circle Station\n",
      "   - Including the Depot\n",
      "   - From Bhadbhada Square to Ratnagiri Tiraha\n",
      "\n",
      "2. **Phases of the Indore Metro**:\n",
      "   - All lines\n",
      "\n",
      "Additionally, the location of the common OCC (Operational Control Center) for the two phases of Bhopal Metro is at the Depot, and similarly, the location of the OCC for the Indore Metro is at its Depot.\n"
     ]
    }
   ],
   "execution_count": 406
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8e031e32a0e24e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
