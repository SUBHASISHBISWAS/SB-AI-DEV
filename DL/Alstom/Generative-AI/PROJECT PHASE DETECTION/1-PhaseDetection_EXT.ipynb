{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:42.768215Z",
     "start_time": "2024-10-09T11:25:42.647963Z"
    }
   },
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import  AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_TYPE']=os.getenv(\"AL_OPENAI_API_TYPE\")\n",
    "os.environ['OPENAI_API_VERSION']=os.getenv(\"AL_OPENAI_API_VERSION\")\n",
    "os.environ['AZURE_OPENAI_ENDPOINT']=os.getenv(\"AL_AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"AL_OPENAI_API_KEY\")\n",
    "os.environ['DEPLOYMENT_NAME']=os.getenv(\"AL_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"AL_LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:43.037002Z",
     "start_time": "2024-10-09T11:25:43.020465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomLangChainEmbedding(Embeddings):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", use_gpu=False):\n",
    "        \"\"\"\n",
    "        Initialize the embedding class with a specific transformer model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained transformer model.\n",
    "            use_gpu (bool): If True, use GPU (CUDA) for inference; otherwise, use CPU.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,clean_up_tokenization_spaces=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Use GPU if available and requested\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to compute sentence embeddings from token embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output[0]  # First element is token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    \n",
    "        \n",
    "    def encode_data(self, sentences):\n",
    "        \"\"\"\n",
    "        \n",
    "        In summary, the encode_data method tokenizes the input sentences, computes their embeddings using a pre-trained transformer model, \n",
    "        normalizes the embeddings, and returns them as a NumPy array\n",
    "         \n",
    "        Encode the input sentences into sentence embeddings.\n",
    "        \n",
    "        Args:\n",
    "            sentences (list of str): List of sentences to encode.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Sentence embeddings as a numpy array.\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "                                         \n",
    "        \n",
    "        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "        return torch.squeeze(sentence_embeddings).numpy() # Convert to numpy for FAISS or other downstream tasks\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        LangChain-compatible method to create embeddings for documents.\n",
    "        \n",
    "        Args:\n",
    "            texts (list of str): List of documents (text) to create embeddings for.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Document embeddings as numpy arrays.\n",
    "        \"\"\"\n",
    "        return self.encode_data(texts)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        LangChain-compatible method to create embedding for a single query.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Query to create embedding for.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Query embedding as a numpy array.\n",
    "        \"\"\"\n",
    "        return self.encode_data(text)"
   ],
   "id": "bf1505461f961cb4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:43.502605Z",
     "start_time": "2024-10-09T11:25:43.456819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class FaissIndexManager:\n",
    "    def __init__(self, embedding, index_path=\"faiss_index\"):\n",
    "        self.embedding = embedding\n",
    "        self.index_path = index_path\n",
    "        self.vector_store = self.load_faiss_index()\n",
    "    \n",
    "    # Function to save the FAISS index to disk\n",
    "    def save_faiss_index(self):\n",
    "        os.makedirs(self.index_path, exist_ok=True)\n",
    "        self.vector_store.save_local(self.index_path)\n",
    "        print(f\"FAISS index and metadata saved to {self.index_path}\")\n",
    "    \n",
    "    # Function to load FAISS index from disk\n",
    "    def load_faiss_index(self):\n",
    "        index_file = os.path.join(self.index_path, \"index.faiss\")\n",
    "        pkl_file = os.path.join(self.index_path, \"index.pkl\")\n",
    "        \n",
    "        if os.path.exists(index_file) and os.path.exists(pkl_file):\n",
    "            print(f\"Loading FAISS index and metadata from {self.index_path}\")\n",
    "            return FAISS.load_local(self.index_path, self.embedding, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(f\"No FAISS index found at {self.index_path}, creating a new one.\")\n",
    "            return None\n",
    "    \n",
    "    # Function to split a document into chunks\n",
    "    @staticmethod\n",
    "    def split_document_into_chunks(document, chunk_size=1000, chunk_overlap=200):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.create_documents([document.page_content])\n",
    "        return chunks\n",
    "    \n",
    "    # Function to generate a consistent document ID using a hash\n",
    "    @staticmethod\n",
    "    def generate_doc_id(content):\n",
    "        normalized_content = content.strip().lower()\n",
    "        return hashlib.sha256(normalized_content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Function to add a PDF document to the FAISS store\n",
    "    def add_pdf_to_faiss(self, pdf_path):\n",
    "        if self.vector_store is None:\n",
    "            # Load or create a new FAISS index\n",
    "            self.vector_store = self.load_faiss_index()\n",
    "\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        documents = pdf_loader.load()\n",
    "\n",
    "        new_documents = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Check for existing documents in vector store\n",
    "        existing_ids = set(\n",
    "            self.generate_doc_id(doc.page_content)\n",
    "            for doc_id, doc in self.vector_store.docstore._dict.items()\n",
    "        ) if self.vector_store is not None else set()\n",
    "\n",
    "        for document in documents:\n",
    "            chunks = self.split_document_into_chunks(document)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                doc_id = self.generate_doc_id(chunk.page_content)\n",
    "                if doc_id not in existing_ids:\n",
    "                    new_embedding = self.embedding.encode_data(chunk.page_content).reshape(1, -1)\n",
    "                    new_documents.append(Document(page_content=chunk.page_content, metadata={\"id\": doc_id}))\n",
    "                    print(f\"Embedding new document chunk with doc_id: {doc_id}\")\n",
    "                    embeddings_list.append(new_embedding)\n",
    "\n",
    "        # Debugging information\n",
    "        print(f\"Total new documents: {len(new_documents)}\")\n",
    "        print(f\"Total embeddings created: {len(embeddings_list)}\")\n",
    "\n",
    "        if new_documents:\n",
    "            if self.vector_store is None:\n",
    "                # Initialize FAISS index manually, passing in precomputed embeddings\n",
    "                self.vector_store = FAISS.from_documents(new_documents, self.embedding)\n",
    "                print(f\"Created new FAISS index for {pdf_path}.\")\n",
    "            else:\n",
    "                # Add the new documents and embeddings to the existing FAISS index\n",
    "                self.vector_store.add_documents(new_documents, embeddings=embeddings_list)\n",
    "                for idx, doc in enumerate(new_documents):\n",
    "                    self.vector_store.index_to_docstore_id[self.vector_store.index.ntotal - len(new_documents) + idx] = doc.metadata[\"id\"]\n",
    "                print(f\"Added {len(new_documents)} new chunks to FAISS index.\")\n",
    "        else:\n",
    "            print(\"No new chunks to add to FAISS.\")\n",
    "\n",
    "        # Save the updated FAISS index\n",
    "        self.save_faiss_index()\n",
    "        return self.vector_store\n",
    "    \n",
    "    # Function to add an Excel document to the FAISS store, using content from the 'description' column\n",
    "    def add_excel_to_faiss(self, excel_path, sheet_name=0):\n",
    "        if self.vector_store is None:\n",
    "            # Load or create a new FAISS index\n",
    "            self.vector_store = self.load_faiss_index()\n",
    "\n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "\n",
    "        # Make sure the 'description' column exists\n",
    "        if 'requirements' not in df.columns:\n",
    "            print(f\"The column 'requirements' was not found in the Excel file.\")\n",
    "            return\n",
    "\n",
    "        new_documents = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        # Check for existing documents in vector store\n",
    "        existing_ids = set(\n",
    "            self.generate_doc_id(doc.page_content)\n",
    "            for doc_id, doc in self.vector_store.docstore._dict.items()\n",
    "        ) if self.vector_store is not None else set()\n",
    "\n",
    "        # Iterate through the 'description' column and treat each cell as a document chunk\n",
    "        for _, row in df.iterrows():\n",
    "            content = str(row['requirements'])  # Extract content from the 'description' column\n",
    "\n",
    "            if pd.isna(content) or not content.strip():\n",
    "                continue  # Skip empty or NaN entries\n",
    "\n",
    "            doc_id = self.generate_doc_id(content)\n",
    "            if doc_id not in existing_ids:\n",
    "                # Split the content into chunks if necessary\n",
    "                chunks = self.split_document_into_chunks(Document(page_content=content))\n",
    "                for chunk in chunks:\n",
    "                    doc_id = self.generate_doc_id(chunk.page_content)\n",
    "                    if doc_id not in existing_ids:\n",
    "                        new_embedding = self.embedding.embed_documents(chunk.page_content)\n",
    "                        new_documents.append(Document(page_content=chunk.page_content, metadata={\"id\": doc_id}))\n",
    "                        print(f\"Embedding new document chunk with doc_id: {doc_id}\")\n",
    "                        embeddings_list.append(new_embedding)\n",
    "\n",
    "        # Debugging information\n",
    "        print(f\"Total new documents: {len(new_documents)}\")\n",
    "        print(f\"Total embeddings created: {len(embeddings_list)}\")\n",
    "\n",
    "        if new_documents:\n",
    "            if self.vector_store is None:\n",
    "                # Initialize FAISS index manually, passing in precomputed embeddings\n",
    "                self.vector_store = FAISS.from_documents(new_documents, self.embedding)\n",
    "                print(f\"Created new FAISS index for {excel_path}.\")\n",
    "            else:\n",
    "                # Add the new documents and embeddings to the existing FAISS index\n",
    "                self.vector_store.add_documents(new_documents, embeddings=embeddings_list)\n",
    "                for idx, doc in enumerate(new_documents):\n",
    "                    self.vector_store.index_to_docstore_id[self.vector_store.index.ntotal - len(new_documents) + idx] = doc.metadata[\"id\"]\n",
    "                print(f\"Added {len(new_documents)} new chunks to FAISS index.\")\n",
    "        else:\n",
    "            print(\"No new chunks to add to FAISS.\")\n",
    "\n",
    "        # Save the updated FAISS index\n",
    "        self.save_faiss_index()\n",
    "        return self.vector_store\n",
    "    \n",
    "    # Function to inspect the FAISS store\n",
    "    def inspect_faiss_store(self):\n",
    "        if self.vector_store is None:\n",
    "            print(\"FAISS store is empty or not loaded.\")\n",
    "            return\n",
    "        \n",
    "        # Check number of vectors stored\n",
    "        num_vectors = self.vector_store.index.ntotal\n",
    "        print(f\"Number of vectors stored: {num_vectors}\")\n",
    "        \n",
    "        # Check stored documents and metadata\n",
    "        print(\"Stored documents:\")\n",
    "        for doc_id, document in self.vector_store.docstore._dict.items():\n",
    "            print(f\"Document ID: {doc_id}\")\n",
    "            print(f\"Content: {document.page_content[:200]}\")  # Print first 200 characters of content\n",
    "            print(f\"Metadata: {document.metadata}\")\n",
    "        \n",
    "        # Retrieve and check stored embeddings\n",
    "        if num_vectors > 0:\n",
    "            for i in range(min(5, num_vectors)):  # Print embeddings of first 5 documents\n",
    "                vector = self.vector_store.index.reconstruct(i)\n",
    "                print(f\"Vector Shape: {vector.shape}...\")\n",
    "                print(f\"Embedding {i}: {vector[:10]}...\")  # Print first 10 dimensions of the embedding\n",
    "        else:\n",
    "            print(\"No embeddings stored.\")"
   ],
   "id": "61ffea0f2b6debc8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:44.161294Z",
     "start_time": "2024-10-09T11:25:43.960439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding = CustomLangChainEmbedding(model_name=\"./Models/all-MiniLM-L6-v2\", use_gpu=False)\n",
    "#embedding=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "llm=AzureChatOpenAI()\n",
    "faiss_manager = FaissIndexManager(embedding)"
   ],
   "id": "dbd2388bcff0a9b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n",
      "Loading FAISS index and metadata from faiss_index\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:45.518011Z",
     "start_time": "2024-10-09T11:25:45.428711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#vector_store=faiss_manager.add_pdf_to_faiss(\"./Requirements/SET MSA Schedule 23_updated.pdf\")\n",
    "vector_store=faiss_manager.add_excel_to_faiss(\"./Requirements/Phases.xlsx\")"
   ],
   "id": "dcdc5069d90c20ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new documents: 0\n",
      "Total embeddings created: 0\n",
      "No new chunks to add to FAISS.\n",
      "FAISS index and metadata saved to faiss_index\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:47.089824Z",
     "start_time": "2024-10-09T11:25:47.086036Z"
    }
   },
   "cell_type": "code",
   "source": "#faiss_manager.inspect_faiss_store()",
   "id": "3efff0363c05cb45",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:25:48.039788Z",
     "start_time": "2024-10-09T11:25:48.001277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})\n",
    "print(retriever.invoke(\"Find the phases\"))"
   ],
   "id": "568d79d4e1580db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'id': 'b247e928a1e452e7cae56718ade2d4f27c32c3f7ddb2931ce74dfabc622819c8'}, page_content='1.6.1 The Site for the Works shall be in Phases of the Bhopal Metro spread from AIIMS to Karond Circle Station including the Depot and Bhadbhada Square to Ratnagiri Tiraha and the Indore Metro all lines. The location of the common OCC for the two phases of Bhopal Metro shall be at Depot. Similarly, the location of the OCC for the Indore Metro shall be at its Depot. The access dates to the Site are referenced in the Key Date Schedules in GS Vol III, Appendix 1'), Document(metadata={'id': 'aa1699cb713b83d6bad7b76b7cac7d96a40eb19b10b22ed1204fc2e933fd547f'}, page_content='In this Schedule, in addition to the words and expressions given specific meanings in Clause 1 and Schedule 22 (Data Protection), the following words and expressions have the meanings given to them as follows:'), Document(metadata={'id': '4b65efd8feeaf10efbe14055cf045fe22eea0cc0db4637bd9421e14892b7a773'}, page_content='SCHEDULE 23 Cyber Security'), Document(metadata={'id': 'fc5426a3b88d65ce3673a053a808a7aab674da13cafdc053904cbc9c6a33abf8'}, page_content='sections [A35 to A40]53 of the Functional Specification (on an ongoing basis);'), Document(metadata={'id': '96835b8b4590763c0a3efaa5a62188bd311136de1f04ff49ee69e8f94f3695fd'}, page_content='CYBER SECURITY')]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T11:08:03.447637Z",
     "start_time": "2024-10-08T11:08:03.442143Z"
    }
   },
   "cell_type": "code",
   "source": [
    " \n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "d0b5b4856bd62682",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T11:08:04.722841Z",
     "start_time": "2024-10-08T11:08:04.719127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contextualize_q_prompt.pretty_print()\n",
    "\n",
    "qa_prompt.pretty_print()"
   ],
   "id": "87ebabc661c8c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m System Message \u001B[0m================================\n",
      "\n",
      "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\n",
      "\n",
      "=============================\u001B[1m Messages Placeholder \u001B[0m=============================\n",
      "\n",
      "\u001B[33;1m\u001B[1;3m{chat_history}\u001B[0m\n",
      "\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "\u001B[33;1m\u001B[1;3m{input}\u001B[0m\n",
      "================================\u001B[1m System Message \u001B[0m================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "\n",
      "\u001B[33;1m\u001B[1;3m{context}\u001B[0m\n",
      "\n",
      "=============================\u001B[1m Messages Placeholder \u001B[0m=============================\n",
      "\n",
      "\u001B[33;1m\u001B[1;3m{chat_history}\u001B[0m\n",
      "\n",
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "\u001B[33;1m\u001B[1;3m{input}\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T11:08:19.565811Z",
     "start_time": "2024-10-08T11:08:19.560199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ],
   "id": "d32ea8ece0e314b1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T11:08:20.311803Z",
     "start_time": "2024-10-08T11:08:20.302552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if  session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "        \n",
    "with_message_history=RunnableWithMessageHistory(llm,get_session_history)"
   ],
   "id": "17832426a728e32c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T11:08:22.501504Z",
     "start_time": "2024-10-08T11:08:22.498295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "id": "e73060bb4226037b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:30:47.648459Z",
     "start_time": "2024-10-09T11:30:45.353340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
    "chat_history=[]\n",
    "\n",
    "\n",
    "question=\"get the phases mentioned in the document for metro line?\"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={ \"configurable\": {\"session_id\": \"session-1\"} },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# for chunk in conversational_rag_chain.stream({\"input\": question},\n",
    "#     config={ \"configurable\": {\"session_id\": \"session-1\"} },):\n",
    "#     print(chunk,end=\"\",Flush=True)\n",
    "\n",
    "print(response['answer'])\n"
   ],
   "id": "291c5cf630de3ed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phases mentioned in the document for the metro line are:\n",
      "\n",
      "1. AIIMS to Karond Circle Station including the Depot.\n",
      "2. Bhadbhada Square to Ratnagiri Tiraha.\n",
      "3. All lines of the Indore Metro.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:31:50.217872Z",
     "start_time": "2024-10-09T11:31:48.241659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question=\"so give chat history how many number phases are there in the document?\"\n",
    "response=conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\"configurable\": {\"session_id\": \"session-1\"}},\n",
    ")\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(response['answer'])"
   ],
   "id": "8fe236713b0f211c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there are three phases mentioned in the document.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:43:58.548414Z",
     "start_time": "2024-09-25T06:43:58.542527Z"
    }
   },
   "cell_type": "code",
   "source": "chat_history.clear()",
   "id": "c3eacdd9a4edbb8e",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:43:58.579115Z",
     "start_time": "2024-09-25T06:43:58.576536Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a0ae6d49ab564bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
