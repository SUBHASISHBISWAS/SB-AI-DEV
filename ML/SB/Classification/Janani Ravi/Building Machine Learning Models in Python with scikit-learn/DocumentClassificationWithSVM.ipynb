{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d4dce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " The data set that we'll use for this problem is available with the scikit-learn library. \n",
    " Scikit-learn contains a number of data sets that can be used to train and validate models. \n",
    " We'll use the fetch 20 newsgroups module to retrieve the data set. \n",
    " This contains roughly 20, 000 newsgroup documents which are split across 20 newsgroups.\n",
    " \n",
    " Each one of these 20 newsgroups corresponds to a particular topic. \n",
    " The return value from this function is a dictionary and these are the keys within the dictionary. \n",
    "'''\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "tewnty_train=fetch_20newsgroups(subset='train',shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1f77a",
   "metadata": {},
   "source": [
    "http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08c5f9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The return value from this function is a dictionary and these are the keys within the dictionary. \n",
    "\n",
    "The data key is what contains our training data. These are our newsgroup documents. \n",
    "\n",
    "Target names are the newsgroups to which these documents belong.\n",
    "These are the labels or the y values associated with each document.\n",
    "'''\n",
    "tewnty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dd2de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here is a document sample from the training data set. \n",
    "This is an email related to cars. \n",
    "\n",
    "'''\n",
    "print(tewnty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65f43e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here are the 20 newsgroups to which these documents belong. \n",
    "They range from sports to computers to politics. \n",
    "'''\n",
    "tewnty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18d2cce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Our categorical variables need to be expressed in numeric form and that's what the target key holds.\n",
    "'''\n",
    "tewnty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7806d4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Every document is x data and and we will represent it in numeric form using the count vectorizer. \n",
    "\n",
    "We'll call the count vectorizers fit transform method on our training data. \n",
    "\n",
    "The output of the count vectorizer is a sparse matrix. \n",
    "\n",
    "Every word is identified uniquely using its document ID and its unique word ID \n",
    "and the frequency of the word in that document is specified.\n",
    "\n",
    "Here is the shape of our sparse matrix. \n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect=CountVectorizer()\n",
    "X_train_counts=count_vect.fit_transform(tewnty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23e489f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56979)\t3\n",
      "  (0, 75358)\t2\n",
      "  (0, 123162)\t2\n",
      "  (0, 118280)\t2\n",
      "  (0, 50527)\t2\n",
      "  (0, 124031)\t2\n",
      "  (0, 85354)\t1\n",
      "  (0, 114688)\t1\n",
      "  (0, 111322)\t1\n",
      "  (0, 123984)\t1\n",
      "  (0, 37780)\t5\n",
      "  (0, 68532)\t3\n",
      "  (0, 114731)\t5\n",
      "  (0, 87620)\t1\n",
      "  (0, 95162)\t1\n",
      "  (0, 64095)\t1\n",
      "  (0, 98949)\t1\n",
      "  (0, 90379)\t1\n",
      "  (0, 118983)\t1\n",
      "  (0, 89362)\t3\n",
      "  (0, 79666)\t1\n",
      "  (0, 40998)\t1\n",
      "  (0, 92081)\t1\n",
      "  (0, 76032)\t1\n",
      "  (0, 4605)\t1\n",
      "  :\t:\n",
      "  (0, 37565)\t1\n",
      "  (0, 113986)\t1\n",
      "  (0, 83256)\t1\n",
      "  (0, 86001)\t1\n",
      "  (0, 51730)\t1\n",
      "  (0, 109271)\t1\n",
      "  (0, 128026)\t1\n",
      "  (0, 96144)\t1\n",
      "  (0, 78784)\t1\n",
      "  (0, 63363)\t1\n",
      "  (0, 90252)\t1\n",
      "  (0, 123989)\t1\n",
      "  (0, 67156)\t1\n",
      "  (0, 128402)\t2\n",
      "  (0, 62221)\t1\n",
      "  (0, 57308)\t1\n",
      "  (0, 76722)\t1\n",
      "  (0, 94362)\t1\n",
      "  (0, 78955)\t1\n",
      "  (0, 114428)\t1\n",
      "  (0, 66098)\t1\n",
      "  (0, 35187)\t1\n",
      "  (0, 35983)\t1\n",
      "  (0, 128420)\t1\n",
      "  (0, 86580)\t1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You can explore the output of the count vectorizer on the very first document. \n",
    "Here you can see the document ID, the word ID and the associated frequency.\n",
    "'''\n",
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96749be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124031"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(\"where\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "834c1724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " I'm going to parse this output of the count vectorizer through a TfidfTransformer. \n",
    " \n",
    " The TfidfTransformer is different from the Tfidf vectorizer in one significant way. \n",
    " \n",
    " The Tfidf vectorizer worked directly on documents and produced a bag of words with corresponding Tfidf course. \n",
    " \n",
    " The TfidfTransformer on the other hand requires a bag of words as its input. \n",
    " \n",
    " That's why the output of the count vectorizer be passed into the TfidfTransformer. \n",
    " \n",
    " The count vectorizer plus the TfidfTransformer is equal to the tfidfvectorizer. \n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "X_train_tfidf=tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1c71366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 128420)\t0.04278499079283093\n",
      "  (0, 128402)\t0.05922294083277842\n",
      "  (0, 128026)\t0.060622095889758885\n",
      "  (0, 124931)\t0.08882569909852546\n",
      "  (0, 124031)\t0.10798795154169122\n",
      "  (0, 123989)\t0.08207027465330353\n",
      "  (0, 123984)\t0.036854292634593756\n",
      "  (0, 123796)\t0.049437556160455476\n",
      "  (0, 123292)\t0.14534718515938805\n",
      "  (0, 123162)\t0.2597090245735688\n",
      "  (0, 118983)\t0.037085978050619146\n",
      "  (0, 118280)\t0.2118680720828169\n",
      "  (0, 115475)\t0.042472629883573\n",
      "  (0, 114731)\t0.14447275512784058\n",
      "  (0, 114688)\t0.06214070986309586\n",
      "  (0, 114579)\t0.03671830826216751\n",
      "  (0, 114455)\t0.12287762616208957\n",
      "  (0, 114428)\t0.05511105154696676\n",
      "  (0, 113986)\t0.17691750674853082\n",
      "  (0, 111322)\t0.01915671802495043\n",
      "  (0, 109581)\t0.10809248404447917\n",
      "  (0, 109271)\t0.10844724822064673\n",
      "  (0, 108252)\t0.07526015712540636\n",
      "  (0, 106116)\t0.09869734624201922\n",
      "  (0, 104813)\t0.08462829788929047\n",
      "  :\t:\n",
      "  (0, 56979)\t0.057470154074851294\n",
      "  (0, 51793)\t0.13412921037839678\n",
      "  (0, 51730)\t0.09714744057976722\n",
      "  (0, 50527)\t0.054614286588587246\n",
      "  (0, 50111)\t0.08452530453354308\n",
      "  (0, 48620)\t0.11603642565244157\n",
      "  (0, 48618)\t0.10015015488700592\n",
      "  (0, 45295)\t0.06621689096551565\n",
      "  (0, 42876)\t0.04951998622750595\n",
      "  (0, 40998)\t0.0780136819691811\n",
      "  (0, 37780)\t0.38133891259493113\n",
      "  (0, 37565)\t0.03431760442478462\n",
      "  (0, 37433)\t0.06908779999621749\n",
      "  (0, 35983)\t0.03770448563619875\n",
      "  (0, 35612)\t0.1328075333935896\n",
      "  (0, 35187)\t0.09353930598317124\n",
      "  (0, 34995)\t0.16713176431412632\n",
      "  (0, 34181)\t0.08716420445779295\n",
      "  (0, 32311)\t0.029911858621703466\n",
      "  (0, 28615)\t0.10278592160069082\n",
      "  (0, 27436)\t0.037098931990947055\n",
      "  (0, 26073)\t0.09534869974107982\n",
      "  (0, 18299)\t0.138749083899155\n",
      "  (0, 16574)\t0.14155752531572685\n",
      "  (0, 4605)\t0.06332603952480323\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let's print this course for one document. \n",
    "Here you can see a mapping of document ID word ID and the corresponding tfidf score. \n",
    "'''\n",
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b4036c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(dual=False, tol=0.001)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Once we've set up our data correctly, using a support vector machine estimator is very easy. \n",
    "We instantiate the linear SVC and pass in a number of input arguments. \n",
    "\n",
    "The penalty function that we are going to use is the L2 norm penalty. \n",
    "\n",
    "Another criteria that we can specify is the tolerance for stopping training on our model. \n",
    "\n",
    "If the losses that we calculate on the objective function go below this tolerance value, \n",
    "we'll assume that our model is good enough and stop training. \n",
    "\n",
    "Notice that in order to prepare the data set, we performed a series of transformations on it. \n",
    "We first used the count vectorizer and then the TfidfTransformer and then pass that \n",
    "output to our linear SVC estimator.\n",
    "\n",
    "'''\n",
    "from sklearn.svm import LinearSVC\n",
    "clf_svc=LinearSVC(penalty='l2',dual=False,tol=1e-3)\n",
    "clf_svc.fit(X_train_tfidf,tewnty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d041a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', LinearSVC(dual=False, tol=0.001))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Scikit-learn provides a very handy tool called the pipeline which allows a linear sequence \n",
    "of data transformations to be chain. \n",
    "\n",
    "Notice that we instantiate a pipeline and within the pipeline we specified the series of \n",
    "transformations that we want performed. \n",
    "\n",
    "We can now simply execute this pipeline by calling the fit method and our training data will \n",
    "be passed through each of these steps. \n",
    "\n",
    "We don't need to call these steps individually. \n",
    "'''\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf_svc_pipeline=Pipeline([('vect',CountVectorizer()),\n",
    "                          ('tfidf',TfidfTransformer()),\n",
    "                          ('clf',LinearSVC(penalty=\"l2\",dual=False,tol=0.001))])\n",
    "clf_svc_pipeline.fit(tewnty_train.data,tewnty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "275fe655",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's go ahead and get the test data and we can pass this test data through the same pipeline. \n",
    "'''\n",
    "tewnty_test=fetch_20newsgroups(subset='test',shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d951bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The last step of the pipeline we'll use our linear support vector classifier for prediction.\n",
    "This is the same classifier that we just trained. \n",
    "'''\n",
    "predicted=clf_svc_pipeline.predict(tewnty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27713dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8532926181625067"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "One way to measure how well our classifier performs is to calculate the accuracy of our predictions. \n",
    "Let's see how many of our predicted labels are equal to the actual labels. \n",
    "Our support vector machine classifier performs pretty well. It has an accuracy of 85%.\n",
    "\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_svm=accuracy_score(tewnty_test.target,predicted)\n",
    "acc_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e82ad75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8150557620817844"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " Let's tweak our model a little bit. Instead of using the L2 norm as our penalty function, \n",
    " we will use the L1 norm. Go ahead and hit shift + enter to execute all the cells. \n",
    " You'll find that the accuracy of this model is around 81. 5%. It has fallen a bit. \n",
    "'''\n",
    "clf_svc_pipeline=Pipeline([('vect',CountVectorizer()),\n",
    "                          ('tfidf',TfidfTransformer()),\n",
    "                          ('clf',LinearSVC(penalty=\"l1\",dual=False,tol=0.001))])\n",
    "clf_svc_pipeline.fit(tewnty_train.data,tewnty_train.target)\n",
    "tewnty_test=fetch_20newsgroups(subset='test',shuffle=True)\n",
    "predicted=clf_svc_pipeline.predict(tewnty_test.data)\n",
    "acc_svm=accuracy_score(tewnty_test.target,predicted)\n",
    "acc_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1a8ca30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7963356346255974"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Let's make a little change to our pipeline. \n",
    "Instead of using the output of the TfidfTransformer, we'll use the output of the count vectorizer directly \n",
    "to feed into our support vector classifier. \n",
    "\n",
    "Train the pipeline and perform predictions on the test data. \n",
    "You can see that the accuracy is around 79. 8%. Compare this with the accuracy of 85% \n",
    "when we use the exact same model but the output of the Tfidfvectorizer. \n",
    "\n",
    "This gives us an idea of the impact of the Tfidf scores in our classification model.\n",
    "'''\n",
    "clf_svc_pipeline=Pipeline([('vect',CountVectorizer()),\n",
    "                          ('clf',LinearSVC(penalty=\"l2\",dual=False,tol=0.001))])\n",
    "clf_svc_pipeline.fit(tewnty_train.data,tewnty_train.target)\n",
    "tewnty_test=fetch_20newsgroups(subset='test',shuffle=True)\n",
    "predicted=clf_svc_pipeline.predict(tewnty_test.data)\n",
    "acc_svm=accuracy_score(tewnty_test.target,predicted)\n",
    "acc_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09060ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
