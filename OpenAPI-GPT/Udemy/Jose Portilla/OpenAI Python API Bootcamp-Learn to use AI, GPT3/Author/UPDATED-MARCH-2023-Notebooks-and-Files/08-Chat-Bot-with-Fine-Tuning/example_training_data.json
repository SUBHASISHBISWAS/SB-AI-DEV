{"prompt": "This is a difficult and open-ended question I know, but I thought I'd throw it to the floor and see if anyone had any interesting suggestions.\n\nI have developed a code-generator that takes our python interface to our C++ code (generated via SWIG) and generates code needed to expose this as WebServices.  When I developed this code I did it using TDD, but I've found my tests to be brittle as hell.  Because each test essentially wanted to verify that for a given bit of input code (which happens to be a C++ header) I'd get a given bit of outputted code I wrote a small engine that reads test definitions from XML input files and generates test cases from these expectations.\n\nThe problem is I dread going in to modify the code at all.  That and the fact that the unit tests themselves are a: complex, and b: brittle.\n\nSo I'm trying to think of alternative approaches to this problem, and it strikes me I'm perhaps tackling it the wrong way.  Maybe I need to focus more on the outcome, IE: does the code I generate actually run and do what I want it to, rather than, does the code look the way I want it to.\n\nHas anyone got any experiences of something similar to this they would care to share?\n", "completion": "I started writing up a summary of my experience with my own code generator, then went back and re-read your question and found you had already touched upon the same issues yourself, focus on the execution results instead of the code layout/look.\n\nProblem is, this is hard to test, the generated code might not be suited to actually run in the environment of the unit test system, and how do you encode the expected results?\n\nI've found that you need to break down the code generator into smaller pieces and unit test those. Unit testing a full code generator is more like integration testing than unit testing if you ask me.\n"}
{"prompt": "I'm creating an ZIP file with ZipFile in Python 2.5, it works ok so far:\n\nimport zipfile, os\n\nlocfile = \"test.txt\"\nloczip = os.path.splitext (locfile)[0] + \".zip\"\nzip = zipfile.ZipFile (loczip, \"w\")\nzip.write (locfile)\nzip.close()\n\n\nbut I couldn't find how to encrypt the files in the ZIP file.\nI could use system and call PKZIP -s, but I suppose there must be a more \"Pythonic\" way.  I'm looking for an open source solution.\n", "completion": "I created a simple library to create a password encrypted zip file in python. - here\n\nimport pyminizip\n\ncompression_level = 5 # 1-9\npyminizip.compress(\"src.txt\", \"dst.zip\", \"password\", compression_level)\n\n\nThe library requires zlib.\n\nI have checked that the file can be extracted in WINDOWS/MAC.\n"}
{"prompt": "I've been trying to wrap my head around how threads work in Python, and it's hard to find good information on how they operate. I may just be missing a link or something, but it seems like the official documentation isn't very thorough on the subject, and I haven't been able to find a good write-up.\n\nFrom what I can tell, only one thread can be running at once, and the active thread switches every 10 instructions or so?\n\nWhere is there a good explanation, or can you provide one? It would also be very nice to be aware of common problems that you run into while using threads with Python.\n", "completion": "Yes, because of the Global Interpreter Lock (GIL) there can only run one thread at a time. Here are some links with some insights about this:\n\n\nhttp://www.artima.com/weblogs/viewpost.jsp?thread=214235\nhttp://smoothspan.wordpress.com/2007/09/14/guido-is-right-to-leave-the-gil-in-python-not-for-multicore-but-for-utility-computing/\n\n\nFrom the last link an interesting quote:\n\n\n  Let me explain what all that means. \n  Threads run inside the same virtual\n  machine, and hence run on the same\n  physical machine.  Processes can run\n  on the same physical machine or in\n  another physical machine.  If you\n  architect your application around\n  threads, you\u00e2\u0080\u0099ve done nothing to access\n  multiple machines.  So, you can scale\n  to as many cores are on the single\n  machine (which will be quite a few\n  over time), but to really reach web\n  scales, you\u00e2\u0080\u0099ll need to solve the\n  multiple machine problem anyway.\n\n\nIf you want to use multi core, pyprocessing defines an process based API to do real parallelization. The PEP also includes some interesting benchmarks.\n"}
{"prompt": "A reliable coder friend told me that Python's current multi-threading implementation is seriously buggy - enough to avoid using altogether.  What can said about this rumor?\n", "completion": "Python threads are good for concurrent I/O programming. Threads are swapped out of the CPU as soon as they block waiting for input from file, network, etc. This allows other Python threads to use the CPU while others wait. This would allow you to write a multi-threaded web server or web crawler, for example.\n\nHowever, Python threads are serialized by the GIL when they enter interpreter core. This means that if two threads are crunching numbers, only one can run at any given moment. It also means that you can't take advantage of multi-core or multi-processor architectures.\n\nThere are solutions like running multiple Python interpreters concurrently, using a C based threading library. This is not for the faint of heart and the benefits might not be worth the trouble. Let's hope for an all Python solution in a future release.\n"}
{"prompt": "I am taking a class that requires Python. We will review the language in class next week, and I am a quick study on new languages, but I was wondering if there are any really great Python books I can grab while I am struggling through the basics of setting up my IDE, server environment and all those other \"gotchas\" that come with a new programming language. Suggestions?\n", "completion": "I loved Dive Into Python, especially if you're a quick study.  The beginning basics are all covered (and may move slowly for you), but the latter few chapters are great learning tools.\n\nPlus, Pilgrim is a pretty good writer.\n"}
{"prompt": "I am starting to use Python (specifically because of Django) and I would like to remove the burden for exhaustive testing by performing some static analysis.  What tools/parameters/etc. exist to detect issues at compile time that would otherwise show up during runtime? (type errors are probably the most obvious case of this, but undefined variables are another big one that could be avoided with an in-depth analysis of the AST.)\n\nObviously testing is important, and I don't imply that tests can be obviated entirely; however, there are many runtime errors in python that are not possible in other languages that perform stricter run-time checking -- I'm hoping that there are tools to bring at least some of these capabilities to python as well.\n", "completion": "pylint is the best such tool I've found. Due to Python's nature it's difficult to statically analyze it, but it will catch undefined variables, basic type errors, unused code, etc. You'll want to tweak the configuration file, as by default it outputs many warnings I consider useless or harmful.\n\nHere's part of my .pylintrc dealing with warning silencing:\n\n[MESSAGES CONTROL]\n\n# Brain-dead errors regarding standard language features\n#   W0142 = *args and **kwargs support\n#   W0403 = Relative imports\n\n# Pointless whinging\n#   R0201 = Method could be a function\n#   W0212 = Accessing protected attribute of client class\n#   W0613 = Unused argument\n#   W0232 = Class has no __init__ method\n#   R0903 = Too few public methods\n#   C0301 = Line too long\n#   R0913 = Too many arguments\n#   C0103 = Invalid name\n#   R0914 = Too many local variables\n\n# PyLint's module importation is unreliable\n#   F0401 = Unable to import module\n#   W0402 = Uses of a deprecated module\n\n# Already an error when wildcard imports are used\n#   W0614 = Unused import from wildcard\n\n# Sometimes disabled depending on how bad a module is\n#   C0111 = Missing docstring\n\n# Disable the message(s) with the given id(s).\ndisable=W0142,W0403,R0201,W0212,W0613,W0232,R0903,W0614,C0111,C0301,R0913,C0103,F0401,W0402,R0914\n\n"}
{"prompt": "In python one can get a dictionary of all local and global variables in the current scope with the built-in functions locals() and globals().  Is there some equivalent way of doing this in javascript?  For instance, I would like to do something like the following:\n\nvar foo = function(){ alert('foo'); };\nvar bar = function(){ alert('bar'); };\n\nvar s = 'foo';\nlocals()[s](); // alerts 'foo'\n\n\nIs this at all possible, or should I just be using a local object for the lookup?\n", "completion": "\nlocals() - No. \nglobals() - Yes.\n\n\nwindow is a reference to the global scope, like globals() in python.\n\nglobals()[\"foo\"]\n\n\nis the same as:\n\nwindow[\"foo\"]\n\n"}
{"prompt": "Is there a way using Python's standard library to easily determine (i.e. one function call) the last day of a given month?\n\nIf the standard library doesn't support that, does the dateutil package support this?\n", "completion": "I didn't notice this earlier when I was looking at the documentation for the calendar module, but a method called monthrange provides this information:\n\n\n  monthrange(year, month)\n  \u00a0\u00a0\u00a0\u00a0Returns weekday of first day of the month and number of days in month, for the specified year and month. \n\n\n>>> import calendar\n>>> calendar.monthrange(2002,1)\n(1, 31)\n>>> calendar.monthrange(2008,2)\n(4, 29)\n>>> calendar.monthrange(2100,2)\n(0, 28)\n\n\nso:\n\ncalendar.monthrange(year, month)[1]\n\n\nseems like the simplest way to go.\n\nJust to be clear, monthrange supports leap years as well:\n\n>>> from calendar import monthrange\n>>> monthrange(2012, 2)\n(2, 29)\n\n\nMy previous answer still works, but is clearly suboptimal.\n"}
{"prompt": "In Django's template language, you can use {% url [viewname] [args] %} to generate a URL to a specific view with parameters. How can you programatically do the same in Python code?\n\nWhat I need is to create a list of menu items where each item has name, URL, and an active flag  (whether it's the current page or not). This is because it will be a lot cleaner to do this in Python than the template language.\n", "completion": "If you need to use something similar to the {% url %} template tag in your code, Django provides the django.core.urlresolvers.reverse(). The reverse function has the following signature:\n\nreverse(viewname, urlconf=None, args=None, kwargs=None)\n\n\nhttps://docs.djangoproject.com/en/dev/ref/urlresolvers/\n"}
{"prompt": "Let's say you want to save a bunch of files somewhere, for instance in BLOBs. Let's say you want to dish these files out via a web page and have the client automatically open the correct application/viewer.\n\nAssumption: The browser figures out which application/viewer to use by the mime-type (content-type?) header in the HTTP response.\n\nBased on that assumption, in addition to the bytes of the file, you also want to save the MIME type.\n\nHow would you find the MIME type of a file? I'm currently on a Mac, but this should also work on Windows. \n\nDoes the browser add this information when posting the file to the web page?\n\nIs there a neat python library for finding this information? A WebService or (even better) a downloadable database?\n", "completion": "The python-magic method suggested by toivotuo is outdated. Python-magic's current trunk is at Github and based on the readme there, finding the MIME-type, is done like this.\n\n# For MIME types\n>>> import magic\n>>> mime = magic.Magic(mime=True)\n>>> mime.from_file(\"testdata/test.pdf\")\n'application/pdf'\n>>>\n\n"}
{"prompt": "If possible I want to catch pressure sensitive input from a Wacom tablet in Python. Are there any Python libraries available that can do this?\n", "completion": "You could perhaps take a look at the software described here. It is a gnome applet, written in Python.\n\nFrom the web site:\n\n\"The gnome wacom applet is a small gnome panel applet that shows how much pressure is being applied to your wacom tablet by the current device. Clicking on the panel icon brings up a dialog allowing you to select a different device and check what pressure and tilt information is being recieved from it. This dialog also contains a small drawing test area to give your pen a quick test.\"\n\nGoogle is your friend\n"}
{"prompt": "I have 2 time values which have the type datetime.time. I want to find their difference. The obvious thing to do is t1 - t2, but this doesn't work. It works for objects of type datetime.datetime but not for datetime.time. So what is the best way to do this?\n", "completion": "Also a little silly, but you could try picking an arbitrary day and embedding each time in it, using datetime.datetime.combine, then subtracting:\n\n>>> import datetime\n>>> t1 = datetime.time(2,3,4)\n>>> t2 = datetime.time(18,20,59)\n>>> dummydate = datetime.date(2000,1,1)\n>>> datetime.datetime.combine(dummydate,t2) - datetime.datetime.combine(dummydate,t1)\ndatetime.timedelta(0, 58675)\n\n"}
{"prompt": "Given a path such as \"mydir/myfile.txt\", how do I find the absolute filepath relative to the current working directory in Python? E.g. on Windows, I might end up with:\n\n\"C:/example/cwd/mydir/myfile.txt\"\n\n", "completion": ">>> import os\n>>> os.path.abspath(\"mydir/myfile.txt\")\n\n"}
{"prompt": "Dictionaries unlike lists are not ordered (and do not have the 'sort' attribute). Therefore, you can not rely on getting the items in the same order when first added. \n\nWhat is the easiest way to loop through a dictionary containing strings as the key value and retrieving them in ascending order by key?\n\nFor example, you had this:\n\nd = {'b' : 'this is b', 'a': 'this is a' , 'c' : 'this is c'}\n\n\nI want to print the associated values in the following sequence sorted by key:\n\nthis is a\nthis is b\nthis is c\n\n", "completion": "Do you mean that you need the values sorted by the value of the key?\nIn that case, this should do it:\n\nfor key in sorted(d):\n    print d[key]\n\n\nEDIT: changed to use sorted(d) instead of sorted(d.keys()), thanks Eli!\n"}
{"prompt": "The documentation for the round() function states that you pass it a number, and the positions past the decimal to round. Thus it should do this:\n\nn = 5.59\nround(n, 1) # 5.6\n\n\nBut, in actuality, good old floating point weirdness creeps in and you get:\n\n5.5999999999999996\n\n\nFor the purposes of UI, I need to display 5.6. I poked around the Internet and found some documentation that this is dependent on my implementation of Python. Unfortunately, this occurs on both my Windows dev machine and each Linux server I've tried. See here also.\n\nShort of creating my own round library, is there any way around this?\n", "completion": "Formatting works correctly even without having to round:\n\n\"%.1f\" % n\n\n"}
{"prompt": "I'm writing a small web server in Python, using BaseHTTPServer and a custom subclass of BaseHTTPServer.BaseHTTPRequestHandler. Is it possible to make this listen on more than one port?\n\nWhat I'm doing now:\n\nclass MyRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\n  def doGET\n  [...]\n\nclass ThreadingHTTPServer(ThreadingMixIn, HTTPServer): \n    pass\n\nserver = ThreadingHTTPServer(('localhost', 80), MyRequestHandler)\nserver.serve_forever()\n\n", "completion": "Sure; just start two different servers on two different ports in two different threads that each use the same handler.  Here's a complete, working example that I just wrote and tested.  If you run this code then you'll be able to get a Hello World webpage at both http://localhost:1111/ and http://localhost:2222/\n\nfrom threading import Thread\nfrom SocketServer import ThreadingMixIn\nfrom BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler\n\nclass Handler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/plain\")\n        self.end_headers()\n        self.wfile.write(\"Hello World!\")\n\nclass ThreadingHTTPServer(ThreadingMixIn, HTTPServer):\n    pass\n\ndef serve_on_port(port):\n    server = ThreadingHTTPServer((\"localhost\",port), Handler)\n    server.serve_forever()\n\nThread(target=serve_on_port, args=[1111]).start()\nserve_on_port(2222)\n\n"}
{"prompt": "I've created a python object, but I want to send signals on it.  I made it inherit from gobject.GObject, but there doesn't seem to be any way to create a new signal on my object.\n", "completion": "You can also define signals inside the class definition:\n\nclass MyGObjectClass(gobject.GObject):\n    __gsignals__ = {\n      \"some-signal\": (gobject.SIGNAL_RUN_FIRST, gobject.TYPE_NONE, (object, )),\n    }\n\n\nThe contents of the tuple are the the same as the three last arguments to gobject.signal_new.\n"}
{"prompt": "Is there any performance difference between tuples and lists when it comes to instantiation and retrieval of elements? \n", "completion": "In general, you might expect tuples to be slightly faster. However you should definitely test your specific case (if the difference might impact the performance of your program --  remember \"premature optimization is the root of all evil\").\n\nPython makes this very easy: timeit is your friend.\n\n$ python -m timeit \"x=(1,2,3,4,5,6,7,8)\"\n10000000 loops, best of 3: 0.0388 usec per loop\n\n$ python -m timeit \"x=[1,2,3,4,5,6,7,8]\"\n1000000 loops, best of 3: 0.363 usec per loop\n\n\nand...\n\n$ python -m timeit -s \"x=(1,2,3,4,5,6,7,8)\" \"y=x[3]\"\n10000000 loops, best of 3: 0.0938 usec per loop\n\n$ python -m timeit -s \"x=[1,2,3,4,5,6,7,8]\" \"y=x[3]\"\n10000000 loops, best of 3: 0.0649 usec per loop\n\n\nSo in this case, instantiation is almost an order of magnitude faster for the tuple, but item access is actually somewhat faster for the list! So if you're creating a few tuples and accessing them many many times, it may actually be faster to use lists instead.\n\nOf course if you want to change an item, the list will definitely be faster since you'd need to create an entire new tuple to change one item of it (since tuples are immutable).\n"}
{"prompt": "I am trying to use the import random statement in python, but it doesn't appear to have any methods in it to use.\n\nAm I missing something?\n", "completion": "You probably have a file named random.py or random.pyc in your working directory.  That's shadowing the built-in random module.  You need to rename random.py to something like my_random.py and/or remove the random.pyc file.\n\nTo tell for sure what's going on, do this:\n\n>>> import random\n>>> print random.__file__\n\n\nThat will show you exactly which file is being imported.\n"}
{"prompt": "What method do I call to get the name of a class?\n", "completion": "In [1]: class test(object):\n   ...:     pass\n   ...: \n\nIn [2]: test.__name__\nOut[2]: 'test'\n\n"}
{"prompt": "I'm trying to use Python with ReportLab 2.2 to create a PDF report.\nAccording to the user guide,\n\n\n  Special TableStyle Indeces [sic]\n  \n  In any style command the first row index may be set to one of the special strings 'splitlast' or 'splitfirst' to indicate that the style should be used only for the last row of a split table, or the first row of a continuation. This allows splitting tables with nicer effects around the split.\n\n\nI've tried using several style elements, including:\n\n('TEXTCOLOR', (0, 'splitfirst'), (1, 'splitfirst'), colors.black) \n('TEXTCOLOR', (0, 'splitfirst'), (1, 0), colors.black) \n('TEXTCOLOR', (0, 'splitfirst'), (1, -1), colors.black)\n\n\nand none of these seems to work.  The first generates a TypeError with the message: \n\nTypeError: cannot concatenate 'str' and 'int' objects\n\n\nand the latter two generate TypeErrors with the message:\n\nTypeError: an integer is required\n\n\nIs this functionality simply broken or am I doing something wrong?  If the latter, what am I doing wrong?\n", "completion": "Well, it looks as if I will be answering my own question.\n\nFirst, the documentation flat out lies where it reads \"In any style command the first row index may be set to one of the special strings 'splitlast' or 'splitfirst' to indicate that the style should be used only for the last row of a split table, or the first row of a continuation.\"  In the current release, the \"splitlast\" and \"splitfirst\" row indices break with the aforementioned TypeErrors on the TEXTCOLOR and BACKGROUND commnds.\n\nMy suspicion, based on reading the source code, is that only the tablestyle line commands (GRID, BOX, LINEABOVE, and LINEBELOW) are currently compatible with the 'splitfirst' and 'splitlast' row indices.  I suspect that all cell commands break with the aforementioned TypeErrors.\n\nHowever, I was able to do what I wanted by subclassing the Table class and overriding the onSplit method.  Here is my code:\n\nclass XTable(Table):\n    def onSplit(self, T, byRow=1):\n        T.setStyle(TableStyle([\n          ('TEXTCOLOR', (0, 1), (1, 1), colors.black)]))\n\n\nWhat this does is apply the text color black to the first and second cell of the second row of each page.  (The first row is a header, repeated by the repeatRows parameter of the Table.)  More precisely, it is doing this to the first and second cell of each frame, but since I am using the SimpleDocTemplate, frames and pages are identical.\n"}
{"prompt": "Since the Google App Engine Datastore is based on Bigtable and we know that's not a relational database, how do you design a database schema/*data model* for applications that use this type of database system?\n", "completion": "Designing a bigtable schema is an open process, and basically requires you to think about:\n\n\nThe access patterns you will be using and how often each will be used\nThe relationships between your types\nWhat indices you are going to need\nThe write patterns you will be using (in order to effectively spread load)\n\n\nGAE's datastore automatically denormalizes your data. That is, each index contains a (mostly) complete copy of the data, and thus every index adds significantly to time taken to perform a write, and the storage space used.\n\nIf this were not the case, designing a Datastore schema would be a lot more work: You would have to think carefully about the primary key for each type, and consider the effect of your decision on the locality of data. For example, when rendering a blog post you would probably need to display the comments to go along with it, so each comment's key would probably begin with the associated post's key.\n\nWith Datastore, this is not such a big deal: The query you use will look something like \"Select * FROM Comment WHERE post_id = N.\" (If you want to page the comments, you would also have a limit clause, and a possible suffix of \" AND comment_id > last_comment_id\".) Once you add such a query, Datastore will build the index for you, and your reads will be magically fast.\n\nSomething to keep in mind is that each additional index creates some additional cost: it is best if you can use as few access patterns as possible, since it will reduce the number of indices GAE will construct, and thus the total storage required by your data.\n\nReading over this answer, I find it a little vague. Maybe a hands-on design question would help to scope this down? :-)\n"}
{"prompt": "I wonder why would a C++, C#, Java developer want to learn a dynamic language?\n\nAssuming the company won't switch its main development language from C++/C#/Java to a dynamic one what use is there for a dynamic language?\n\nWhat helper tasks can be done by the dynamic languages faster or better after only a few days of learning than with the static language that you have been using for several years?\n\nUpdate\n\nAfter seeing the first few responses it is clear that there are two issues.\nMy main interest would be something that is justifiable to the employer as an expense.\nThat is, I am looking for justifications for the employer to finance the learning of a dynamic language. Aside from the obvious that the employee will have broader view, the\nemployers are usually looking for some \"real\" benefit.\n", "completion": "A lot of times some quick task comes up that isn't part of the main software you are developing.  Sometimes the task is one off ie compare this file to the database and let me know the differences.  It is a lot easier to do text parsing in Perl/Ruby/Python than it is in Java or C# (partially because it is a lot easier to use regular expressions).  It will probably take a lot less time to parse the text file using Perl/Ruby/Python (or maybe even vbscript cringe and then load it into the database than it would to create a Java/C# program to do it or to do it by hand.\n\nAlso, due to the ease at which most of the dynamic languages parse text, they are great for code generation.  Sure your final project must be in C#/Java/Transact SQL but instead of cutting and pasting 100 times, finding errors, and cutting and pasting another 100 times it is often (but not always) easier just to use a code generator.\n\nA recent example at work is we needed to get data from one accounting system into our accounting system.  The system has an import format, but the old system had a completely different format (fixed width although some things had to be matched).  The task is not to create a program to migrate the data over and over again.  It is to shove the data into our system and then maintain it there going forward.  So even though we are a C# and SQL Server shop, I used Python to convert the data into the format that could be imported by our application.  Ultimately it doesn't matter that I used python, it matters that the data is in the system.  My boss was pretty impressed.\n\nWhere I often see the dynamic languages used for is testing.  It is much easier to create a Python/Perl/Ruby program to link to a web service and throw some data against it than it is to create the equivalent Java program.  You can also use python to hit against command line programs, generate a ton of garbage (but still valid) test data, etc.. quite easily.\n\nThe other thing that dynamic languages are big on is code generation.  Creating the C#/C++/Java code.  Some examples follow:\n\nThe first code generation task I often see is people using dynamic languages to maintain constants in the system.  Instead of hand coding a bunch of enums, a dynamic language can be used to fairly easily parse a text file and create the Java/C# code with the enums.\n\nSQL is a whole other ball game but often you get better performance by cut and pasting 100 times instead of trying to do a function (due to caching of execution plans or putting complicated logic in a function causing you to go row by row instead of in a set).  In fact it is quite useful to use the table definition to create certain stored procedures automatically.\n\nIt is always better to get buy in for a code generator.  But even if you don't, is it more fun to spend time cutting/pasting or is it more fun to create a Perl/Python/Ruby script once and then have that generate the code?  If it takes you hours to hand code something but less time to create a code generator, then even if you use it once you have saved time and hence money.  If it takes you longer to create a code generator than it takes to hand code once but you know you will have to update the code more than once, it may still make sense.  If it takes you 2 hours to hand code, 4 hours to do the generator but you know you'll have to hand code equivalent work another 5 or 6 times than it is obviously better to create the generator.\n\nAlso some things are easier with dynamic languages than Java/C#/C/C++.  In particular regular expressions come to mind.  If you start using regular expressions in Perl and realize their value, you may suddenly start making use of the Java regular expression library if you haven't before.  If you have then there may be something else.\n\nI will leave you with one last example of a task that would have been great for a dynamic language.  My work mate had to take a directory full of files and burn them to various cd's for various customers.  There were a few customers but a lot of files and you had to look in them to see what they were.  He did this task by hand....A Java/C# program would have saved time, but for one time and with all the development overhead it isn't worth it.  However slapping something together in Perl/Python/Ruby probably would have been worth it.  He spent several hours doing it.  It would have taken less than one to create the Python script to inspect each file, match which customer it goes to, and then move the file to the appropriate place.....Again, not part of the standard job.  But the task came up as a one off.  Is it better to do it yourself, spend the larger amount of time to make Java/C# do the task, or spend a much smaller amount of time doing it in Python/Perl/Ruby.  If you are using C or C++ the point is even more dramatic due to the extra concerns of programming in C or C++ (pointers, no array bounds checking, etc.).\n"}
{"prompt": "Working with python interactively, it's sometimes necessary to display a result which is some arbitrarily complex data structure (like lists with embedded lists, etc.)\nThe default way to display them is just one massive linear dump which just wraps over and over and you have to parse carefully to read it.\n\nIs there something that will take any python object and display it in a more rational manner. e.g.\n\n[0, 1,\n    [a, b, c],\n    2, 3, 4]\n\n\ninstead of:\n\n[0, 1, [a, b, c], 2, 3, 4]\n\n\nI know that's not a very good example, but I think you get the idea.\n", "completion": "from pprint import pprint\na = [0, 1, ['a', 'b', 'c'], 2, 3, 4]\npprint(a)\n\n\nNote that for a short list like my example, pprint will in fact print it all on one line. However, for more complex structures it does a pretty good job of pretty printing data.\n"}
{"prompt": "Looking for an open source library, for C++, Java, C# or Python, for reading the data from Quicken .qdf files.\n\n@Swati:  Quicken .qif format is for transfer only and is not kept up to date by the application  like the .qdf file is.\n", "completion": "QDF is proprietary and not really meant for reading other than my Quicken, probably for a reason as it is messy.  \n\nI would recommend finding a way to export the qdf into an OFX (Open Financial Exchange) or qif file.  I have done some financial and quickbooks automation and I did something similar.  The problem is if you don't export to an exchange format, each version differs and strange things happen for many conditions that since they aren't documented (QDF) it becomes a bad situation for the programmer.\n\nOFX is what allows online banking, brokerages and apps like mint.com securely get financial data.  It is a standard and consistent.  Finding a way to this is much better if at all possible.  \n"}
{"prompt": "I've gotten to grips with the basics of Python and I've got a small holiday which I want to use some of to learn a little more Python. The problem is that I have no idea what to learn or where to start. I'm primarily web development but in this case I don't know how much difference it will make.\n", "completion": "Well, there are great ressources for advanced Python programming :\n\n\nDive Into Python (read it for free)\nOnline python cookbooks (e.g. here and there)\nO'Reilly's Python Cookbook (see amazon)\nA funny riddle game : Python Challenge \n\n\nHere is a list of subjects you must master if you want to write \"Python\" on your resume :\n\n\nlist comprehensions\niterators and generators\ndecorators\n\n\nThey are what make Python such a cool language (with the standard library of course, that I keep discovering everyday).\n"}
{"prompt": "I came back today to an old script I had for logging into Gmail via SSL. The script worked fine last time I ran it (several months ago) but now it dies immediately with:\n\n<urlopen error The read operation timed out>\n\n\nIf I set the timeout (no matter how long), it dies even more immediately with:\n\n<urlopen error The connect operation timed out>\n\n\nThe latter is reproducible with:\n\nimport socket\nsocket.setdefaulttimeout(30000)\nsock = socket.socket()\nsock.connect(('www.google.com', 443))\nssl = socket.ssl(sock)\n\n\nreturning:\n\nsocket.sslerror: The connect operation timed out\n\n\nbut I can't seem to reproduce the former and, after much stepping thru the code, I have no clue what's causing any of this.\n", "completion": "import socket\nsocket.setdefaulttimeout(30000)\nsock = socket.socket()\nsock.connect(('www.google.com', 443))\nssl = socket.ssl(sock)\nssl.server()\n--> '/C=US/ST=California/L=Mountain View/O=Google Inc/CN=www.google.com'\n\n\nIt works just fine. I can't reproduce your error.\n"}
{"prompt": "Basically, something similar to System.Xml.XmlWriter - A streaming XML Writer that doesn't incur much of a memory overhead. So that rules out xml.dom and xml.dom.minidom. Suggestions?\n", "completion": "I think you'll find XMLGenerator from xml.sax.saxutils is the closest thing to what you want.\n\n\nimport time\nfrom xml.sax.saxutils import XMLGenerator\nfrom xml.sax.xmlreader import AttributesNSImpl\n\nLOG_LEVELS = ['DEBUG', 'WARNING', 'ERROR']\n\n\nclass xml_logger:\n    def __init__(self, output, encoding):\n        \"\"\"\n        Set up a logger object, which takes SAX events and outputs\n        an XML log file\n        \"\"\"\n        logger = XMLGenerator(output, encoding)\n        logger.startDocument()\n        attrs = AttributesNSImpl({}, {})\n        logger.startElementNS((None, u'log'), u'log', attrs)\n        self._logger = logger\n        self._output = output\n        self._encoding = encoding\n        return\n\n    def write_entry(self, level, msg):\n        \"\"\"\n        Write a log entry to the logger\n        level - the level of the entry\n        msg   - the text of the entry.  Must be a Unicode object\n        \"\"\"\n        #Note: in a real application, I would use ISO 8601 for the date\n        #asctime used here for simplicity\n        now = time.asctime(time.localtime())\n        attr_vals = {\n            (None, u'date'): now,\n            (None, u'level'): LOG_LEVELS[level],\n            }\n        attr_qnames = {\n            (None, u'date'): u'date',\n            (None, u'level'): u'level',\n            }\n        attrs = AttributesNSImpl(attr_vals, attr_qnames)\n        self._logger.startElementNS((None, u'entry'), u'entry', attrs)\n        self._logger.characters(msg)\n        self._logger.endElementNS((None, u'entry'), u'entry')\n        return\n\n    def close(self):\n        \"\"\"\n        Clean up the logger object\n        \"\"\"\n        self._logger.endElementNS((None, u'log'), u'log')\n        self._logger.endDocument()\n        return\n\nif __name__ == \"__main__\":\n    #Test it out\n    import sys\n    xl = xml_logger(sys.stdout, 'utf-8')\n    xl.write_entry(2, u\"Vanilla log entry\")\n    xl.close()   \n\n\n\nYou'll probably want to look at the rest of the article I got that from at http://www.xml.com/pub/a/2003/03/12/py-xml.html.\n"}
{"prompt": "Simple question:\n\n\nWhat Python GUI API's are out there and what are the advantages of any given API?\n\n\nI'm not looking for a religious war here, I'm just wanting to get a good handle on all that is out there in terms of Python GUI APIs.\n", "completion": "Here's a good list.\n"}
{"prompt": "I need to read selected files, matching on the file name, from a remote zip archive using Python. I don't want to save the full zip to a temporary file (it's not that large, so I can handle everything in memory).\n\nI've already written the code and it works, and I'm answering this myself so I can search for it later. But since evidence suggests that I'm one of the dumber participants on Stackoverflow, I'm sure there's room for improvement.\n", "completion": "Here's how I did it (grabbing all files ending in \".ranks\"):\n\nimport urllib2, cStringIO, zipfile\n\ntry:\n    remotezip = urllib2.urlopen(url)\n    zipinmemory = cStringIO.StringIO(remotezip.read())\n    zip = zipfile.ZipFile(zipinmemory)\n    for fn in zip.namelist():\n        if fn.endswith(\".ranks\"):\n            ranks_data = zip.read(fn)\n            for line in ranks_data.split(\"\\n\"):\n                # do something with each line\nexcept urllib2.HTTPError:\n    # handle exception\n\n"}
{"prompt": "Given a datetime.time value in Python, is there a standard way to add an integer number of seconds to it, so that 11:34:59 + 3 = 11:35:02, for example?\n\nThese obvious ideas don't work:\n\n>>> datetime.time(11, 34, 59) + 3\nTypeError: unsupported operand type(s) for +: 'datetime.time' and 'int'\n>>> datetime.time(11, 34, 59) + datetime.timedelta(0, 3)\nTypeError: unsupported operand type(s) for +: 'datetime.time' and 'datetime.timedelta'\n>>> datetime.time(11, 34, 59) + datetime.time(0, 0, 3)\nTypeError: unsupported operand type(s) for +: 'datetime.time' and 'datetime.time'\n\n\nIn the end I have written functions like this:\n\ndef add_secs_to_time(timeval, secs_to_add):\n    secs = timeval.hour * 3600 + timeval.minute * 60 + timeval.second\n    secs += secs_to_add\n    return datetime.time(secs // 3600, (secs % 3600) // 60, secs % 60)\n\n\nI can't help thinking that I'm missing an easier way to do this though.\n\nRelated\n\n\npython time + timedelta equivalent\n\n", "completion": "You can use full datetime variables with timedelta, and by providing a dummy date then using time to just get the time value.\n\nFor example:\n\nimport datetime\na = datetime.datetime(100,1,1,11,34,59)\nb = a + datetime.timedelta(0,3) # days, seconds, then other fields.\nprint a.time()\nprint b.time()\n\n\nresults in the two values, three seconds apart:\n\n11:34:59\n11:35:02\n\n\nYou could also opt for the more readable\n\nb = a + datetime.timedelta(seconds=3)\n\n\nif you're so inclined.\n\n\n\nIf you're after a function that can do this, you can look into using addSecs below:\n\nimport datetime\n\ndef addSecs(tm, secs):\n    fulldate = datetime.datetime(100, 1, 1, tm.hour, tm.minute, tm.second)\n    fulldate = fulldate + datetime.timedelta(seconds=secs)\n    return fulldate.time()\n\na = datetime.datetime.now().time()\nb = addSecs(a, 300)\nprint a\nprint b\n\n\nThis outputs:\n\n 09:11:55.775695\n 09:16:55\n\n"}
{"prompt": "How do you generate all the permutations of a list in Python, independently of the type of elements in that list?\n\nFor example:\n\npermutations([])\n[]\n\npermutations([1])\n[1]\n\npermutations([1, 2])\n[1, 2]\n[2, 1]\n\npermutations([1, 2, 3])\n[1, 2, 3]\n[1, 3, 2]\n[2, 1, 3]\n[2, 3, 1]\n[3, 1, 2]\n[3, 2, 1]\n\n\nEDIT:\nEliben pointed to a solution that's similar to mine although simpler, so I'm choosing it as the accepted answer, although Python 2.6+ has a builtin solution in the itertools module:\n\nimport itertools\nitertools.permutations([1, 2, 3])\n\n", "completion": "And in Python 2.6 onwards:\n\nimport itertools\nitertools.permutations([1,2,3])\n\n\n(returned as a generator.  Use list(permutations(l)) to return as a list.)\n"}
{"prompt": "What's the difference between file and open in Python?  When should I use which one?  (Say I'm in 2.5)\n", "completion": "You should always use open().\n\nAs the documentation states:\n\n\n  When opening a file, it's preferable\n  to use open() instead of invoking this\n  constructor directly. file is more\n  suited to type testing (for example,\n  writing \"isinstance(f, file)\").\n\n\nAlso, file() has been removed since Python 3.0.\n"}
{"prompt": "One of the basic data structures in Python is the dictionary, which allows one to record \"keys\" for looking up \"values\" of any type.  Is this implemented internally as a hash table?  If not, what is it?\n", "completion": "Yes, it is a hash mapping or hash table. You can read a description of python's dict implementation, as written by Tim Peters, here.\n\nThat's why you can't use something 'not hashable' as a dict key, like a list:\n\n>>> a = {}\n>>> b = ['some', 'list']\n>>> hash(b)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: list objects are unhashable\n>>> a[b] = 'some'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: list objects are unhashable\n\n\nYou can read more about hash tables or check how it has been implemented in python and why it is implemented that way.\n"}
{"prompt": "For example, the standard division symbol '/' rounds to zero:\n\n>>> 4 / 100\n0\n\n\nHowever, I want it to return 0.04. What do I use?\n", "completion": "There are three options:\n\n>>> 4 / float(100)\n0.04\n>>> 4 / 100.0\n0.04\n\n\nwhich is the same behavior as the C, C++, Java etc, or \n\n>>> from __future__ import division\n>>> 4 / 100\n0.04\n\n\nYou can also activate this behavior by passing the argument -Qnew to the Python interpreter:\n\n$ python -Qnew\n>>> 4 / 100\n0.04\n\n\nThe second option will be the default in Python 3.0. If you want to have the old integer division, you have to use the // operator. \n\nEdit: added section about -Qnew, thanks to \u00ce\u00a4\u00ce\u0096\u00ce\u00a9\u00ce\u00a4\u00ce\u0096\u00ce\u0099\u00ce\u009f\u00ce\u00a5!\n"}
{"prompt": "I'm trying to teach Komodo to fire up IDLE when I hit the right keystrokes.  I can use the exact path of the shortcut in start menu in the Windows Explorer location bar to launch IDLE so I was hoping Komodo would be able to use it as well.  But, giving this path to Komodo causes it to say that 1 is returned.  This appears to be a failure as IDLE doesn't start up.\n\nI thought I'd avoid the shortcut and just use the exact path.  I go to the start menu, find the shortcut for IDLE, right click to look at the properties.  The target is grayed out, but says \"Python 2.5.2\".  The \"Start in\" is set to, \"C:\\Python25\\\".  The \"Open File Location\" button is also grayed out.\n\nHow do I find out where this shortcut is really pointing?  I have tried starting python.exe and pythonw.exe both in C:\\Python25, but neither starts up IDLE.\n", "completion": "There's a file called idle.py in your Python installation directory in Lib\\idlelib\\idle.py\n\nIf you run that file with Python, then IDLE should start.\n\n\n  c:\\Python25\\pythonw.exe c:\\Python25\\Lib\\idlelib\\idle.py\n\n"}
{"prompt": "This came up in  Hidden features of Python, but I can't see good documentation or examples that explain how the feature works.\n", "completion": "The ellipsis is used to slice higher-dimensional data structures. \n\nIt's designed to mean at this point, insert as many full slices (:) to extend the multi-dimensional slice to all dimensions.\n\nExample:\n\n>>> from numpy import arange\n>>> a = arange(16).reshape(2,2,2,2)\n\n\nNow, you have a 4-dimensional matrix of order 2x2x2x2. To select all first elements in the 4th dimension, you can use the ellipsis notation\n\n>>> a[..., 0].flatten()\narray([ 0,  2,  4,  6,  8, 10, 12, 14])\n\n\nwhich is equivalent to\n\n>>> a[:,:,:,0].flatten()\narray([ 0,  2,  4,  6,  8, 10, 12, 14])\n\n\nIn your own implementations, you're free to ignore the contract mentioned above and use it for whatever you see fit.\n"}
{"prompt": "Python allocates integers automatically based on the underlying system architecture. Unfortunately I have a huge dataset which needs to be fully loaded into memory. \n\nSo, is there a way to force Python to use only 2 bytes for some integers (equivalent of C++ 'short')?\n", "completion": "Nope.  But you can use short integers in arrays:\n\nfrom array import array\na = array(\"h\") # h = signed short, H = unsigned short\n\n\nAs long as the value stays in that array it will be a short integer.\n\n\ndocumentation for the array module\n\n"}
{"prompt": "My Apache server runs on some non-default (not-root) account. When it tries to run a python script which in turn executes a subversion check-out command, 'svn checkout' fails with the following error message:\n\nsvn: Can't open file '/root/.subversion/servers': Permission denied\n\n\nAt the same time running that python script with subversion checkout command inside from command line under the same user account goes on perfectly well.\n\nApache server 2.2.6 with mod_python 3.2.8 runs on Fedora Core 6 machine.\n\nCan anybody help me out? Thanks a lot.\n", "completion": "It sounds like the environment you apache process is running under is a little unusual.  For whatever reason, svn seems to think the user configuration files it needs are in /root.  You can avoid having svn use the root versions of the files by specifying on the command line which config directory to use, like so:\n\nsvn --config-dir /home/myuser/.subversion checkout http://example.com/path\n\n\nWhile not fixing your enviornment, it will at least allow you to have your script run properly...\n"}
{"prompt": "If I open an image with open(\"image.jpg\"), how can I get the RGB values of a pixel, if I have the coordinates of the pixel?\n\nThen how can I do the reverse of this? Starting with a blank graphic, 'write' a pixel with a certain RGB value?\n\nIt would be so much better if I didn't have to download any additional libraries.\n", "completion": "It's probably best to use the Python Image Library to do this which I'm afraid is a separate download.\n\nThe easiest way to do what you want is via the load() method on the Image object which returns a pixel access object which you can manipulate like an array:\n\nfrom PIL import Image\nim = Image.open(\"dead_parrot.jpg\") #Can be many different formats.\npix = im.load()\nprint im.size #Get the width and hight of the image for iterating over\nprint pix[x,y] #Get the RGBA Value of the a pixel of an image\npix[x,y] = value # Set the RGBA Value of the image (tuple)\n\n\nAlternatively, look at ImageDraw which gives a much richer API for creating images.\n"}
{"prompt": "is it possible to extend vim functionality via custom extension (preferably, written in Python)?\n\nWhat I need ideally is custom command when in command mode. E.g.\n\nESC\n\n:do_this\n\n:do_that\n", "completion": "vim supports scripting in python (and in perl as well, I think).\n\nYou just have to make sure that the vim distribution you are using has been compiled with python support.\n\nIf you are using a Linux system, you can download the source and then compile it with\n\n./configure --enable-pythoninterp \nmake\nsudo make install\n\n\nInside vim, you can type\n\n:version\n\n\nto list the available features; if it has python support, you should see a '+python' somewhere (a '-python' otherwise).\n\nThen, to check the usage of the python module, you can type\n\n:help python\n\n\nP.S: if you're going to compile the vim sources, make sure to check the available configure options, you might need to specify --with-python-config-dir as well.\n\nP.P.S: to create a \"custom command in command mode\" (if I understand correctly what you mean), you can create a function \"MyFunction\" in a vim script (using python or the vim scripting language) and then invoke it with \n\n:Call MyFunction()\n\n\nCheck \n\n:help user-functions\n\n\nfor details\n"}
{"prompt": "I have a python module installed on my system and I'd like to be able to see what functions/classes/methods are available in it.   \n\nI want to call the doc function on each one. In ruby I can do something like ClassName.methods to get a list of all the methods available on that class. Is there something similar in python?\n\neg. something like:\n\nfrom somemodule import foo\nprint foo.methods # or whatever is the correct method to call\n\n", "completion": "You can use dir(module) to see all available methods/attributes.  Also check out PyDocs.\n"}
{"prompt": "What would be the quickest way to construct a python binding to a C or C++ library?\n\n(using windows if this matters)\n", "completion": "I like ctypes a lot, swig always tended to give me problems. Also ctypes has the advantage that you don't need to satisfy any compile time dependency on python, and your binding will work on any python that has ctypes, not just the one it was compiled against.\n\nSuppose you have a simple C++ example class you want to talk to in a file called foo.cpp:\n\n#include <iostream>\n\nclass Foo{\n    public:\n        void bar(){\n            std::cout << \"Hello\" << std::endl;\n        }\n};\n\n\nSince ctypes can only talk to C functions, you need to provide those declaring them as extern \"C\"\n\nextern \"C\" {\n    Foo* Foo_new(){ return new Foo(); }\n    void Foo_bar(Foo* foo){ foo->bar(); }\n}\n\n\nNext you have to compile this to a shared library\n\ng++ -c -fPIC foo.cpp -o foo.o\ng++ -shared -Wl,-soname,libfoo.so -o libfoo.so  foo.o\n\n\nAnd finally you have to write your python wrapper (e.g. in fooWrapper.py)\n\nfrom ctypes import cdll\nlib = cdll.LoadLibrary('./libfoo.so')\n\nclass Foo(object):\n    def __init__(self):\n        self.obj = lib.Foo_new()\n\n    def bar(self):\n        lib.Foo_bar(self.obj)\n\n\nOnce you have that you can call it like\n\nf = Foo()\nf.bar() #and you will see \"Hello\" on the screen\n\n"}
{"prompt": "I have Eclipse setup with PyDev and love being able to debug my scripts/apps.  I've just started playing around with Pylons and was wondering if there is a way to start up the paster server through Eclipse so I can debug my webapp?\n", "completion": "Create a new launch configuration (Python Run)\n\nMain tab\n\nUse paster-script.py as main module (you can find it in the Scripts sub-directory in your python installation directory)\n\nDon't forget to add the root folder of your application in the PYTHONPATH zone\n\nArguments\nSet the base directory to the root folder also.\n\nAs Program Arguments use \"serve development.ini\" (or whatever you use to debug your app\")\n\nCommon Tab\n\nCheck allocate console and launch in background\n"}
{"prompt": "What is the best way to check whether a given object is of a given type? How about checking whether the object inherits from a given type?\n\nLet's say I have an object o. How do I check whether it's a str?\n", "completion": "To check if the type of o is exactly str:\n\ntype(o) is str\n\n\nTo check if o is an instance of str or any subclass of str (this would be the \"canonical\" way):\n\nisinstance(o, str)\n\n\nThe following also works, and can be useful in some cases:\n\nissubclass(type(o), str)\ntype(o) in ([str] + str.__subclasses__())\n\n\nSee Built-in Functions in the Python Library Reference for relevant information.\n\nOne more note: in this case, you may actually want to use:\n\nisinstance(o, basestring)\n\n\nbecause this will also catch Unicode strings (unicode is not a subclass of str; both str and unicode are subclasses of basestring).\n\nAlternatively, isinstance accepts a tuple of classes. This will return True if x is an instance of any subclass of any of (str, unicode):\n\nisinstance(o, (str, unicode))\n\n"}
{"prompt": "Is there a framework equivalent to Guice (http://code.google.com/p/google-guice) for Python?\n", "completion": "Spring Python is an offshoot of the Java-based Spring Framework and Spring Security, targeted for Python. This project currently contains the following features:\n\n\nInversion Of Control (dependency injection) - use either classic XML, or the python @Object decorator (similar to the Spring JavaConfig subproject) to wire things together. While the @Object format isn't identical to the Guice style (centralized wiring vs. wiring information in each class), it is a valuable way to wire your python app.\nAspect-oriented Programming - apply interceptors in a horizontal programming paradigm (instead of vertical OOP inheritance) for things like transactions, security, and caching.\nDatabaseTemplate - Reading from the database requires a monotonous cycle of opening cursors, reading rows, and closing cursors, along with exception handlers. With this template class, all you need is the SQL query and row-handling function. Spring Python does the rest.\nDatabase Transactions - Wrapping multiple database calls with transactions can make your code hard to read. This module provides multiple ways to define transactions without making things complicated.\nSecurity - Plugin security interceptors to lock down access to your methods, utilizing both authentication and domain authorization.\nRemoting - It is easy to convert your local application into a distributed one. If you have already built your client and server pieces using the IoC container, then going from local to distributed is just a configuration change.\nSamples - to help demonstrate various features of Spring Python, some sample applications have been created:\n\nPetClinic - Spring Framework's sample web app has been rebuilt from the ground up using python web containers including: CherryPy. Go check it out for an example of how to use this framework. (NOTE: Other python web frameworks will be added to this list in the future).\nSpring Wiki - Wikis are powerful ways to store and manage content, so we created a simple one as a demo!\nSpring Bot - Use Spring Python to build a tiny bot to manage the IRC channel of your open source project.\n\n\n"}
{"prompt": "I'm just trying to time a piece of code. The pseudocode looks like:\n\nstart = get_ticks()\ndo_long_code()\nprint \"It took \" + (get_ticks() - start) + \" seconds.\"\n\n\nHow does this look in Python?\n\nMore specifically, how do I get the number of ticks since midnight (or however Python organizes that timing)?\n", "completion": "In the time module, there are two timing functions: time and clock. time gives you \"wall\" time, if this is what you care about.\n\nHowever, the python docs say that clock should be used for benchmarking. Note that clock behaves different in separate systems:\n\n\non MS Windows, it uses the Win32 function QueryPerformanceCounter(), with \"resolution typically better than a microsecond\". It has no special meaning, it's just a number (it starts counting the first time you call clock in your process).\n\n\n\n    # ms windows\n    t0= time.clock()\n    do_something()\n    t= time.clock() - t0 # t is wall seconds elapsed (floating point)\n\n\n\non *nix, clock reports CPU time. Now, this is different, and most probably the value you want, since your program hardly ever is the only process requesting CPU time (even if you have no other processes, the kernel uses CPU time now and then). So, this number, which typically is smaller\u00c2\u00b9 than the wall time (i.e. time.time() - t0), is more meaningful when benchmarking code:\n\n\n\n    # linux\n    t0= time.clock()\n    do_something()\n    t= time.clock() - t0 # t is CPU seconds elapsed (floating point)\n\n\nApart from all that, the timeit module has the Timer class that is supposed to use what's best for benchmarking from the available functionality.\n\n\u00c2\u00b9 unless threading gets in the way\u00e2\u0080\u00a6\n\n\u00c2\u00b2 Python \u00e2\u0089\u00a53.3: there are time.perf_counter() and time.process_time(). perf_counter is being used by the timeit module.\n"}
{"prompt": "I have one thread that writes results into a Queue.\n\nIn another thread (GUI), I periodically (in the IDLE event) check if there are results in the queue, like this:\n\ndef queue_get_all(q):\n    items = []\n    while 1:\n        try:\n            items.append(q.get_nowait())\n        except Empty, e:\n            break\n    return items\n\n\nIs this a good way to do it ? \n\nEdit:\n\n\n  I'm asking because sometimes the\n  waiting thread gets stuck for a few\n  seconds without taking out new\n  results.\n\n\nThe \"stuck\" problem turned out to be because I was doing the processing in the idle event handler, without making sure that such events are actually generated by calling wx.WakeUpIdle, as is recommended.\n", "completion": "If you're always pulling all available items off the queue, is there any real point in using a queue, rather than just a list with a lock?  ie:\n\nfrom __future__ import with_statement\nimport threading\n\nclass ItemStore(object):\n    def __init__(self):\n        self.lock = threading.Lock()\n        self.items = []\n\n    def add(self, item):\n        with self.lock:\n            self.items.append(item)\n\n    def getAll(self):\n        with self.lock:\n            items, self.items = self.items, []\n        return items\n\n\nIf you're also pulling them individually, and making use of the blocking behaviour for empty queues, then you should use Queue, but your use case looks much simpler, and might be better served by the above approach.\n\n[Edit2]  I'd missed the fact that you're polling the queue from an idle loop, and from your update, I see that the problem isn't related to contention, so the below approach isn't really relevant to your problem.  I've left it in in case anyone finds a blocking variant of this useful:\n\nFor cases where you do want to block until you get at least one result, you can modify the above code to wait for data to become available through being signalled by the producer thread.  Eg.\n\nclass ItemStore(object):\n    def __init__(self):\n        self.cond = threading.Condition()\n        self.items = []\n\n    def add(self, item):\n        with self.cond:\n            self.items.append(item)\n            self.cond.notify() # Wake 1 thread waiting on cond (if any)\n\n    def getAll(self, blocking=False):\n        with self.cond:\n            # If blocking is true, always return at least 1 item\n            while blocking and len(self.items) == 0:\n                self.cond.wait()\n            items, self.items = self.items, []\n        return items\n\n"}
{"prompt": "Coming from a C# background the naming convention for variables and method names are usually either CamelCase or Pascal Case:\n\n// C# example\nstring thisIsMyVariable = \"a\"\npublic void ThisIsMyMethod()\n\n\nIn Python, I have seen the above but I have also seen underscores being used:\n\n# python example\nthis_is_my_variable = 'a'\ndef this_is_my_function():\n\n\nIs there a more preferable, definitive coding style for Python?\n", "completion": "See Python PEP 8.\n\n\n  Function names should be lowercase,\n  with words separated by underscores as\n  necessary to improve readability.\n  \n  mixedCase is allowed only in contexts\n  where that's already the prevailing\n  style\n\n\nVariables...\n\n\n  Use the function naming rules:\n  lowercase with words separated by\n  underscores as necessary to improve\n  readability.\n\n\nPersonally, I deviate from this because I also prefer mixedCase over lower_case for my own projects.\n"}
{"prompt": "I've done some Python but have just now starting to use Ruby\nI could use a good explanation of the difference between \"self\" in these two languages.  \n\nObvious on first glance:\nSelf is not a keyword in Python, but there is a \"self-like\" value no matter what you call it.\nPython methods receive self as an explicit argument, whereas Ruby does not.\nRuby sometimes has methods explicitly defined as part of self using dot notation.\n\nInitial Googling reveals\nhttp://rubylearning.com/satishtalim/ruby_self.html\nhttp://www.ibiblio.org/g2swap/byteofpython/read/self.html  \n", "completion": "Python is designed to support more than just object-oriented programming. Preserving the same interface between methods and functions lets the two styles interoperate more cleanly.\n\nRuby was built from the ground up to be object-oriented. Even the literals are objects (evaluate 1.class and you get Fixnum). The language was built such that self is a reserved keyword that returns the current instance wherever you are.\n\nIf you're inside an instance method of one of your class, self is a reference to said instance. \n\nIf you're in the definition of the class itself (not in a method), self is the class itself:\n\nclass C\n  puts \"I am a #{self}\"\n  def instance_method\n    puts 'instance_method'\n  end\n  def self.class_method\n    puts 'class_method'\n  end\nend\n\n\nAt class definition time, 'I am a C' will be printed.\n\nThe straight 'def' defines an instance method, whereas the 'def self.xxx' defines a class method.\n\nc=C.new\n\nc.instance_method\n#=> instance_method\nC.class_method\n#=> class_method\n\n"}
{"prompt": "The standalone flashplayer takes no arguments other than a .swf file when you launch it from the command line. I need the player to go full screen, no window borders and such. This can be accomplished by hitting ctrl+f once the program has started. I want to do this programmatically as I need it to launch into full screen without any human interaction.\n\nMy guess is that I need to some how get a handle to the window and then send it an event that looks like the \"ctrl+f\" keystroke. \n\nIf it makes any difference, it looks like flashplayer is a gtk application and I have python with pygtk installed.\n\nUPDATE (the solution I used... thanks to ypnos' answer):\n\n./flashplayer http://example.com/example.swf & sleep 3 && ~/xsendkey -window \"Adobe Flash Player 10\" Control+F\n\n", "completion": "You can use a dedicated application which sends the keystroke to the window manager, which should then pass it to flash, if the window starts as being the active window on the screen. This is quite error prone, though, due to delays between starting flash and when the window will show up.\n\nFor example, your script could do something like this:\nflashplayer *.swf\nsleep 3 && xsendkey Control+F\n\nThe application xsendkey can be found here: http://people.csail.mit.edu/adonovan/hacks/xsendkey.html\nWithout given a specific window, it will send it to the root window, which is handled by your window manager. You could also try to figure out the Window id first, using xprop or something related to it.\n\nAnother option is a Window manager, which is able to remember your settings and automatically apply them. Fluxbos for example provides this feature. You could set fluxbox to make the Window decor-less and stretch it over the whole screen, if flashplayer supports being resized. This is also not-so-nice, as it would probably affect all the flashplayer windows you open ever.\n"}
{"prompt": "How do I write a decorator that restores the current working directory to what it was before the decorated function was called?  In other words, if I use the decorator on a function that does an os.chdir(), the cwd will not be changed after the function is called.\n", "completion": "The answer for a decorator has been given; it works at the function definition stage as requested.\n\nWith Python 2.5+, you also have an option to do that at the function call stage using a context manager:\n\nfrom __future__ import with_statement # needed for 2.5 \u00e2\u0089\u00a4 Python < 2.6\nimport contextlib, os\n\n@contextlib.contextmanager\ndef remember_cwd():\n    curdir= os.getcwd()\n    try: yield\n    finally: os.chdir(curdir)\n\n\nwhich can be used if needed at the function call time as:\n\nprint \"getcwd before:\", os.getcwd()\nwith remember_cwd():\n    walk_around_the_filesystem()\nprint \"getcwd after:\", os.getcwd()\n\n\nIt's a nice option to have.\n\nEDIT: I added error handling as suggested by codeape. Since my answer has been voted up, it's fair to offer a complete answer, all other issues aside.\n"}
{"prompt": "I'm writing a simulator in Python, and am curious about options and opinions regarding basic 2D animations. By animation, I'm referring to rendering on the fly, not displaying prerendered images.\n\nI'm currently using matplotlib (Wxagg backend), and it's possible that I'll be able to continue using it, but I suspect it won't be able to sufficiently scale in terms of performance or capabilities.\n\nRequirements are:\n\n\nCross-platform (Linux, MacOS X,\nWindows)\nLow complexity overhead\nPlays well with wxpython (at least won't step on each other's toes unduly)\nInteractivity. Detect when objects are clicked on, moused over, etc. \n\n\nNote that high performance isn't on the list, but the ability to handle ~100 bitmap objects on the screen would be good.\n\nYour thoughts?\n", "completion": "I am a fan of pyglet which is a completely self contained library for doing graphical work under win32, linux, and OS X. \n\nIt has very low overhead, and you can see this for yourself from the tutorial on the website. It should play well with wxpython, or at least I seem to recall posts to the mailing list about wxpython and pyglet being used together.\n\nIt however does not offer selection of objects via mouse clicks - this you will have to handle yourself. Generally speaking for a 2D application this is not too difficult to do.\n\nmactorii is an OS X application of mine written in pure python+pyglet, and has some basic animation (scrolling) and click detection. It doesn't use wxpython, but perhaps it will give you an idea of what is involved. Note however mactorii is using the old pyglet api, so the run loop I have in there is obsolete. I will get around to updating it one day... :P\n"}
{"prompt": "I'm developing a website (in Django) that uses OpenID to authenticate users. As I'm currently only running on my local machine I can't authenticate using one of the OpenID providers on the web. So I figure I need to run a local OpenID server that simply lets me type in a username and then passes that back to my main app.\n\nDoes such an OpenID dev server exist? Is this the best way to go about it?\n", "completion": "The libraries at OpenID Enabled ship with examples that are sufficient to run a local test provider.  Look in the examples/djopenid/ directory of the python-openid source distribution.  Running that will give you an instance of this test provider.\n"}
{"prompt": "This is really two questions, but they are so similar, and to keep it simple, I figured I'd just roll them together:\n\n\nFirstly: Given an established python project, what are some decent ways to speed it up beyond just plain in-code optimization?\nSecondly:  When writing a program from scratch in python, what are some good ways to greatly improve performance?\n\n\nFor the first question, imagine you are handed a decently written project and you need to improve performance, but you can't seem to get much of a gain through refactoring/optimization.  What would you do to speed it up in this case short of rewriting it in something like C?\n", "completion": "Regarding \"Secondly: When writing a program from scratch in python, what are some good ways to greatly improve performance?\"\n\nRemember the Jackson rules of optimization: \n\n\nRule 1: Don't do it.\nRule 2 (for experts only): Don't do it yet.\n\n\nAnd the Knuth rule:\n\n\n\"Premature optimization is the root of all evil.\"\n\n\nThe more useful rules are in the General Rules for Optimization.\n\n\nDon't optimize as you go.  First get it right.  Then get it fast.  Optimizing a wrong program is still wrong.\nRemember the 80/20 rule.\nAlways run \"before\" and \"after\" benchmarks.  Otherwise, you won't know if you've found the 80%.\nUse the right algorithms and data structures.  This rule should be first.  Nothing matters as much as algorithm and data structure.\n\n\nBottom Line\n\nYou can't prevent or avoid the \"optimize this program\" effort.  It's part of the job.  You have to plan for it and do it carefully, just like the design, code and test activities.\n"}
{"prompt": "I have a list string tag.\n\nI am trying to initialize a dictionary with the key as the tag string and values as the array index.\n\nfor i, ithTag in enumerate(tag):\n    tagDict.update(ithTag=i)\n\n\nThe above returns me {'ithTag': 608}  608 is the 608th index\n\nMy problem is that while the i is being interpreted as a variable, Python is treating the \"ithTag\" as a string instead of a variable.\n\nI'm confused, it is kind of hard to google these kind of specific questions.  I hope I worded the title of this question correctly,\n\nThanks!\n", "completion": "You actually want to do this:\n\nfor i, tag in enumerate(tag):\n    tagDict[tag] = i\n\n\nThe .update() method is used for updating a dictionary using another dictionary, not for changing a single key/value pair.\n"}
{"prompt": "py2exe does not work with the standard email module\n\nHello. I am trying to use py2exe for converting a script into an exe. The build process shows this:\n\n\n\nThe following modules appear to be missing\n\n['email.Encoders', 'email.Generator', 'email.Iterators', 'email.MIMEBase', 'email.MIMEMultipart', 'email.MIMEText', 'email.Utils', 'email.base64MIME']\n\nThe executable does not work. The referenced modules are not included. I researched this on the Internet and I found out that py2exe has a problem with the Lazy import used in the standard lib email module. Unfortunately I have not succeeded in finding a workaround for this problem. Can anyone help?\n\nThank you,\n\nP.S.\nImports in the script look like this:\n\nCode: Select all\n    import string,time,sys,os,smtplib\n    from email.MIMEMultipart import MIMEMultipart\n    from email.MIMEBase import MIMEBase\n    from email.MIMEText import MIMEText\n    from email import Encoders\n", "completion": "Have a look at this question how-to-package-twisted-program-with-py2exe it seems to be the same problem.\n\nThe answer given there is to explicitly include the modules on the command line to py2exe.\n"}
{"prompt": "I've discovered that cElementTree is about 30 times faster than xml.dom.minidom and I'm rewriting my XML encoding/decoding code. However, I need to output XML that contains CDATA sections and there doesn't seem to be a way to do that with ElementTree.\n\nCan it be done?\n", "completion": "After a bit of work, I found the answer myself. Looking at the ElementTree.py source code, I found there was special handling of XML comments and preprocessing instructions. What they do is create a factory function for the special element type that uses a special (non-string) tag value to differentiate it from regular elements.\n\ndef Comment(text=None):\n    element = Element(Comment)\n    element.text = text\n    return element\n\n\nThen in the _write function of ElementTree that actually outputs the XML, there's a special case handling for comments:\n\nif tag is Comment:\n    file.write(\"<!-- %s -->\" % _escape_cdata(node.text, encoding))\n\n\nIn order to support CDATA sections, I create a factory function called CDATA, extended the ElementTree class and changed the _write function to handle the CDATA elements.\n\nThis still doesn't help if you want to parse an XML with CDATA sections and then output it again with the CDATA sections, but it at least allows you to create XMLs with CDATA sections programmatically, which is what I needed to do.\n\nThe implementation seems to work with both ElementTree and cElementTree.\n\nimport elementtree.ElementTree as etree\n#~ import cElementTree as etree\n\ndef CDATA(text=None):\n    element = etree.Element(CDATA)\n    element.text = text\n    return element\n\nclass ElementTreeCDATA(etree.ElementTree):\n    def _write(self, file, node, encoding, namespaces):\n        if node.tag is CDATA:\n            text = node.text.encode(encoding)\n            file.write(\"\\n<![CDATA[%s]]>\\n\" % text)\n        else:\n            etree.ElementTree._write(self, file, node, encoding, namespaces)\n\nif __name__ == \"__main__\":\n    import sys\n\n    text = \"\"\"\n    <?xml version='1.0' encoding='utf-8'?>\n    <text>\n    This is just some sample text.\n    </text>\n    \"\"\"\n\n    e = etree.Element(\"data\")\n    cdata = CDATA(text)\n    e.append(cdata)\n    et = ElementTreeCDATA(e)\n    et.write(sys.stdout, \"utf-8\")\n\n"}
{"prompt": "I use a 3rd party tool that outputs a file in Unicode format.  However, I prefer it to be in ASCII. The tool does not have settings to change the file format.\n\nWhat is the best way to convert the entire file format using Python?\n", "completion": "You can convert the file easily enough just using the unicode function, but you'll run into problems with Unicode characters without a straight ASCII equivalent.\n\nThis blog recommends the unicodedata  module, which seems to take care of roughly converting characters without direct corresponding ASCII values, e.g.\n\n>>> title = u\"Kl\u00c3\u00bcft skr\u00c3\u00a4ms inf\u00c3\u00b6r p\u00c3\u00a5 f\u00c3\u00a9d\u00c3\u00a9ral \u00c3\u00a9lectoral gro\u00c3\u009fe\"\n\n\nis typically converted to \n\nKlft skrms infr p fdral lectoral groe\n\n\nwhich is pretty wrong. However, using the unicodedata module, the result can be much closer to the original text:\n\n>>> import unicodedata\n>>> unicodedata.normalize('NFKD', title).encode('ascii','ignore')\n'Kluft skrams infor pa federal electoral groe'\n\n"}
{"prompt": "Sometimes I break long conditions in IFs to several lines. The most obvious way to do this is:\n\n  if (cond1 == 'val1' and cond2 == 'val2' and\n      cond3 == 'val3' and cond4 == 'val4'):\n      do_something\n\n\nIsn't very very appealing visually, because the action blends with the conditions. However, it is the natural way using correct Python indentation of 4 spaces.\n\nFor the moment I'm using:\n\n  if (    cond1 == 'val1' and cond2 == 'val2' and\n          cond3 == 'val3' and cond4 == 'val4'):\n      do_something\n\n\nBut this isn't very pretty. :-)\n\nCan you recommend an alternative way ?\n", "completion": "You don't need to use 4 spaces on your second conditional line.  Maybe use:\n\nif (cond1 == 'val1' and cond2 == 'val2' and \n       cond3 == 'val3' and cond4 == 'val4'):\n    do_something\n\n\nAlso, don't forget the whitespace is more flexible than you might think:\n\nif (   \n       cond1 == 'val1' and cond2 == 'val2' and \n       cond3 == 'val3' and cond4 == 'val4'\n   ):\n    do_something\nif    (cond1 == 'val1' and cond2 == 'val2' and \n       cond3 == 'val3' and cond4 == 'val4'):\n    do_something\n\n\nBoth of those are fairly ugly though.\n\nMaybe lose the brackets (the Style Guide discourages this though)?\n\nif cond1 == 'val1' and cond2 == 'val2' and \\\n   cond3 == 'val3' and cond4 == 'val4':\n    do_something\n\n\nThis at least gives you some differentiation.\n\nOr even:\n\nif cond1 == 'val1' and cond2 == 'val2' and \\\n                       cond3 == 'val3' and \\\n                       cond4 == 'val4':\n    do_something\n\n\nI think I prefer:\n\nif cond1 == 'val1' and \\\n   cond2 == 'val2' and \\\n   cond3 == 'val3' and \\\n   cond4 == 'val4':\n    do_something\n\n\nHere's the Style Guide, which (since 2010) recommends using brackets.\n"}
{"prompt": "Still 'diving in' to Python, and want to make sure I'm not overlooking something. I wrote a script that extracts files from several zip files, and saves the extracted files together in one directory. To prevent duplicate filenames from being over-written, I wrote this little function - and I'm just wondering if there is a better way to do this?\nThanks!\n\ndef unique_filename(file_name):\ncounter = 1\nfile_name_parts = os.path.splitext(file_name) # returns ('/path/file', '.ext')\nwhile os.path.isfile(file_name): \n    file_name = file_name_parts[0] + '_' + str(counter) + file_name_parts[1]\n    counter += 1\nreturn file_name\n\n\nI really do require the files to be in a single directory, and numbering duplicates is definitely acceptable in my case, so I'm not looking for a more robust method (tho' I suppose any pointers are welcome), but just to make sure that what this accomplishes is getting done the right way.\n", "completion": "One issue is that there is a race condition in your above code, since there is a gap between testing for existance, and creating the file.  There may be security implications to this (think about someone maliciously inserting a symlink to a sensitive file which they wouldn't be able to overwrite, but your program running with a higher privilege could)  Attacks like these are why things like os.tempnam() are deprecated.\n\nTo get around it, the best approach is to actually try create the file in such a way that you'll get an exception if it fails, and on success, return the actually opened file object.  This can be done with the lower level os.open functions, by passing both the os.O_CREAT and os.O_EXCL flags.  Once opened, return the actual file (and optionally filename) you create.  Eg, here's your code modified to use this approach (returning a (file, filename) tuple):\n\ndef unique_file(file_name):\n    counter = 1\n    file_name_parts = os.path.splitext(file_name) # returns ('/path/file', '.ext')\n    while 1:\n        try:\n            fd = os.open(file_name, os.O_CREAT | os.O_EXCL | os.O_RDRW)\n            return os.fdopen(fd), file_name\n        except OSError:\n            pass\n        file_name = file_name_parts[0] + '_' + str(counter) + file_name_parts[1]\n        counter += 1\n\n\n[Edit]  Actually, a better way, which will handle the above issues for you, is probably to use the tempfile module, though you may lose some control over the naming.  Here's an example of using it (keeping a similar interface):\n\ndef unique_file(file_name):\n    dirname, filename = os.path.split(file_name)\n    prefix, suffix = os.path.splitext(filename)\n\n    fd, filename = tempfile.mkstemp(suffix, prefix+\"_\", dirname)\n    return os.fdopen(fd), filename\n\n>>> f, filename=unique_file('/home/some_dir/foo.txt')\n>>> print filename\n/home/some_dir/foo_z8f_2Z.txt\n\n\nThe only downside with this approach is that you will always get a filename with some random characters in it, as there's no attempt to create an unmodified file (/home/some_dir/foo.txt) first.\nYou may also want to look at tempfile.TemporaryFile and NamedTemporaryFile, which will do the above and also automatically delete from disk when closed.\n"}
{"prompt": "In the Python documentation\nit says:\n\n\n  A thread can be flagged as a \"daemon thread\". The significance of this\n  flag is that the entire Python program exits when only daemon threads\n  are left. The initial value is inherited from the creating thread.\n\n\nDoes anyone have a clearer explanation of what that means or a practical example showing where you would want to set threads as daemonic?\n\n\n\nTo clarify for me:\n\nso the only time you wouldn't set threads as daemonic is if you wanted them to continue running after the main thread exits?\n", "completion": "Some threads do background tasks, like sending keepalive packets, or performing periodic garbage collection, or whatever. These are only useful when the main program is running, and it's okay to kill them off once the other, non-daemon, threads have exited.\n\nWithout daemon threads, you'd have to keep track of them, and tell them to exit, before your program can completely quit. By setting them as daemon threads, you can let them run and forget about them, and when your program quits, any daemon threads are killed automatically.\n"}
{"prompt": "dir(re.compile(pattern)) \n\n\ndoes not return pattern as one of the lists's elements. Namely it returns:\n\n['__copy__', '__deepcopy__', 'findall', 'finditer', 'match', 'scanner', 'search', 'split', 'sub', 'subn']\n\n\nAccording to the manual, it is supposed to contain \n\n\n  the object's attributes' names, the\n  names of its class's attributes, and\n  recursively of the attributes of its\n  class's base classes.\n\n\nIt says also that\n\n\n  The list is not necessarily complete.\n\n\nIs there a way to get the complete list? I always assumed that dir returns a complete list but apparently it does not...\n\nAlso: is there a way to list only attributes? Or only methods?\n\nEdit: this is actually a bug in python -> supposedly it is fixed in the 3.0 branch (and perhaps also in 2.6)\n", "completion": "For the complete list of attributes, the short answer is: no. The problem is that the attributes are actually defined as the arguments accepted by the getattr built-in function. As the user can reimplement __getattr__, suddenly allowing any kind of attribute, there is no possible generic way to generate that list. The dir function returns the keys in the __dict__ attribute, i.e. all the attributes accessible if the __getattr__ method is not reimplemented.\n\nFor the second question, it does not really make sense. Actually, methods are callable attributes, nothing more. You could though filter callable attributes, and, using the inspect module determine the class methods, methods or functions.\n"}
{"prompt": "How, in the simplest possible way, distinguish between Windows XP and Windows Vista, using Python and pywin32 or wxPython?\n\nEssentially, I need a function that called will return True iff current OS is Vista:\n\n>>> isWindowsVista()\nTrue\n\n", "completion": "Python has the lovely 'platform' module to help you out.\n\n>>> import platform\n>>> platform.win32_ver()\n('XP', '5.1.2600', 'SP2', 'Multiprocessor Free')\n>>> platform.system()\n'Windows'\n>>> platform.version()\n'5.1.2600'\n>>> platform.release()\n'XP'\n\n\nNOTE: As mentioned in the comments proper values may not be returned when using older versions of python.\n"}
{"prompt": "I have a dict, which I need to pass key/values as keyword arguments.. For example..\n\nd_args = {'kw1': 'value1', 'kw2': 'value2'}\nexample(**d_args)\n\n\nThis works fine, but if there are values in the d_args dict that are not accepted by the example function, it obviously dies.. Say, if the example function is defined as def example(kw2):\n\nThis is a problem since I don't control either the generation of the d_args, or the example function.. They both come from external modules, and example only accepts some of the keyword-arguments from the dict..\n\nIdeally I would just do\n\nparsed_kwargs = feedparser.parse(the_url)\nvalid_kwargs = get_valid_kwargs(parsed_kwargs, valid_for = PyRSS2Gen.RSS2)\nPyRSS2Gen.RSS2(**valid_kwargs)\n\n\nI will probably just filter the dict, from a list of valid keyword-arguments, but I was wondering: Is there a way to programatically list the keyword arguments the a specific function takes?\n", "completion": "A little nicer than inspecting the code object directly and working out the variables is to use the inspect module.\n\n>>> import inspect\n>>> def func(a,b,c=42, *args, **kwargs): pass\n>>> inspect.getargspec(func)\n(['a', 'b', 'c'], 'args', 'kwargs', (42,))\n\n\nIf you want to know if its callable with a particular set of args, you need the args without a default already specified.  These can be got by:\n\ndef getRequiredArgs(func):\n    args, varargs, varkw, defaults = inspect.getargspec(func)\n    if defaults:\n        args = args[:-len(defaults)]\n    return args   # *args and **kwargs are not required, so ignore them.\n\n\nThen a function to tell what you are missing from your particular dict is:\n\ndef missingArgs(func, argdict):\n    return set(getRequiredArgs(func)).difference(argdict)\n\n\nSimilarly, to check for invalid args, use:\n\ndef invalidArgs(func, argdict):\n    args, varargs, varkw, defaults = inspect.getargspec(func)\n    if varkw: return set()  # All accepted\n    return set(argdict) - set(args)\n\n\nAnd so a full test if it is callable is :\n\ndef isCallableWithArgs(func, argdict):\n    return not missingArgs(func, argdict) and not invalidArgs(func, argdict)\n\n\n(This is good only as far as python's arg parsing.  Any runtime checks for invalid values in kwargs obviously can't be detected.)\n"}
{"prompt": "The only thing I can get python omnicomplete to work with are system modules. I get nothing for help with modules in my site-packages or modules that I'm currently working on.\n", "completion": "Once I generated ctags for one of my site-packages, it started working for that package -- so I'm guessing that the omnicomplete function depends on ctags for non-sys modules.\n\nEDIT: Not true at all.\n\nHere's the problem -- poor testing on my part -- omnicomplete WAS working for parts of my project, just not most of it.\n\nThe issue was that I'm working on a django project, and in order to import django.db, you need to have an environment variable set. Since I couldn't import django.db, any class that inherited from django.db, or any module that imported a class that inherited from django.db wouldn't complete.\n"}
{"prompt": "In many symbolic math systems, such as Matlab or Mathematica, you can use a variable like Ans or % to retrieve the last computed value. Is there a similar facility in the Python shell?\n", "completion": "Underscore.\n\n>>> 5+5\n10\n>>> _\n10\n>>> _ + 5\n15\n>>> _\n15\n\n"}
{"prompt": "Is there any meaningful distinction between:\n\nclass A(object):\n    foo = 5   # some default value\n\n\nvs.\n\nclass B(object):\n    def __init__(self, foo=5):\n        self.foo = foo\n\n\nIf you're creating a lot of instances, is there any difference in performance or space requirements for the two styles? When you read the code, do you consider the meaning of the two styles to be significantly different?\n", "completion": "Beyond performance considerations, there is a significant semantic difference.  In the class attribute case, there is just one object referred to.  In the instance-attribute-set-at-instantiation, there can be multiple objects referred to.  For instance\n\n>>> class A: foo = []\n>>> a, b = A(), A()\n>>> a.foo.append(5)\n>>> b.foo\n[5]\n>>> class A:\n...  def __init__(self): self.foo = []\n>>> a, b = A(), A()\n>>> a.foo.append(5)\n>>> b.foo    \n[]\n\n"}
{"prompt": "I currently do my textfile manipulation through a bunch of badly remembered AWK, sed, Bash and a tiny bit of Perl.\n\nI've seen mentioned a few places that python is good for this kind of thing, I know a little and I would like to know more. Is Python a good choice for this, and is there a good book or guide to learning how to use Python to replace shell scripting, AWK, sed and friends?\n", "completion": "Any shell has several sets of features.\n\n\nThe Essential Linux/Unix commands.  All of these are available through the subprocess library.  This isn't always the best first choice for doing all external commands.  Look also at shutil for some commands that are separate Linux commands, but you could probably implement directly in your Python scripts.  Another huge batch of Linux commands are in the os library; you can do these more simply in Python.\n\nAnd -- bonus! -- more quickly.  Each separate Linux command in the shell (with a few exceptions) forks a subprocess.  By using Python shutil and os modules, you don't fork a subprocess.\nThe shell environment features.  This includes stuff that sets a command's environment (current directory and environment variables and what-not).  You can easily manage this from Python directly.\nThe shell programming features.  This is all the process status code checking, the various logic commands (if, while, for, etc.) the test command and all of it's relatives.  The function definition stuff.  This is all much, much easier in Python.  This is one of the huge victories in getting rid of bash and doing it in Python.\nInteraction features.  This includes command history and what-not.  You don't need this for writing shell scripts.  This is only for human interaction, and not for script-writing.\nThe shell file management features.  This includes redirection and pipelines.  This is trickier.  Much of this can be done with subprocess.  But some things that are easy in the shell are unpleasant in Python.  Specifically stuff like (a | b; c ) | something >result.  This runs two processes in parallel (with output of a as input to b), followed by a third process.  The output from that sequence is run in parallel with something and the output is collected into a file named result.  That's just complex to express in any other language.\n\n\nSpecific programs (awk, sed, grep, etc.) can often be rewritten as Python modules.  Don't go overboard.  Replace what you need and evolve your \"grep\" module.  Don't start out writing a Python module that replaces \"grep\".\n\nThe best thing is that you can do this in steps.\n\n\nReplace AWK and PERL with Python.  Leave everything else alone.\nLook at replacing GREP with Python.  This can be a bit more complex, but your version of GREP can be tailored to your processing needs.\nLook at replacing FIND with Python loops that use os.walk.  This is a big win because you don't spawn as many processes.\nLook at replacing common shell logic (loops, decisions, etc.) with Python scripts.\n\n"}
{"prompt": "Imagine that you have:\n\nkeys = ('name', 'age', 'food')\nvalues = ('Monty', 42, 'spam')\n\n\nWhat is the simplest way to produce the following dictionary ?\n\ndict = {'name' : 'Monty', 'age' : 42, 'food' : 'spam'}\n\n\nThis code works, but I'm not really proud of it :\n\ndict = {}\njunk = map(lambda k, v: dict.update({k: v}), keys, values)\n\n", "completion": "Like this:\n\n>>> keys = ['a', 'b', 'c']\n>>> values = [1, 2, 3]\n>>> dictionary = dict(zip(keys, values))\n>>> print dictionary\n{'a': 1, 'b': 2, 'c': 3}\n\n\nVoila :-)  The pairwise dict constructor and zip function are awesomely useful: https://docs.python.org/2/library/functions.html#func-dict\n"}
{"prompt": "When using __import__ with a dotted name, something like: somepackage.somemodule, the module returned isn't somemodule, whatever is returned seems to be mostly empty! what's going on here?\n", "completion": "From the python docs on __import__:\n\n\n__import__( name[, globals[, locals[, fromlist[, level]]]])\n\n  \n  ...\n  \n  When the name variable is of the form\n  package.module, normally, the\n  top-level package (the name up till\n  the first dot) is returned, not the\n  module named by name. However, when a\n  non-empty fromlist argument is given,\n  the module named by name is returned.\n  This is done for compatibility with\n  the bytecode generated for the\n  different kinds of import statement;\n  when using \"import spam.ham.eggs\", the\n  top-level package spam  must be placed\n  in the importing namespace, but when\n  using \"from spam.ham import eggs\", the\n  spam.ham subpackage must be used to\n  find the eggs variable. As a\n  workaround for this behavior, use\n  getattr() to extract the desired\n  components. For example, you could\n  define the following helper:\n\ndef my_import(name):\n    mod = __import__(name)\n    components = name.split('.')\n    for comp in components[1:]:\n        mod = getattr(mod, comp)\n    return mod\n\n\n\nTo paraphrase:\n\nWhen you ask for somepackage.somemodule, __import__ returns somepackage.__init__.py, which is often empty.\n\nIt will return somemodule if you provide fromlist (a list of the variable names inside somemodule you want, which are not actually returned)\n\nYou can also, as I did, use the function they suggest.\n\nNote: I asked this question fully intending to answer it myself. There was a big bug in my code, and having misdiagnosed it, it took me a long time to figure it out, so I figured I'd help the SO community out and post the gotcha I ran into here.\n"}
{"prompt": "How do I get the inverse of a matrix in python? I've implemented it myself, but it's pure python, and I suspect there are faster modules out there to do it.\n", "completion": "You should have a look at numpy if you do matrix manipulation. This is a module mainly written in C, which will be much faster than programming in pure python. Here is an example of how to invert a matrix, and do other matrix manipulation.\n\nfrom numpy import matrix\nfrom numpy import linalg\nA = matrix( [[1,2,3],[11,12,13],[21,22,23]]) # Creates a matrix.\nx = matrix( [[1],[2],[3]] )                  # Creates a matrix (like a column vector).\ny = matrix( [[1,2,3]] )                      # Creates a matrix (like a row vector).\nprint A.T                                    # Transpose of A.\nprint A*x                                    # Matrix multiplication of A and x.\nprint A.I                                    # Inverse of A.\nprint linalg.solve(A, x)     # Solve the linear equation system.\n\n\nYou can also have a look at the array module, which is a much more efficient implementation of lists when you have to deal with only one data type.\n"}
{"prompt": "From what I understand, the parent attribute of a db.Model (typically defined/passed in the constructor call) allows you to define hierarchies in your data models. As a result, this increases the size of the entity group. However, it's not very clear to me why we would want to do that. Is this strictly for ACID compliance? I would like to see scenarios where each is best suited or more appropriate.\n", "completion": "There are several differences:\n\n\nAll entities with the same ancestor are in the same entity group. Transactions can only affect entities inside a single entity group.\nAll writes to a single entity group are serialized, so throughput is limited.\nThe parent entity is set on creation and is fixed. References can be changed at any time.\nWith reference properties, you can only query for direct relationships, but with parent properties you can use the .ancestor() filter to find everything (directly or indirectly) descended from a given ancestor.\nEach entity has only a single parent, but can have multiple reference properties.\n\n"}
{"prompt": "I have a Bluehost account where I can run Python scripts as CGI. I guess it's the simplest CGI, because to run I have to define the following in .htaccess:\n\nOptions +ExecCGI\nAddType text/html py\nAddHandler cgi-script .py\n\n\nNow, whenever I look up web programming with Python, I hear a lot about WSGI and how most frameworks use it. But I just don't understand how it all fits together, especially when my web server is given (Apache running at a host's machine) and not something I can really play with (except defining .htaccess commands).\n\nHow are WSGI, CGI, and the frameworks all connected? What do I need to know, install, and do if I want to run a web framework (say web.py or CherryPy) on my basic CGI configuration? How to install WSGI support?\n", "completion": "How WSGI, CGI, and the frameworks are all connected ?\n\nApache listens on port 80.  It gets an HTTP request.  It parses the request to find a way to respond.  Apache has a LOT of choices for responding.   One way to respond is to use CGI to run a script.  Another way to respond is to simply serve a file.  \n\nIn the case of CGI, Apache prepares an environment and invokes the script through the CGI protocol.  This is a standard Unix Fork/Exec situation -- the CGI subprocess inherits an OS environment including the socket and stdout.  The CGI subprocess writes a response, which goes back to Apache; Apache sends this response to the browser.\n\nCGI is primitive and annoying.  Mostly because it forks a subprocess for every request, and subprocess must exit or close stdout and stderr to signify end of response.\n\nWSGI is an interface that is based on the CGI design pattern.  It is not necessarily CGI -- it does not have to fork a subprocess for each request.  It can be CGI, but it doesn't have to be.\n\nWSGI adds to the CGI design pattern in several important ways.  It parses the HTTP Request Headers for you and adds these to the environment.  It supplies any POST-oriented input as a file-like object in the environment.  It also provides you a function that will formulate the response, saving you from a lot of formatting details.\n\nWhat do I need to know / install / do if I want to run a web framework (say web.py or cherrypy) on my basic CGI configuration ?\n\nRecall that forking a subprocess is expensive.  There are two ways to work around this.\n\n\nEmbedded mod_wsgi or mod_python embeds Python inside Apache; no process is forked.  Apache runs the Django application directly.\nDaemon mod_wsgi or mod_fastcgi allows Apache to interact with a separate daemon (or \"long-running process\"), using the WSGI protocol.  You start your long-running Django process, then you configure Apache's mod_fastcgi to communicate with this process.\n\n\nNote that mod_wsgi can work in either mode: embedded or daemon.\n\nWhen you read up on mod_fastcgi, you'll see that Django uses flup to create a WSGI-compatible interface from the information provided by mod_fastcgi.  The pipeline works like this.\n\nApache -> mod_fastcgi -> FLUP (via FastCGI protocol) -> Django (via WSGI protocol)\n\n\nDjango has several \"django.core.handlers\" for the various interfaces.\n\nFor mod_fastcgi, Django provides a manage.py runfcgi that integrates FLUP and the handler.\n\nFor mod_wsgi, there's a core handler for this.\n\nHow to install WSGI support ?\n\nFollow these instructions.\n\nhttp://code.google.com/p/modwsgi/wiki/IntegrationWithDjango\n\nFor background see this\n\nhttp://docs.djangoproject.com/en/dev/howto/deployment/#howto-deployment-index\n"}
{"prompt": "I'd like to write Python scripts that drive Visual Studio 2008 and Visual C++ 2008.  All the examples I've found so far use win32com.client.Dispatch.  This works fine for Excel 2007 and Word 2007 but fails for Visual Studio 2008:\n\nimport win32com.client\napp1 = win32com.client.Dispatch( 'Excel.Application' ) # ok\napp2 = win32com.client.Dispatch( 'Word.Application' )  # ok\napp3 = win32com.client.Dispatch( 'MSDev.Application' ) # error\n\n\nAny ideas?  Does Visual Studio 2008 use a different string to identify itself?  Is the above method out-dated?\n", "completion": "Depending on what exactly you're trying to do, AutoIt may meet your needs.  In fact, I'm sure it will do anything you need it to do.\n\nTaken from my other post about how to use AutoIt with Python:\n\nimport win32com.client\noAutoItX = win32com.client.Dispatch( \"AutoItX3.Control\" )\n\noAutoItX.Opt(\"WinTitleMatchMode\", 2) #Match text anywhere in a window title\n\nwidth = oAutoItX.WinGetClientSizeWidth(\"Firefox\")\nheight = oAutoItX.WinGetClientSizeHeight(\"Firefox\")\n\nprint width, height\n\n\nYou can of course use any of the AutoItX functions (note that that link goes to the AutoIt function reference, the com version of AutoIt - AutoItX has a subset of that list...the documentation is included in the download) in this way.  I don't know what you're wanting to do, so I can't point you towards the appropriate functions, but this should get you started.\n"}
{"prompt": "I need to perform a filtered query from within a django template, to get a set of objects equivalent to python code within a view:\n\nqueryset = Modelclass.objects.filter(somekey=foo)\n\n\nIn my template I would like to do\n\n{% for object in data.somekey_set.FILTER %}\n\n\nbut I just can't seem to find out how to write FILTER.\n", "completion": "You can't do this, which is by design.  The Django framework authors intended a strict separation of presentation code from data logic.  Filtering models is data logic, and outputting HTML is presentation logic.\n\nSo you have several options.  The easiest is to do the filtering, then pass the result to render_to_response.  Or you could write a method in your model so that you can say {% for object in data.filtered_set %}.  Finally, you could write your own template tag, although in this specific case I would advise against that.\n"}
{"prompt": "Say I have a form like:\n\nclass GeneralForm(forms.Form):\n    field1 = forms.IntegerField(required=False)\n    field2 = forms. IntegerField(required=False)\n\n\nAnd I want to show it twice on a page within one form tag each time with a different prefix e.g.,:\n\nrest of page ...\n<form ..>\nGeneralForm(data,prefix=\"form1\").as_table()\nGeneralForm(data,prefix=\"form2\").as_table()\n<input type=\"submit\" />\n</form>\nrest of page ...\n\n\nWhen the user submits this, how do I get the submitted form back into two separate forms to do validation, and redisplay it?\n\nThis was the only documentation I could find and it's peckish.\n", "completion": "You process each form as you normally would, ensuring that you create instances which have the same prefixes as those used to generate the form initially.\n\nHere's a slightly awkward example using the form you've given, as I don't know what the exact use case is:\n\ndef some_view(request):\n    if request.method == 'POST':\n        form1 = GeneralForm(request.POST, prefix='form1')\n        form2 = GeneralForm(request.POST, prefix='form2')\n        if all([form1.is_valid(), form2.is_valid()]):\n            pass # Do stuff with the forms\n    else:\n        form1 = GeneralForm(prefix='form1')\n        form2 = GeneralForm(prefix='form2')\n    return render_to_response('some_template.html', {\n        'form1': form1,\n        'form2': form2,\n    })\n\n\nHere's some real-world sample code which demonstrates processing forms using the prefix:\n\nhttp://collingrady.wordpress.com/2008/02/18/editing-multiple-objects-in-django-with-newforms/\n"}
{"prompt": "So I'm teaching myself Python, and I'm having an issue with lists. I want to pass my function a list and pop items off it while retaining the original list. How do I make python \"instance\" the passed list rather that passing a pointer to the original one?\n\nExample:\n\ndef burninate(b):\n    c = []\n    for i in range(3):\n    \tc.append(b.pop())\n    return c\n\na = range(6)\nd = burninate(a)\nprint a, d\n\n\nOutput: [0, 1, 2] [5, 4, 3]\nDesired output: [0, 1, 2, 3, 4, 5] [5, 4, 3]\n\nThanks!\n", "completion": "As other answers have suggested, you can provide your function with a copy of the list.\n\nAs an alternative, your function could take a copy of the argument:\n\ndef burninate(b):\n    c = []\n    b = list(b)\n    for i in range(3):\n        c.append(b.pop())\n    return c\n\n\nBasically, you need to be clear in your mind (and in your documentation) whether your function will change its arguments.  In my opinion, functions that return computed values should not change their arguments, and functions that change their arguments should not return anything.  See python's [].sort(), [].extend(), {}.update(), etc. for examples.  Obviously there are exceptions (like .pop()).\n\nAlso, depending on your particular case, you could rewrite the function to avoid using pop() or other functions that modify the argument.  e.g.\n\ndef burninante(b):\n    return b[:-4:-1]   # return the last three elements in reverse order\n\n"}
{"prompt": "As an example, lets say I wanted to list the frequency of each letter of the alphabet in a string. What would be the easiest way to do it?\n\nThis is an example of what I'm thinking of... the question is how to make allTheLetters equal to said letters without something like allTheLetters = \"abcdefg...xyz\". In many other languages I could just do letter++ and increment my way through the alphabet, but thus far I haven't come across a way to do that in python.\n\ndef alphCount(text):\n  lowerText = text.lower()\n  for letter in allTheLetters:  \n    print letter + \":\", lowertext.count(letter)\n\n", "completion": "The question you've asked (how to iterate through the alphabet) is not the same question as the problem you're trying to solve (how to count the frequency of letters in a string).\n\nYou can use string.lowercase, as other posters have suggested:\n\nimport string\nallTheLetters = string.lowercase\n\n\nTo do things the way you're \"used to\", treating letters as numbers, you can use the \"ord\" and \"chr\" functions.  There's absolutely no reason to ever do exactly this, but maybe it comes closer to what you're actually trying to figure out:\n\ndef getAllTheLetters(begin='a', end='z'):\n    beginNum = ord(begin)\n    endNum = ord(end)\n    for number in xrange(beginNum, endNum+1):\n        yield chr(number)\n\n\nYou can tell it does the right thing because this code prints True:\n\nimport string\nprint ''.join(getAllTheLetters()) == string.lowercase\n\n\nBut, to solve the problem you're actually trying to solve, you want to use a dictionary and collect the letters as you go:\n\nfrom collections import defaultdict    \ndef letterOccurrances(string):\n    frequencies = defaultdict(lambda: 0)\n    for character in string:\n        frequencies[character.lower()] += 1\n    return frequencies\n\n\nUse like so:\n\noccs = letterOccurrances(\"Hello, world!\")\nprint occs['l']\nprint occs['h']\n\n\nThis will print '3' and '1' respectively.\n\nNote that this works for unicode as well:\n\n# -*- coding: utf-8 -*-\noccs = letterOccurrances(u\"h\u00c3\u00a9\u00c4\u00ba\u00c4\u00ba\u00c3\u00b3, \u00e1\u00ba\u0083\u00c3\u00b3\u00c5\u0095\u00c4\u00bad!\")\nprint occs[u'l']\nprint occs[u'\u00c4\u00ba']\n\n\nIf you were to try the other approach on unicode (incrementing through every character) you'd be waiting a long time; there are millions of unicode characters.\n\nTo implement your original function (print the counts of each letter in alphabetical order) in terms of this:\n\ndef alphCount(text):\n    for character, count in sorted(letterOccurrances(text).iteritems()):\n        print \"%s: %s\" % (character, count)\n\nalphCount(\"hello, world!\")\n\n"}
{"prompt": "I'm running Python 2.6 on Unix and when I run the interactive prompt (SQLite is supposed to be preinstalled) I get:\n\n[root@idev htdocs]# python\nPython 2.6 (r26:66714, Oct 23 2008, 16:25:34)\n[GCC 3.2.2 20030222 (Red Hat Linux 3.2.2-5)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sqlite\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nImportError: No module named sqlite\n>>>\n\n\nHow do I resolve this?\n", "completion": "The error: \n\nImportError: No module named _sqlite3\n\n\nmeans that SQLite 3 does not find the associated shared library. On Mac OS X it's _sqlite3.so and it should be the same on other Unix systems. \n\nTo resolve the error you have to locate the _sqlite3.so library on your computer and then check your PYTHONPATH for this directory location. \n\nTo print the Python search path enter the following in the Python shell:\n\nimport sys\nprint sys.path\n\n\nIf the directory containing your library is missing you can try adding it interactively with \n\nsys.path.append('/your/dir/here')\n\n\nand try \n\nimport sqlite3\n\n\nagain. If this works you have to add this directory permanently to your PYTHONPATH environment variable. \n\nPS: If the library is missing you should (re-)install the module.\n"}
{"prompt": "Is there a simple way to support wildcards (\"*\") when searching strings - without using RegEx?\n\nUsers are supposed to enter search terms using wildcards, but should not have to deal with the complexity of RegEx:\n\n\"foo*\"   =>  str.startswith(\"foo\")\n\"*foo\"   =>  str.endswith(\"foo\")\n\"*foo*\"  =>  \"foo\" in str\n\n\n(it gets more complicated when there are multiple search terms though, e.g. \"foo*bar*baz\")\n\nThis seems like a common issue, so I wonder whether there's a ready-made solution for it.\n\nAny help would be greatly appreciated!\n", "completion": "You could try the fnmatch module, it's got a shell-like wildcard syntax.\n"}
{"prompt": "I have a third-party product, a terminal emulator, which provides a DLL that can be linked to a C program to basically automate the driving of this product (send keystrokes, detect what's on the screen and so forth).\n\nI want to drive it from a scripting language (I'm comfortable with Python and slightly less so with Perl) so that we don't have to compile and send out executables to our customers whenever there's a problem found.\n\nWe also want the customers to be able to write their own scripts using ours as baselines and they won't entertain the idea of writing and compiling C code.\n\nWhat's a good way of getting Python/Perl to interface to a Windows DLL. My first thought was to write a server program and have a Python script communicate with it via TCP but there's got to be an easier solution.\n", "completion": "One way to call C libraries from Python is to use ctypes:\n\n>>> from ctypes import *\n>>> windll.user32.MessageBoxA(None, \"Hello world\", \"ctypes\", 0);\n\n"}
{"prompt": "Is there explicit support for Single Table Inheritance in Django? Last I heard, the feature was still under development and debate. \n\nAre there libraries/hacks I can use in the meantime to capture the basic behavior? I have a hierarchy that mixes different objects. The canonical example of a corporation structure with an Employee class, subclasses for types of employees, and a manager_id (parent_id) would be a good approximation of the problem I am solving. \n\nIn my case, I would like to represent the idea that an employee can manage other employees while being managed by a different employee. There are not separate classes for Manager and Worker, which makes this hard to spread across tables. Sub-classes would represent types of employees-programmers, accountants, sales, etc and would be independent of who supervises who (OK, I guess it's no longer a typical corporation in some respect). \n", "completion": "There are currently two forms of inheritance in Django - MTI (model table inheritance) and ABC (abstract base classes).\n\nI wrote a tutorial on what's going on under the hood.\n\nYou can also reference the official docs on model inheritance.\n"}
{"prompt": "Emacs does not recognize my correct python path. I think it is a general problem with emacs not recognizing my environment variables. I have GNU Emacs 22.1.1 (i386-apple-darwin8.9.1, Carbon Version 1.6.0) of 2007-06-17 installed.\n\nI have set the PYTHONPATH in my ~/.bashrc. Maybe I should set it somewhere else?\n", "completion": ".bashrc only gets read when a shell starts; it won't affect Carbon Emacs. Instead, use setenv in your .emacs:\n\n(setenv \"PYTHONPATH\" \"PATH_STRING_HERE\")\n\n\nYou can set PYTHONPATH for the entire Mac OS session, by adding it to ~/.MacOSX/environment.plist (more here). You probably don't want to do this unless you have XCode (and its property list editor) installed.\n\n(Via Procrastiblog)\n"}
{"prompt": "I'm planning to write gateway web application, which would need \"terminal window\" with VT100/ANSI escape code support. Are there any AJAX based alternatives for such a task?\n\nI'm thinking something like this: http://tryruby.hobix.com/\n\nMy preferred backend for the system is Python/Twisted/Pylons, but since I'm just planning, I will explore every option.\n", "completion": "Try\n\nAnyTerm\n\nAjaxTerm\n\nWebShell\n"}
{"prompt": "I want to detect whether module has changed. Now, using inotify is simple, you just need to know the directory you want to get notifications from.\n\nHow do I retrieve a module's path in python?\n", "completion": "import a_module\nprint a_module.__file__\n\n\nWill actually give you the path to the .pyc file that was loaded, at least on Mac OS X. So I guess you can do\n\nimport os\npath = os.path.dirname(amodule.__file__)\n\n\nYou can also try\n\npath = os.path.abspath(amodule.__file__)\n\n\nTo get the directory to look for changes.\n"}
{"prompt": "Recently I wrote a function to generate certain sequences with nontrivial constraints. The problem came with a natural recursive solution. Now it happens that, even for relatively small input, the sequences are several thousands, thus I would prefer to use my algorithm as a generator instead of using it to fill a list with all the sequences.\n\nHere is an example. Suppose we want to compute all the permutations of a string with a recursive function. The following naive algorithm takes an extra argument 'storage' and appends a permutation to it whenever it finds one:\n\ndef getPermutations(string, storage, prefix=\"\"):\n   if len(string) == 1:\n      storage.append(prefix + string)   # <-----\n   else:\n      for i in range(len(string)):\n         getPermutations(string[:i]+string[i+1:], storage, prefix+string[i])\n\nstorage = []\ngetPermutations(\"abcd\", storage)\nfor permutation in storage: print permutation\n\n\n(Please don't care about inefficiency, this is only an example.)\n\nNow I want to turn my function into a generator, i.e. to yield a permutation instead of appending it to the storage list:\n\ndef getPermutations(string, prefix=\"\"):\n   if len(string) == 1:\n      yield prefix + string             # <-----\n   else:\n      for i in range(len(string)):\n         getPermutations(string[:i]+string[i+1:], prefix+string[i])\n\nfor permutation in getPermutations(\"abcd\"):\n   print permutation\n\n\nThis code does not work (the function behaves like an empty generator).\n\nAm I missing something?\nIs there a way to turn the above recursive algorithm into a generator without replacing it with an iterative one?\n", "completion": "def getPermutations(string, prefix=\"\"):\n    if len(string) == 1:\n        yield prefix + string\n    else:\n        for i in xrange(len(string)):\n            for perm in getPermutations(string[:i] + string[i+1:], prefix+string[i]):\n                yield perm\n\n\nOr without an accumulator:\n\ndef getPermutations(string):\n    if len(string) == 1:\n        yield string\n    else:\n        for i in xrange(len(string)):\n            for perm in getPermutations(string[:i] + string[i+1:]):\n                yield string[i] + perm\n\n"}
{"prompt": "I have a Django model with a large number of fields and 20000+ table rows.  To facilitate human readable URLs and the ability to break down the large list into arbitrary sublists, I would like to have a URL that looks like this:\n\n/browse/<name1>/<value1>/<name2>/<value2>/ .... etc ....\n\n\nwhere 'name' maps to a model attribute and 'value' is the search criteria for that attribute.  Each \"name\" will be treated like a category to return subsets of the model instances where the categories match.\n\nNow, this could be handled with GET parameters, but I prefer more readable URLs for both the user's sake and the search engines.  These URLs subsets will be embedded on each page that displays this model, so it seems worth the effort to make pretty URLs.\n\nIdeally each name/value pair will be passed to the view function as a parameter named name1, name2, etc.  However, I don't believe it's possible to defined named patterns via a regex's matched text.  Am I wrong there?\n\nSo, it seems I need to do something like this:\n\nurlpatterns = patterns('',\n    url(r'^browse/(?:([\\w]+)/([\\w]+)/)+$', 'app.views.view', name=\"model_browse\"),\n)\n\n\nIt seems this should match any sets of two name/value pairs.  While it matches it successfully, it only passes the last name/value pair as parameters to the view function.  My guess is that each match is overwriting the previous match.  Under the guess that the containing (?:...)+ is causing it, I tried a simple repeating pattern instead:\n\nurlpatterns = patterns('',\n    url(r'^browse/([\\w]+/)+$', 'app.views.view', name=\"model_browse\"),\n)\n\n\n... and got the same problem, but this time *args only includes the last matched pattern.\n\nIs this a limitation of Django's url dispatcher, and/or Python's regex support?  It seems either of these methods should work.  Is there a way to achieve this without hardcoding each possible model attribute in the URL as an optional (.*) pattern?\n", "completion": "A possibility that you might consider is matching the entire string of possible values within the url pattern portion and pull out the specific pieces within your view.  As an example:\n\nurlpatterns = patterns('',\n    url(r'^browse/(?P<match>.+)/$', 'app.views.view', name='model_browse'),\n)\n\ndef view(request, match):\n    pieces = match.split('/')\n    # even indexed pieces are the names, odd are values\n    ...\n\n\nNo promises about the regexp I used, but I think you understand what I mean.\n\n(Edited to try and fix the regexp.)\n"}
{"prompt": "I don't care what the differences are.  I just want to know whether the contents are different.\n", "completion": "The low level way:\n\nfrom __future__ import with_statement\nwith open(filename1) as f1:\n   with open(filename2) as f2:\n      if f1.read() == f2.read():\n         ...\n\n\nThe high level way:\n\nimport filecmp\nif filecmp.cmp(filename1, filename2, shallow=False):\n   ...\n\n"}
{"prompt": "In my website, users have the possibility to store links.\n\nDuring typing the internet address into the designated field I would like to display a suggest/autocomplete box similar to Google Suggest or the Chrome Omnibar.\n\nExample:\n\nUser is typing as URL:\n\nhttp://www.sta\n\n\nSuggestions which would be displayed:\n\nhttp://www.staples.com\nhttp://www.starbucks.com\nhttp://www.stackoverflow.com\n\n\nHow can I achieve this while not reinventing the wheel? :)\n", "completion": "You could try with\nhttp://google.com/complete/search?output=toolbar&q=keyword\n\nand then parse the xml result.\n"}
{"prompt": "Considering the criteria listed below, which of Python, Groovy or Ruby would you use?\n\n\nCriteria (Importance out of 10, 10 being most important)\nRichness of API/libraries available (eg. maths, plotting, networking) (9)\nAbility to embed in desktop (java/c++) applications (8)\nEase of deployment (8)\nAbility to interface with DLLs/Shared Libraries (7)\nAbility to generate GUIs (7)\nCommunity/User support (6)\nPortability (6)\nDatabase manipulation (3)\nLanguage/Semantics (2)\n\n", "completion": "I think it's going to be difficult to get an objective comparison.  I personally prefer Python.  To address one of your criteria, Python was designed from the start to be an embeddable language.  It has a very rich C API, and the interpreter is modularized to make it easy to call from C.  If Java is your host environment, you should look at Jython, an implementation of Python inside the Java environment (VM and libs).\n"}
{"prompt": "Most of the documentation available for building Python extension modules\nuses distutils, but I would like to achieve this by using the appropriate\npython autoconf & automake macros instead.\n\nI'd like to know if there is an open source project out there that does\nexactly this. Most of the ones I've found end up relying on a setup.py file.\nUsing that approach works, but unfortunately ends up rebuilding the entire\nsource tree any time I make a modification to the module source files.\n", "completion": "Supposing that you have a project with a directory called src, so let's follow the follow steps to get a python extension built and packaged using autotools:\n\nCreate the Makefile.am files\n\nFirst, you need to create one Makefile.am in the root of your project, basically (but not exclusively) listing the subdirectories that should also be processed. You will end up with something like this:\n\nSUBDIRS = src\n\n\nThe second one, inside the src directory will hold the instructions to actually compile your python extension. It will look like this:\n\nmyextdir = $(pkgpythondir)\nmyext_PYTHON = file1.py file2.py\n\npyexec_LTLIBRARIES = _myext.la\n\n_myext_la_SOURCES = myext.cpp\n_myext_la_CPPFLAGS = $(PYTHON_CFLAGS)\n_myext_la_LDFLAGS = -module -avoid-version -export-symbols-regex initmyext\n_myext_la_LIBADD = $(top_builddir)/lib/libhollow.la\n\nEXTRA_DIST = myext.h\n\n\nWrite the configure.ac\n\nThis file must be created in the root directory of the project and must list all libraries, programs or any kind of tool that your project needs to be built, such as a compiler, linker, libraries, etc.\n\nLazy people, like me, usually don't create it from scratch, I prefer to use the autoscan tool, that looks for things that you are using and generate a configure.scan file that can be used as the basis for your real configure.ac.\n\nTo inform automake that you will need python stuff, you can add this to your configure.ac:\n\ndnl python checks (you can change the required python version bellow)\nAM_PATH_PYTHON(2.7.0)\nPY_PREFIX=`$PYTHON -c 'import sys ; print sys.prefix'`\nPYTHON_LIBS=\"-lpython$PYTHON_VERSION\"\nPYTHON_CFLAGS=\"-I$PY_PREFIX/include/python$PYTHON_VERSION\"\nAC_SUBST([PYTHON_LIBS])\nAC_SUBST([PYTHON_CFLAGS])\n\n\nWrap up\n\nBasically, automake has a built-in extension that knows how to deal with python stuff, you just need to add it to your configure.ac file and then take the advantage of this feature in your Makefile.am.\n\nPyGtk is definitely an awesome example, but it's pretty big, so maybe you will want to check another project, like Guake\n"}
{"prompt": "It seems like there should be a simpler way than:\n\nimport string\ns = \"string. With. Punctuation?\" # Sample string \nout = s.translate(string.maketrans(\"\",\"\"), string.punctuation)\n\n\nIs there?\n", "completion": "From an efficiency perspective, you're not going to beat translate() - it's performing raw string operations in C with a lookup table - there's not much that will beat that but writing your own C code.\nIf speed isn't a worry, another option though is:\n\nexclude = set(string.punctuation)\ns = ''.join(ch for ch in s if ch not in exclude)\n\n\nThis is faster than s.replace with each char, but won't perform as well as non-pure python approaches such as regexes or string.translate, as you can see from the below timings.  For this type of problem, doing it at as low a level as possible pays off.\n\nTiming code:\n\nimport re, string, timeit\n\ns = \"string. With. Punctuation\"\nexclude = set(string.punctuation)\ntable = string.maketrans(\"\",\"\")\nregex = re.compile('[%s]' % re.escape(string.punctuation))\n\ndef test_set(s):\n    return ''.join(ch for ch in s if ch not in exclude)\n\ndef test_re(s):  # From Vinko's solution, with fix.\n    return regex.sub('', s)\n\ndef test_trans(s):\n    return s.translate(table, string.punctuation)\n\ndef test_repl(s):  # From S.Lott's solution\n    for c in string.punctuation:\n        s=s.replace(c,\"\")\n    return s\n\nprint \"sets      :\",timeit.Timer('f(s)', 'from __main__ import s,test_set as f').timeit(1000000)\nprint \"regex     :\",timeit.Timer('f(s)', 'from __main__ import s,test_re as f').timeit(1000000)\nprint \"translate :\",timeit.Timer('f(s)', 'from __main__ import s,test_trans as f').timeit(1000000)\nprint \"replace   :\",timeit.Timer('f(s)', 'from __main__ import s,test_repl as f').timeit(1000000)\n\n\nThis gives the following results:\n\nsets      : 19.8566138744\nregex     : 6.86155414581\ntranslate : 2.12455511093\nreplace   : 28.4436721802\n\n"}
{"prompt": "I'm writing some mail-processing software in Python that is encountering strange bytes in header fields.  I suspect this is just malformed mail; the message itself claims to be us-ascii, so I don't think there is a true encoding, but I'd like to get out a unicode string approximating the original one without throwing a UnicodeDecodeError .\n\nSo, I'm looking for a function that takes a str and optionally some hints and does its darndest to give me back a unicode.  I could write one of course, but if such a function exists its author has probably thought a bit deeper about the best way to go about this.\n\nI also know that Python's design prefers explicit to implicit and that the standard library is designed to avoid implicit magic in decoding text. I just want to explicitly say \"go ahead and guess\".\n", "completion": "You may be interested in Universal Encoding Detector.\n"}
{"prompt": "I saw this the other day (scroll all the way down to see some of the clever stuff): \n\n\n  http://www.mono-project.com/docs/tools+libraries/tools/repl/\n\n\nAnd wondered whether something like this exists for Python.\n\nSo, is there a good Python GUI shell that can do stuff like that C# shell can do?\n\nEdit: Here are links to screenshots from the article, showing what I'm interested in doing.\n\nAn example of the type of things I'm interested: \n\nhttp://www.mono-project.com/archived/images/7/75/GSharpPlot.png\n\nThey are able to add hooks to produce GUI elements like the plot, or even do silly things like:\n\nhttp://www.mono-project.com/archived/images/b/bf/GSharpRenderHandler.png\n\nI don't think this is possible with any of the console shells I've tried (the regular python shell, ipython).\n\nEdit: I'm not looking for an IDE. If you look at the link, you'll get an idea of what I want.\n", "completion": "One project I'm aware of that provides similar features (inline plotting, customisable rendering) is Reinteract.  Another (though possibly a bit heavyweight for general usage) is SAGE which provides functionality for web-based notebooks.\n\nThese aren't quite shells - they're designed more as a mathematical notebook (so for instance, you can modify an earlier result and have the change propogate to later calculations), but they're close to what you're looking for, and could probably be modified to be used as such.\n"}
{"prompt": "In .Net, I found this great library, HtmlAgilityPack that allows you  to easily parse non-well-formed HTML using XPath.  I've used this for a couple years in my .Net sites, but I've had to settle for more painful libraries for my Python, Ruby and other projects.  Is anyone aware of similar libraries for other languages?\n", "completion": "I'm surprised there isn't a single mention of lxml. It's blazingly fast and will work in any environment that allows CPython libraries.\n\nHere's how you can parse HTML via XPATH using lxml.\n\n>>> from lxml import etree\n>>> doc = '<foo><bar></bar></foo>'\n>>> tree = etree.HTML(doc)\n\n>>> r = tree.xpath('/foo/bar')\n>>> len(r)\n1\n>>> r[0].tag\n'bar'\n\n>>> r = tree.xpath('bar')\n>>> r[0].tag\n'bar'\n\n"}
{"prompt": "I have a huge text file (~1GB) and sadly the text editor I use won't read such a large file.  However, if I can just split it into two or three parts I'll be fine, so, as an exercise I wanted to write a program in python to do it.  \n\nWhat I think I want the program to do is to find the size of a file, divide that number into parts, and for each part, read up to that point in chunks, writing to a filename.nnn output file, then read up-to the next line-break and write that, then close the output file, etc.  Obviously the last output file just copies to the end of the input file.\n\nCan you help me with the key filesystem related parts: filesize, reading and writing in chunks and reading to a line-break?\n\nI'll be writing this code test-first, so there's no need to give me a complete answer, unless its a one-liner ;-)\n", "completion": "linux has a split command\n\nsplit -l 100000 file.txt\n\nwould split into files of equal 100,000 line size\n"}
{"prompt": "I need to set my process to run under 'nobody', I've found os.setuid(), but how do I find uid if I have login?\n\nI've found out that uids are in /etc/passwd, but maybe there is a more pythonic way than scanning /etc/passwd. Anybody?\n", "completion": "You might want to have a look at the pwd module in the python stdlib, for example:\n\nimport pwd\npw = pwd.getpwnam(\"nobody\")\nuid = pw.pw_uid\n\n\nit uses /etc/passwd (well, technically it uses the posix C API, so I suppose it might work on an OS if it didn't use /etc/passwd but exposed the needed functions) but is cleaner than parsing it manually\n"}
{"prompt": "What is and how can it be used in C#.\nCan you use the same concept in Python/Perl?\n", "completion": "A partial type (it doesn't have to be a class; structs and interfaces can be partial too) is basically a single type which has its code spread across multiple files.\n\nThe main use for this is to allow a code generator (e.g. a Visual Studio designer) to \"own\" one file, while hand-written code is put in another.\n\nI've no idea whether Python/Perl have the same capabilities, I'm afraid.\n"}
{"prompt": "We're looking into transport/protocol solutions and were about to do various performance tests, so I thought I'd check with the community if they've already done this:\n\nHas anyone done server performance tests for simple echo services as well as serialization/deserialization for various messages sizes comparing EJB3, Thrift, and Protocol Buffers on Linux?\n\nPrimarily languages will be Java, C/C++, Python, and PHP.\n\nUpdate: I'm still very interested in this, if anyone has done any further benchmarks please let me know. Also, very interesting benchmark showing compressed JSON performing similar / better than Thrift / Protocol Buffers, so I'm throwing JSON into this question as well.\n", "completion": "Latest comparison available here at the thrift-protobuf-compare project wiki. It includes many other serialization libraries.\n"}
{"prompt": "I am getting an 'access is denied' error when I attempt to delete a folder that is not empty. I used the following command in my attempt: os.remove(\"/folder_name\"). \n\nWhat is the most effective way of removing/deleting a folder/directory that is not empty?\n", "completion": "import shutil\n\nshutil.rmtree('/folder_name')\n\n\nStandard Library Reference: shutil.rmtree.\n"}
{"prompt": "I am trying to write a decorator to do logging:\n\ndef logger(myFunc):\n    def new(*args, **keyargs):\n        print 'Entering %s.%s' % (myFunc.im_class.__name__, myFunc.__name__)\n        return myFunc(*args, **keyargs)\n\n    return new\n\nclass C(object):\n    @logger\n    def f():\n        pass\n\nC().f()\n\n\nI would like this to print:\n\nEntering C.f\n\n\nbut instead I get this error message:\n\nAttributeError: 'function' object has no attribute 'im_class'\n\n\nPresumably this is something to do with the scope of 'myFunc' inside 'logger', but I've no idea what.\n", "completion": "Claudiu's answer is correct, but you can also cheat by getting the class name off of the self argument.  This will give misleading log statements in cases of inheritance, but will tell you the class of the object whose method is being called.  For example:\n\nfrom functools import wraps  # use this to preserve function signatures and docstrings\ndef logger(func):\n    @wraps(func)\n    def with_logging(*args, **kwargs):\n        print \"Entering %s.%s\" % (args[0].__class__.__name__, func.__name__)\n        return func(*args, **kwargs)\n    return with_logging\n\nclass C(object):\n    @logger\n    def f(self):\n        pass\n\nC().f()\n\n\nAs I said, this won't work properly in cases where you've inherited a function from a parent class; in this case you might say\n\nclass B(C):\n    pass\n\nb = B()\nb.f()\n\n\nand get the message Entering B.f where you actually want to get the message Entering C.f since that's the correct class.  On the other hand, this might be acceptable, in which case I'd recommend this approach over Claudiu's suggestion.\n"}
{"prompt": "Assume I have the following list:\n\nfoo = ['a', 'b', 'c', 'd', 'e']\n\n\nWhat is the simplest way to retrieve an item at random from this list?\n", "completion": "Use random.choice:\n\nimport random\n\nfoo = ['a', 'b', 'c', 'd', 'e']\nprint(random.choice(foo))\n\n"}
{"prompt": "I'm a relatively new convert to Python.  I've written some code to grab/graph data from various sources to automate some weekly reports and forecasts.  I've been intrigued by the Jython concept, and would like to port some Python code that I've written to Jython.  In order to do this quickly, I need a NumPy clone for Jython (or Java).  Is there anything like this out there?\n", "completion": "I can't find anything that's a clone of numpy, but there's a long list of Java numerics packages here - these should all be usable from Jython. Which one meets your requirements depends on what you're doing with numpy, I guess.\n"}
{"prompt": "Is there a way to access a list(or tuple, or other iterable)'s next, or previous element while looping through with for loop?\n\nl=[1,2,3]\nfor item in l:\n    if item==2:\n        get_previous(l,item)\n\n", "completion": "Expressed as a generator function:\n\ndef neighborhood(iterable):\n    iterator = iter(iterable)\n    prev = None\n    item = iterator.next()  # throws StopIteration if empty.\n    for next in iterator:\n        yield (prev,item,next)\n        prev = item\n        item = next\n    yield (prev,item,None)\n\n\nUsage:\n\nfor prev,item,next in neighborhood(l):\n    print prev, item, next\n\n\nEdit: I thought it would reduce the readability, but this way seem to look better.\n"}
{"prompt": "If I want to split a list of words separated by a delimiter character, I can use\n\n>>> 'abc,foo,bar'.split(',')\n['abc', 'foo', 'bar']\n\n\nBut how to easily and quickly do the same thing if I also want to handle quoted-strings which can contain the delimiter character ?\n\nIn: 'abc,\"a string, with a comma\",\"another, one\"'\nOut: ['abc', 'a string, with a comma', 'another, one']\n\n\nRelated question: How can i parse a comma delimited string into a list (caveat)?\n", "completion": "import csv\n\ninput = ['abc,\"a string, with a comma\",\"another, one\"']\nparser = csv.reader(input)\n\nfor fields in parser:\n  for i,f in enumerate(fields):\n    print i,f    # in Python 3 and up, print is a function; use: print(i,f)\n\n\nResult:\n\n\n0 abc\n1 a string, with a comma\n2 another, one\n\n"}
{"prompt": "I found the Computational Geometry Algorithms Library in my search for an algorithm to decompose a concave polygon into the minimum number of convex components.  Links off the site and numerous google results indicate there are python bindings for it, which would be really handy, but all the links are dead!  What happened to it?  Where can I get it now?\n", "completion": "A rewrite of the CGAL-Python bindings has been done as part of the cgal-bindings project.  Check it out : http://code.google.com/p/cgal-bindings/\n"}
{"prompt": "I'm using urllib2 to read in a page. I need to do a quick regex on the source and pull out a few variables but urllib2 presents as a file object rather than a string.\n\nI'm new to python so I'm struggling to see how I use a file object to do this. Is there a quick way to convert this into a string?\n", "completion": "You can use Python in interactive mode to search for solutions.\n\nif f is your object, you can enter dir(f) to see all methods and attributes. There's one called read. Enter help(f.read) and it tells you that f.read() is the way to retrieve a string from an file object.\n"}
{"prompt": "I have some flatpages with empty content field and their content inside the template (given with template_name field).\n\nWhy I am using django.contrib.flatpages\n\n\nIt allows me to serve (mostly) static pages with minimal URL configuration.\nI don't have to write views for each of them.\n\n\nWhy I don't need the model FlatPage\n\n\nI leave the content empty and just supply a template path. Therefore I can take advantage of having the source in a file;\n\nI can edit the source directly from the file system, without the help of a server (such as admin).\nI can take advantage of syntax highlightning and other editor features.\n\nWith the model I have to maintain fixtures for flatpages.\n\nSo the data for the same entity is in two seperate places.\nIf I move the content inside the fixture it'll be more difficult to edit.\n\nEven if fixture maintenance was a non-issue I'd still need to dump and load these fixtures again and again during development.\n\n\n\n\nWhat I am looking for\n\nBasically; getting rid of FlatPage model while maintaining contrib.flatpages functionality. I don't have a clear idea how this should be solved. If there's a clean way of modifying (like add_to_class) FlatPages to get the information somewhere other than the database I'd prefer that. Maybe the metadata can be inserted to the templates and then a special manager that reads this data would replace the default manager of FlatPages.\n\nIf I don't prefer manual editing over admin functionality for flatpages, how can take the database out of the equation?\n", "completion": "Using the direct_to_template generic view would be a lot simpler.  You could use the passed in parameters on one view to specify the actual template in urls.py, if you don't want to add an entry for each page:\n\nr'^foo/(?P<template_name>.+)/$','direct_to_template', {'template': 'foo_index.html'}),\n\n\nThen import the template in your foo_index.html:\n\n{% include template_name %}\n\n"}
{"prompt": "How do I connect to Gmail and determine which messages have attachments?  I then want to download each attachment, printing out the Subject: and From: for each message as I process it.\n", "completion": "Hard one :-)\n\nimport email, getpass, imaplib, os\n\ndetach_dir = '.' # directory where to save attachments (default: current)\nuser = raw_input(\"Enter your GMail username:\")\npwd = getpass.getpass(\"Enter your password: \")\n\n# connecting to the gmail imap server\nm = imaplib.IMAP4_SSL(\"imap.gmail.com\")\nm.login(user,pwd)\nm.select(\"[Gmail]/All Mail\") # here you a can choose a mail box like INBOX instead\n# use m.list() to get all the mailboxes\n\nresp, items = m.search(None, \"ALL\") # you could filter using the IMAP rules here (check http://www.example-code.com/csharp/imap-search-critera.asp)\nitems = items[0].split() # getting the mails id\n\nfor emailid in items:\n    resp, data = m.fetch(emailid, \"(RFC822)\") # fetching the mail, \"`(RFC822)`\" means \"get the whole stuff\", but you can ask for headers only, etc\n    email_body = data[0][1] # getting the mail content\n    mail = email.message_from_string(email_body) # parsing the mail content to get a mail object\n\n    #Check if any attachments at all\n    if mail.get_content_maintype() != 'multipart':\n        continue\n\n    print \"[\"+mail[\"From\"]+\"] :\" + mail[\"Subject\"]\n\n    # we use walk to create a generator so we can iterate on the parts and forget about the recursive headach\n    for part in mail.walk():\n        # multipart are just containers, so we skip them\n        if part.get_content_maintype() == 'multipart':\n            continue\n\n        # is this part an attachment ?\n        if part.get('Content-Disposition') is None:\n            continue\n\n        filename = part.get_filename()\n        counter = 1\n\n        # if there is no filename, we create one with a counter to avoid duplicates\n        if not filename:\n            filename = 'part-%03d%s' % (counter, 'bin')\n            counter += 1\n\n        att_path = os.path.join(detach_dir, filename)\n\n        #Check if its already there\n        if not os.path.isfile(att_path) :\n            # finally write the stuff\n            fp = open(att_path, 'wb')\n            fp.write(part.get_payload(decode=True))\n            fp.close()\n\n\nWowww! That was something. ;-) But try the same in Java, just for fun!\n\nBy the way, I tested that in a shell, so some errors likely remain.\n\nEnjoy\n\nEDIT:\n\nBecause mail-box names can change from one country to another, I recommend doing m.list() and picking an item in it before m.select(\"the mailbox name\") to avoid this error:\n\n\n  imaplib.error: command SEARCH illegal in state AUTH, only allowed in\n  states SELECTED\n\n"}
{"prompt": "Recently I needed to do weighted random selection of elements from a list, both with and without replacement. While there are well known and good algorithms for unweighted selection, and some for weighted selection without replacement (such as modifications of the resevoir algorithm), I couldn't find any good algorithms for weighted selection with replacement. I also wanted to avoid the resevoir method, as I was selecting a significant fraction of the list, which is small enough to hold in memory.\n\nDoes anyone have any suggestions on the best approach in this situation? I have my own solutions, but I'm hoping to find something more efficient, simpler, or both.\n", "completion": "One of the fastest ways to make many with replacement samples from an unchanging list is the alias method.  The core intuition is that we can create a set of equal-sized bins for the weighted list that can be indexed very efficiently through bit operations, to avoid a binary search.  It will turn out that, done correctly, we will need to only store two items from the original list per bin, and thus can represent the split with a single percentage.\n\nLet's us take the example of five equally weighted choices, (a:1, b:1, c:1, d:1, e:1)\n\nTo create the alias lookup:\n\n\nNormalize the weights such that they sum to 1.0.  (a:0.2 b:0.2 c:0.2 d:0.2 e:0.2)  This is the probability of choosing each weight.\nFind the smallest power of 2 greater than or equal to the number of variables, and create this number of partitions, |p|.  Each partition represents a probability mass of 1/|p|.  In this case, we create 8 partitions, each able to contain 0.125.\nTake the variable with the least remaining weight, and place as much of it's mass as possible in an empty partition.  In this example, we see that a fills the first partition.  (p1{a|null,1.0},p2,p3,p4,p5,p6,p7,p8) with (a:0.075, b:0.2 c:0.2 d:0.2 e:0.2)\nIf the partition is not filled, take the variable with the most weight, and fill the partition with that variable.  \n\n\nRepeat steps 3 and 4, until none of the weight from the original partition need be assigned to the list.\n\nFor example, if we run another iteration of 3 and 4, we see \n\n(p1{a|null,1.0},p2{a|b,0.6},p3,p4,p5,p6,p7,p8) with (a:0, b:0.15 c:0.2 d:0.2 e:0.2) left to be assigned\n\nAt runtime:\n\n\nGet a U(0,1) random number, say binary 0.001100000\nbitshift it lg2(p), finding the index partition.  Thus, we shift it by 3, yielding 001.1, or position 1, and thus partition 2.\nIf the partition is split, use the decimal portion of the shifted random number to decide the split. In this case, the value is 0.5, and 0.5 < 0.6, so return a.\n\n\nHere is some code and another explanation, but unfortunately it doesn't use the bitshifting technique, nor have I actually verified it.\n"}
{"prompt": "I need a way to compute the nth root of a long integer in Python. \n\nI tried pow(m, 1.0/n), but it doesn't work:\n\n\n  OverflowError: long int too large to convert to float\n\n\nAny ideas?\n\nBy long integer I mean REALLY long integers like:\n\n\n  11968003966030964356885611480383408833172346450467339251\n  196093144141045683463085291115677488411620264826942334897996389\n  485046262847265769280883237649461122479734279424416861834396522\n  819159219215308460065265520143082728303864638821979329804885526\n  557893649662037092457130509980883789368448042961108430809620626\n  059287437887495827369474189818588006905358793385574832590121472\n  680866521970802708379837148646191567765584039175249171110593159\n  305029014037881475265618958103073425958633163441030267478942720\n  703134493880117805010891574606323700178176718412858948243785754\n  898788359757528163558061136758276299059029113119763557411729353\n  915848889261125855717014320045292143759177464380434854573300054\n  940683350937992500211758727939459249163046465047204851616590276\n  724564411037216844005877918224201569391107769029955591465502737\n  961776799311859881060956465198859727495735498887960494256488224\n  613682478900505821893815926193600121890632\n\n", "completion": "If it's a REALLY big number. You could use a binary search.\n\ndef find_invpow(x,n):\n    \"\"\"Finds the integer component of the n'th root of x,\n    an integer such that y ** n <= x < (y + 1) ** n.\n    \"\"\"\n    high = 1\n    while high ** n <= x:\n        high *= 2\n    low = high/2\n    while low < high:\n        mid = (low + high) // 2\n        if low < mid and mid**n < x:\n            low = mid\n        elif high > mid and mid**n > x:\n            high = mid\n        else:\n            return mid\n    return mid + 1\n\n\nFor example:\n\n>>> x = 237734537465873465\n>>> n = 5\n>>> y = find_invpow(x,n)\n>>> y\n2986\n>>> y**n <= x <= (y+1)**n\nTrue\n>>>\n>>> x = 119680039660309643568856114803834088331723464504673392511960931441>\n>>> n = 45\n>>> y = find_invpow(x,n)\n>>> y\n227661383982863143360L\n>>> y**n <= x < (y+1)**n\nTrue\n>>> find_invpow(y**n,n) == y\nTrue\n>>>\n\n"}
{"prompt": "I'm doing a Python script where I need to spawn several ssh-copy-id processes, and they need for me to type in a password, so i'm using PExpect.\n\nI have basically this:\n\nchild = pexpect.spawn('command')\nchild.expect('password:')\nchild.sendline('the password')\n\n\nand then I want to spawn another process, I don't care about this one anymore, whether it ended or not.\n\nchild = pexpect.spawn('command2')\nchild.expect('password:')\nchild.sendline('the password')\n\n\nAnd the code is hanging at the second \"spawn\"\n\nHowever, if I comment out the first call, the second one works, so i'm guessing that the fact that the first one is still running or something is keeping it from working.\n\nNow, the other thing I haven't been able to do is wait until the first one stops.\nI've tried:\nchild.close() - it hangs (both with True and False as parameters)\nchild.read(-1) - it hangs\nchild.expect(pexpect.EOF) - it hangs.\nchild.terminate() - it hangs  (both with True and False as parameters)\n\nAny ideas on what could be happening?\nNOTE: I'm not a Python expert, and i have never used pexpect before, so ANY idea is more than welcome.\n\nThanks!\n\n\n\nUPDATE: This is definitely related to ssh-copy-id, because with other processes, spawn works well even if they don't return.\nAlso, apparently ssh-copy-id never returns an EOF.\n", "completion": "Fortunately or not, but OpenSSH client seems to be very picky about passwords and where they come from.\n\nYou may try using Paramiko Python SSH2 library. Here's a simple example how to use it with password authentication, then issue some shell commands (echo \"...\" >> $HOME/.ssh/authorized_keys being the simplest) to add your public key on remote host.\n"}
{"prompt": "I'm looking for ways to watch mouse and keyboard events on Windows, Linux and Mac from Python.\n\nMy application is a time tracker. I'm not looking into the event, I just record the time when it happens. If there are no events for a certain time, say 10 minutes, I assume that the user has left and stop the current project.\n\nWhen the user returns (events come in again), I wait a moment (so this doesn't get triggered by the cleaning crew or your pets or an earthquake). If the events persist over a longer period of time, I assume that the user has returned and I pop up a small, inactive window where she can choose to add the time interval to \"break\", the current project (meeting, etc) or a different project.\n\nI've solved the keylogger for Windows using the pyHook.\n\nOn Linux, I have found a solution but I don't like it: I can watch all device nodes in /etc/input and update a timestamp somewhere in /var or /tmp every time I see an event. There are two drawbacks: 1. I can't tell whether the event if from the user who is running the time tracker and 2. this little program needs to be run as root (not good).\n\nOn Mac, I have no idea, yet.\n\nQuestions:\n\n\nIs there a better way to know whether the user is creating events than watching the event devices on Linux?\nAny pointers how to do that on a Mac?\n\n", "completion": "There are couple of open source apps that might give you some pointers:\n\n\nPyKeylogger is python keylogger for windows and linux\nlogKext is a c++ keylogger for mac\n\n"}
{"prompt": "Currently my code is organized in the following tree structure:\n\nsrc/\n    module1.py\n    module2.py\n    test_module1.py\n    test_module2.py\n    subpackage1/\n        __init__.py\n        moduleA.py\n        moduleB.py\n        test_moduleA.py\n        test_moduleB.py\n\n\nWhere the module*.py files contains the source code and the test_module*.py contains the TestCases for the relevant module.\n\nWith the following comands I can run the tests contained in a single file, for example:\n\n$ cd src\n$ nosetests test_filesystem.py\n..................\n----------------------------------------------------------------------\nRan 18 tests in 0.390s\n\nOK\n\n\nHow can I run all tests? I tried with nosetests -m 'test_.*' but it doesn't work.\n\n$cd src\n$ nosetests -m 'test_.*'\n\n----------------------------------------------------------------------\nRan 0 tests in 0.000s\n\nOK\n\n\nThanks\n", "completion": "Whether you seperate or mix tests and modules is probably a matter of taste, although I would strongly advocate for keeping them apart (setup reasons, code stats etc).\n\nWhen you're using nosetests, make sure that all directories with tests are real packages:\n\nsrc/\n    module1.py\n    module2.py\n    subpackage1/\n        __init__.py\n        moduleA.py\n        moduleB.py\ntests/\n    __init__.py\n    test_module1.py\n    test_module2.py\n    subpackage1/\n        __init__.py\n        test_moduleA.py\n        test_moduleB.py\n\n\nThis way, you can just run nosetests in the toplevel directory and all tests will be found. You need to make sure that src/ is on the PYTHONPATH, however, otherwise all the tests will fail due to missing imports.\n"}
{"prompt": "just want to know what are the main differences among them? and the power of each language (where it's better to use it).\n\nEdit: it's not \"vs.\" like topic, just information.\n", "completion": "In order of appearance, the languages are sed, awk, perl, python.\n\nThe sed program is a stream editor, and is designed to apply the actions from a script to each line (or, more generally, to specified ranges of lines) of the input file or files.  Its language is based on ed, the Unix editor, and although it has conditionals and so on, it is hard to work with for complex tasks.  You can work minor miracles with it - but at a cost to the hair on your head.  However, it is probably the fastest of the programs when attempting tasks within its remit.  (It has the least powerful regular expressions of the programs discussed - adequate for many purposes, but certainly not PCRE - Perl-Compatible Regular Expressions)\n\nThe awk program (name from the initials of its authors - Aho, Weinberger and Kernighan) is a tool originally for formatting reports.  It can be used as a souped up sed; in its more recent versions, it is computationally complete.  It uses an interesting idea - the program is based on 'patterns matched' and 'actions taken when the pattern matches'.  The patterns are fairly powerful (Extended Regular Expressions).  The language for the actions is similar to C.  One of the key features of awk is that it splits the input lines into fields automatically.\n\nPerl was written in part as an awk-killer and sed-killer.  Two of the programs provided with it are a2p and s2p for converting awk scripts and sed scripts into Perl.  Perl is one of the earliest of the next generation of scripting languages (Tcl/Tk can probably claim primacy).  It has powerful integrated regular expression handling with a vastly more powerful language.  It provides access to almost all system calls, and has the extensibility of the CPAN modules.  (Neither awk nor sed is extensible.)  One of Perl's mottos is \"TMTOWTDI - There's more than one way to do it\" (pronounced \"tim-toady\").  Perl has 'objects', but it is more of an add-on than a fundamental part of the language.\n\nPython was written last, and probably in part as a reaction to Perl.  It has some interesting syntactic ideas (indenting to indicate levels - no braces or equivalents).  It is more fundamentally object-oriented than Perl; it is just as extensible as Perl.\n\nOK - when to use each?\n\n\nsed - when you need to do simple text transforms on files.\nawk - when you only need simple formatting and summarization or transformation of data.\nperl - for almost any task, but especially when the task needs complex regular expressions.\npython - for the same tasks that you could use Perl for.\n\n\nI'm not aware of anything that Perl can do that Python can't, nor vice versa.  The choice between the two would depend on other factors.  I learned Perl before there was a Python, so I tend to use it.  Python has less accreted syntax and is generally somewhat simpler to learn.  Perl 6, when it becomes available, will be a fascinating development.\n\n(Note that the 'overviews' of Perl and Python, in particular, are woefully incomplete; whole books could be written on the topic.)\n"}
{"prompt": "I'm interested in how much up front validation people do in the Python they write.\n\nHere are a few examples of simple functions:\n\ndef factorial(num):\n    \"\"\"Computes the factorial of num.\"\"\"\n\ndef isPalindrome(inputStr):\n    \"\"\"Tests to see if inputStr is the same backwards and forwards.\"\"\"\n\ndef sum(nums):\n    \"\"\"Same as the built-in sum()... computes the sum of all the numbers passed in.\"\"\"\n\n\nHow thoroughly do you check the input values before beginning computation, and how do you do your checking? Do you throw some kind of proprietary exception if input is faulty (BadInputException defined in the same module, for example)? Do you just start your calculation and figure it will throw an exception at some point if bad data was passed in (\"asd\" to factorial, for example)?\n\nWhen the passed in value is supposed to be a container do you check not only the container but all the values inside it?\n\nWhat about situations like factorial, where what's passed in might be convertible to an int (e.g. a float) but you might lose precision when doing so?\n", "completion": "I assert what's absolutely essential.\n\nImportant: What's absolutely essential.  Some people over-test things.\n\ndef factorial(num):\n    assert int(num)\n    assert num > 0\n\n\nIsn't completely correct.  long is also a legal possibility.\n\ndef factorial(num):\n    assert type(num) in ( int, long )\n    assert num > 0\n\n\nIs better, but still not perfect.  Many Python types (like rational numbers, or number-like objects) can also work in a good factorial function.  It's hard to assert that an object has basic integer-like properties without being too specific and eliminating future unthought-of classes from consideration.\n\nI never define unique exceptions for individual functions.  I define a unique exception for a significant module or package.  Usually, however, just an Error class or something similar.  That way the application says except somelibrary.Error,e: which is about all you need to know.  Fine-grained exceptions get fussy and silly.\n\nI've never done this, but I can see places where it might be necessary. \n\nassert all( type(i) in (int,long) for i in someList ) \n\n\nGenerally, however, the ordinary Python built-in type checks work fine. They find almost all of the exceptional situations that matter almost all the time.   When something isn't the right type, Python raises a TypeError that always points at the right line of code.\n\nBTW.  I only add asserts at design time if I'm absolutely certain the function will be abused.  I sometimes add assertions later when I have a unit test that fails in an obscure way.\n"}
{"prompt": "I've had some experience with Pygame, but there seems to be a lot of buzz around Pyglet these days.\n\nHow do these two libraries compare? What would be the advantage of using one over the other, both in features and ease of use?\n\nFinally, would you say that one is more Pythonic than the other?\n", "completion": "Pygame: LGPL license\n\nPyglet: BSD license\n\nPygame relies on SDL libraries heavily\n\nPyglet is a pure python library with fewer dependencies, I think it requires better understanding of OpenGL\n\nPygame is around here for a long time, a lot of people used it\n\nPyglet is a new lib\n\nPygame is geared towards game development (cursors, sprites, joystick/gamepad support)\n\nPyglet is more general purpose (though it has a Sprite class)\n\nI found also this discussion on pyglet-users mailing list: from pygame+pyopengl to pyglet\n\nDisclaimer: I did not use either yet, only tried some tutorials ;-)\n"}
{"prompt": "Is there a way to find the application name of the current active window at a given time on Mac OS X using Python?\n", "completion": "This should work:\n\n#!/usr/bin/python\n\nfrom AppKit import NSWorkspace\nactiveAppName = NSWorkspace.sharedWorkspace().activeApplication()['NSApplicationName']\nprint activeAppName\n\n\nOnly works on Leopard, or on Tiger if you have PyObjC installed and happen to point at the right python binary in line one (not the case if you've installed universal MacPython, which you'd probably want to do on Tiger). But Peter's answer with the Carbon way of doing this will probably be quite a bit faster, since importing anything from AppKit in Python takes a while, or more accurately, importing something from AppKit for the first time in a Python process takes a while.\n\nIf you need this inside a PyObjC app, what I describe will work great and fast, since you only experience the lag of importing AppKit once.  If you need this to work as a command-line tool, you'll notice the performance hit.  If that's relevant to you, you're probably better off building a 10 line Foundation command line tool in Xcode using Peter's code as a starting point.\n"}
{"prompt": "The best I can come up with for now is this monstrosity:\n\n>>> datetime.utcnow() \\\n...   .replace(tzinfo=pytz.UTC) \\\n...   .astimezone(pytz.timezone(\"Australia/Melbourne\")) \\\n...   .replace(hour=0,minute=0,second=0,microsecond=0) \\\n...   .astimezone(pytz.UTC) \\\n...   .replace(tzinfo=None)\ndatetime.datetime(2008, 12, 16, 13, 0)\n\n\nI.e., in English, get the current time (in UTC), convert it to some other timezone, set the time to midnight, then convert back to UTC.\n\nI'm not just using now() or localtime() as that would use the server's timezone, not the user's timezone.\n\nI can't help feeling I'm missing something, any ideas?\n", "completion": "I think you can shave off a few method calls if you do it like this:\n\n>>> from datetime import datetime\n>>> datetime.now(pytz.timezone(\"Australia/Melbourne\")) \\\n            .replace(hour=0, minute=0, second=0, microsecond=0) \\\n            .astimezone(pytz.utc)\n\n\nBUT\u00e2\u0080\u00a6 there is a bigger problem than aesthetics in your code: it will give the wrong result on the day of the switch to or from Daylight Saving Time.\n\nThe reason for this is that neither the datetime constructors nor replace() take DST changes into account.\n\nFor example:\n\n>>> now = datetime(2012, 4, 1, 5, 0, 0, 0, tzinfo=pytz.timezone(\"Australia/Melbourne\"))\n>>> print now\n2012-04-01 05:00:00+10:00\n>>> print now.replace(hour=0)\n2012-04-01 00:00:00+10:00 # wrong! midnight was at 2012-04-01 00:00:00+11:00\n>>> print datetime(2012, 3, 1, 0, 0, 0, 0, tzinfo=tz)\n2012-03-01 00:00:00+10:00 # wrong again!\n\n\nHowever, the documentation for tz.localize() states:\n\n\n  This method should be used to construct localtimes, rather\n  than passing a tzinfo argument to a datetime constructor.\n\n\nThus, your problem is solved like so:\n\n>>> import pytz\n>>> from datetime import datetime, date, time\n\n>>> tz = pytz.timezone(\"Australia/Melbourne\")\n>>> the_date = date(2012, 4, 1) # use date.today() here\n\n>>> midnight_without_tzinfo = datetime.combine(the_date, time())\n>>> print midnight_without_tzinfo\n2012-04-01 00:00:00\n\n>>> midnight_with_tzinfo = tz.localize(midnight_without_tzinfo)\n>>> print midnight_with_tzinfo\n2012-04-01 00:00:00+11:00\n\n>>> print midnight_with_tzinfo.astimezone(pytz.utc)\n2012-03-31 13:00:00+00:00\n\n\nNo guarantees for dates before 1582, though.\n"}
{"prompt": "Is there a Pythonic way to have only one instance of a program running? \n\nThe only reasonable solution I've come up with is trying to run it as a server on some port, then second program trying to bind to same port - fails. But it's not really a great idea, maybe there's something more lightweight than this?  \n\n(Take into consideration that program is expected to fail sometimes, i.e. segfault - so things like \"lock file\" won't work)\n\nUpdate: the solutions offered are much more complex and less reliant than just having a port occupied with a non-existent server, so I'd have to go with that one.\n", "completion": "The following code should do the job, it is cross-platform and runs on Python 2.4-3.2. I tested it on Windows, OS X and Linux.\n\nfrom tendo import singleton\nme = singleton.SingleInstance() # will sys.exit(-1) if other instance is running\n\n\nThe latest code version is available singleton.py. Please file bugs here.\n\nYou can install tend using one of the following methods:\n\n\neasy_install tendo\npip install tendo\nmanually by getting it from http://pypi.python.org/pypi/tendo\n\n"}
{"prompt": "For example I can point the url '^/accounts/password/reset/$' to django.contrib.auth.views.password_reset with my template filename in the context but I think need to send more context details.\n\nI need to  know exactly what context to add for each of the password reset and change views.\n", "completion": "If you take a look at the sources for django.contrib.auth.views.password_reset you'll see that it uses RequestContext. The upshot is, you can use Context Processors to modify the context which may allow you to inject the information that you need.\n\nThe b-list has a good introduction to context processors.\n\nEdit (I seem to have been confused about what the actual question was):\n\nYou'll notice that password_reset takes a named parameter called template_name:\n\ndef password_reset(request, is_admin_site=False, \n            template_name='registration/password_reset_form.html',\n            email_template_name='registration/password_reset_email.html',\n            password_reset_form=PasswordResetForm, \n            token_generator=default_token_generator,\n            post_reset_redirect=None):\n\n\nCheck password_reset for more information.\n\n... thus, with a urls.py like:\n\nfrom django.conf.urls.defaults import *\nfrom django.contrib.auth.views import password_reset\n\nurlpatterns = patterns('',\n     (r'^/accounts/password/reset/$', password_reset, {'template_name': 'my_templates/password_reset.html'}),\n     ...\n)\n\n\ndjango.contrib.auth.views.password_reset will be called for URLs matching '/accounts/password/reset' with the keyword argument template_name = 'my_templates/password_reset.html'.\n\nOtherwise, you don't need to provide any context as the password_reset view takes care of itself. If you want to see what context you have available, you can trigger a TemplateSyntax error and look through the stack trace find the frame with a local variable named context. If you want to modify the context then what I said above about context processors is probably the way to go.\n\nIn summary: what do you need to do to use your own template? Provide a template_name keyword argument to the view when it is called. You can supply keyword arguments to views by including a dictionary as the third member of a URL pattern tuple.\n"}
{"prompt": "When writing custom classes it is often important to allow equivalence by means of the == and != operators. In Python, this is made possible by implementing the __eq__ and __ne__ special methods, respectively. The easiest way I've found to do this is the following method:\n\nclass Foo:\n    def __init__(self, item):\n        self.item = item\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        else:\n            return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n\nDo you know of more elegant means of doing this? Do you know of any particular disadvantages to using the above method of comparing __dict__s?\n\nNote: A bit of clarification--when __eq__ and __ne__ are undefined, you'll find this behavior:\n\n>>> a = Foo(1)\n>>> b = Foo(1)\n>>> a is b\nFalse\n>>> a == b\nFalse\n\n\nThat is, a == b evaluates to False because it really runs a is b, a test of identity (i.e., \"Is a the same object as b?\").\n\nWhen __eq__ and __ne__ are defined, you'll find this behavior (which is the one we're after):\n\n>>> a = Foo(1)\n>>> b = Foo(1)\n>>> a is b\nFalse\n>>> a == b\nTrue\n\n", "completion": "You need to be careful with inheritance:\n\n>>> class Foo:\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        else:\n            return False\n\n>>> class Bar(Foo):pass\n\n>>> b = Bar()\n>>> f = Foo()\n>>> f == b\nTrue\n>>> b == f\nFalse\n\n\nCheck types more strictly, like this:\n\ndef __eq__(self, other):\n    if type(other) is type(self):\n        return self.__dict__ == other.__dict__\n    return False\n\n\nBesides that, your approach will work fine, that's what special methods are there for.\n"}
{"prompt": "I have a friend who likes to use metaclasses, and regularly offers them as a solution.\n\nI am of the mind that you almost never need to use metaclasses. Why? because I figure if you are doing something like that to a class, you should probably be doing it to an object. And a small redesign/refactor is in order.\n\nBeing able to use metaclasses has caused a lot of people in a lot of places to use classes as some kind of second rate object, which just seems disastrous to me. Is programming to be replaced by meta-programming? The addition of class decorators has unfortunately made it even more acceptable.\n\nSo please, I am desperate to know your valid (concrete) use-cases for metaclasses in Python. Or to be enlightened as to why mutating classes is better than mutating objects, sometimes.\n\nI will start:\n\n\n  Sometimes when using a third-party\n  library it is useful to be able to\n  mutate the class in a certain way.\n\n\n(this is the only case I can think of, and it's not concrete)\n", "completion": "The purpose of metaclasses isn't to replace the class/object distinction with metaclass/class - it's to change the behaviour of class definitions (and thus their instances) in some way.  Effectively it's to alter the behaviour of the class statement in ways that may be more useful for your particular domain than the default.  The things I have used them for are:\n\n\nTracking subclasses, usually to register handlers.  This is handy when using a plugin style setup, where you wish to register a handler for a particular thing simply by subclassing and setting up a few class attributes.  eg.  suppose you write a handler for various music formats, where each class implements appropriate methods (play / get tags etc) for its type.  Adding a handler for a new type becomes:\n\nclass Mp3File(MusicFile):\n    extensions = ['.mp3']  # Register this type as a handler for mp3 files\n    ...\n    # Implementation of mp3 methods go here\n\n\nThe metaclass then maintains a dictionary of {'.mp3' : MP3File, ... } etc, and constructs an object of the appropriate type when you request a handler through a factory function.\nChanging behaviour.  You may want to attach a special meaning to certain attributes, resulting in altered behaviour when they are present.  For example, you may want to look for methods with the name _get_foo and _set_foo and transparently convert them to properties.  As a real-world example, here's a recipe I wrote to give more C-like struct definitions.  The metaclass is used to convert the declared items into a struct format string, handling inheritance etc, and produce a class capable of dealing with it.\n\nFor other real-world examples, take a look at various ORMs, like sqlalchemy's ORM or sqlobject.  Again, the purpose is to interpret defintions (here SQL column definitions) with a particular meaning.\n\n"}
{"prompt": "Is there a way in Python to override a class method at instance level?\nFor example:\n\nclass Dog:\n    def bark(self):\n        print \"WOOF\"\n\nboby = Dog()\nboby.bark() # WOOF\n# METHOD OVERRIDE\nboby.bark() # WoOoOoF!!\n\n", "completion": "Yes, it's possible:\n\nclass Dog:\n    def bark(self):\n        print \"Woof\"\n\ndef new_bark(self):\n    print \"Woof Woof\"\n\nfoo = Dog()\n\nfuncType = type(Dog.bark)\n\n# \"Woof\"\nfoo.bark()\n\n# replace bark with new_bark for this object only\nfoo.bark = funcType(new_bark, foo, Dog)\n\nfoo.bark()\n# \"Woof Woof\"\n\n"}
{"prompt": "I have the following classes: Ingredients, Recipe and RecipeContent...\n\nclass Ingredient(models.Model):\n    name = models.CharField(max_length=30, primary_key=True)\n    qty_on_stock = models.IntegerField()\n\n    def __unicode__(self):\n        return self.name\n\nclass Recipe(models.Model):\n    name = models.CharField(max_length=30, primary_key=True)\n    comments = models.TextField(blank=True)\n    ingredient = models.ManyToManyField(Ingredient)\n\n    def __unicode__(self):\n        return self.name\n\nclass RecipeContent(models.Model):\n    recipe = models.ForeignKey(Recipe)\n    ingredients = models.ForeignKey(Ingredient)\n    qty_used = models.IntegerField()\n\n\nbut for __unicode__() in RecipeContent I would like to use the Recipe name to which this RecipeContent belongs to... is there a way to do it?\n", "completion": "class RecipeContent(models.Model):\n  ...\n  def __unicode__(self):\n    # You can access ForeignKey properties through the field name!\n    return self.recipe.name\n\n"}
{"prompt": "I was just reading through \"Learning Python\" by Mark Lutz and came across this code sample:\n\n\n>>> L = ['grail']\n>>> L.append(L)\n>>> L\n['grail', [...]]\n\n\n\nIt was identified as a cyclic data structure.\n\nSo I was wondering, and here is my question:\n\nWhat is a 'cyclic data structure' used for in real life programming?\n\nThere seems to be a little confusion, which i think stems from the very brief code sample... here's a few more lines using the same object L\n\n\n>>> L[0]\n'grail'\n>>> L[1][0]\n'grail'\n>>> L[1][1][0]\n'grail'\n\n\n", "completion": "Lots of things.  Circular buffer, for example:  you have some collection of data with a front and a back, but an arbitrary number of nodes, and the \"next\" item from the last should take you back to the first.\n\nGraph structures are often cyclic; acyclicity is a special case.  Consider, for example, a graph containing all the cities and roads in a traveling salesman problem.\n\n\n\nOkay, here's a particular example for you.  I set up a collection of towns here in Colorado:\n\nV=[\"Boulder\", \"Denver\", \"Colorado Springs\", \"Pueblo\", \"Limon\"]\n\n\nI then set up pairs of cities where there is a road connecting them.\n\nE=[[\"Boulder\", \"Denver\"],\n   [\"Denver\", \"Colorado Springs\"],\n   [\"Colorado Springs\", \"Pueblo\"],\n   [\"Denver\", \"Limon\"],\n   [\"Colorado Springs\", \"Limon\"]]\n\n\nThis has a bunch of cycles.  For example, you can drive from Colorado Springs, to Limon, to Denver, and back to Colorado Springs.\n\nIf you create a data structure that contains all the cities in V and all the roads in E, that's a graph data structure.  This graph would have cycles.\n"}
{"prompt": "I have the following data structure (a list of lists)\n\n[\n ['4', '21', '1', '14', '2008-10-24 15:42:58'], \n ['3', '22', '4', '2somename', '2008-10-24 15:22:03'], \n ['5', '21', '3', '19', '2008-10-24 15:45:45'], \n ['6', '21', '1', '1somename', '2008-10-24 15:45:49'], \n ['7', '22', '3', '2somename', '2008-10-24 15:45:51']\n]\n\n\nI would like to be able to\n\n\nUse a function to reorder the list so that I can group by each item in the list. For example I'd like to be able to group by the second column (so that all the 21's are together) \nUse a function to only display certain values from each inner list. For example i'd like to reduce this list to only contain the 4th field value of '2somename' \n\n\nso the list would look like this \n\n[\n     ['3', '22', '4', '2somename', '2008-10-24 15:22:03'], \n     ['7', '22', '3', '2somename', '2008-10-24 15:45:51']\n]\n\n", "completion": "For the first question, the first thing you should do is sort the list by the second field:\n\nx = [\n ['4', '21', '1', '14', '2008-10-24 15:42:58'], \n ['3', '22', '4', '2somename', '2008-10-24 15:22:03'], \n ['5', '21', '3', '19', '2008-10-24 15:45:45'], \n ['6', '21', '1', '1somename', '2008-10-24 15:45:49'], \n ['7', '22', '3', '2somename', '2008-10-24 15:45:51']\n]\n\nfrom operator import itemgetter\n\nx.sort(key=itemgetter(1))\n\n\nThen you can use itertools' groupby function:\n\nfrom itertools import groupby\ny = groupby(x, itemgetter(1))\n\n\nNow y is an iterator containing tuples of (element, item iterator). It's more confusing to explain these tuples than it is to show code:\n\nfor elt, items in groupby(x, itemgetter(1)):\n    print(elt, items)\n    for i in items:\n        print(i)\n\n\nWhich prints:\n\n21 <itertools._grouper object at 0x511a0>\n['4', '21', '1', '14', '2008-10-24 15:42:58']\n['5', '21', '3', '19', '2008-10-24 15:45:45']\n['6', '21', '1', '1somename', '2008-10-24 15:45:49']\n22 <itertools._grouper object at 0x51170>\n['3', '22', '4', '2somename', '2008-10-24 15:22:03']\n['7', '22', '3', '2somename', '2008-10-24 15:45:51']\n\n\nFor the second part, you should use list comprehensions as mentioned already here:\n\nfrom pprint import pprint as pp\npp([y for y in x if y[3] == '2somename'])\n\n\nWhich prints:\n\n[['3', '22', '4', '2somename', '2008-10-24 15:22:03'],\n ['7', '22', '3', '2somename', '2008-10-24 15:45:51']]\n\n"}
{"prompt": "Without having the full module path of a Django model, is it possible to do something like:\n\nmodel = 'User' [in Django namespace]\nmodel.objects.all()\n\n\n...as opposed to:\n\nUser.objects.all().\n\n\nEDIT: I am trying to make this call based on command-line input. Is it possible to avoid the import statement, e.g., \n\nmodel = django.authx.models.User\n\n\nWithout Django returning the error:\n\n\"global name django is not defined.\"\n\n", "completion": "I think you're looking for this:\n\nfrom django.db.models.loading import get_model\nmodel = get_model('app_name', 'model_name')\n\n\nThere are other methods, of course, but this is the way I'd handle it if you don't know what models file you need to import into your namespace.  (Note there's really no way to safely get a model without first knowing what app it belongs to.  Look at the source code to loading.py if you want to test your luck at iterating over all the apps' models.)\n\nUpdate: According to Django's deprecation timeline, django.db.models.loading has been deprecated in Django 1.7 and will be removed in Django 1.9. As pointed out in Alasdair's answer, a new API for dynamically loading models was added to Django 1.7.\n"}
{"prompt": "According to setuptools documentation, setup.py develop is supposed to create the egg-link file and update easy_install.pth when installing into site-packages folder.  However, in my case it's only creating the egg-link file.  How does setuptools decide if it needs to update easy_install.pth?\n\nSome more info:\nIt works when I have setuptools 0.6c7 installed as a folder under site-packages.  But when I use setuptools 0.6c9 installed as a zipped egg, it does not work.\n", "completion": "Reinstall setuptools with the command easy_install --always-unzip --upgrade setuptools. If that fixes it then the zipping was the problem.\n"}
{"prompt": "I'm storing some additional per-user information using the AUTH_PROFILE_MODULE.\n\nWe can access the user in a Django template using {{ request.user }} but how do we access fields in the profile since the profile is only accessible via a function user.get_profile() ?\n\nIs it really required to explicitly pass the profile into the template every time?\n", "completion": "Use {{ request.user.get_profile.whatever }}. Django's templating language automatically calls things that are callable - in this case, the .get_profile() method.\n"}
{"prompt": "How to return more than one variable from a function in Python?\n", "completion": "You separate the values you want to return by commas:\n\ndef get_name():\n   # you code\n   return first_name, last_name\n\n\nThe commas indicate it's a tuple, so you could wrap your values by parentheses:\n\nreturn (first_name, last_name)\n\n\nThen when you call the function you a) save all values to one variable as a tuple, or b) separate your variable names by commas\n\nname = get_name() # this is a tuple\nfirst_name, last_name = get_name()\n(first_name, last_name) = get_name() # You can put parentheses, but I find it ugly\n\n"}
{"prompt": "I am interested in getting an install of Django running on IronPython, has anyone had any success getting this running with some level of success?  \n\nIf so can you please tell of your experiences, performance, suggest some tips, resources and gotchas?\n", "completion": "Besides the Jeff Hardy blog post on Django + IronPython mentioned by Tony Meyer, it might be useful to also read Jeff's two other posts in the same series on his struggles with IronPython, easy_install and zlib. The first is Solving the zlib problem which discusses the absence of zlib for IronPython; hence, no easyinstall. Jeff reimplemented zlib based on ComponentAce's zlib.net. And finally, in easy_install on IronPython, Part Deux Jeff discusses some final tweaks that are needed before easy_install can be used with IronPython.\n"}
{"prompt": "I'm trying to unit test some code that looks like this:\n\ndef main():\n    parser = optparse.OptionParser(description='This tool is cool', prog='cool-tool')\n    parser.add_option('--foo', action='store', help='The foo option is self-explanatory')\n    options, arguments = parser.parse_args()\n    if not options.foo:\n        parser.error('--foo option is required')\n    print \"Your foo is %s.\" % options.foo\n    return 0\n\nif __name__ == '__main__':\n   sys.exit(main())\n\n\nWith code that looks like this:\n\n@patch('optparse.OptionParser')\ndef test_main_with_missing_p4clientsdir_option(self, mock_optionparser):\n    #\n    # setup\n    #\n    optionparser_mock = Mock()\n    mock_optionparser.return_value = optionparser_mock\n    options_stub = Mock()\n    options_stub.foo = None\n    optionparser_mock.parse_args.return_value = (options_stub, sentinel.arguments)\n    def parser_error_mock(message):\n        self.assertEquals(message, '--foo option is required')\n        sys.exit(2)\n    optionparser_mock.error = parser_error_mock\n\n    #\n    # exercise & verify\n    #\n    self.assertEquals(sut.main(), 2)\n\n\nI'm using Michael Foord's Mock, and nose to run the tests.\n\nWhen I run the test, I get:\n\n  File \"/Users/dspitzer/Programming/Python/test-optparse-error/tests/sut_tests.py\", line 27, in parser_error_mock\n    sys.exit(2)\nSystemExit: 2\n\n----------------------------------------------------------------------\nRan 1 test in 0.012s\n\nFAILED (errors=1)\n\n\nThe problem is that OptionParser.error does a sys.exit(2), and so main() naturally relies on that.  But nose or unittest detects the (expected) sys.exit(2) and fails the test.\n\nI can make the test pass by adding \"return 2\" under the parser.error() call in main() and removing the sys.exit() call from parser_error_mock(), but I find it distasteful to modify the code under test to allow a test to pass.  Is there a better solution?\n\nUpdate: df's answer works, although the correct call is \"self.assertRaises(SystemExit, sut.main)\".\n\nWhich means the test passes whatever the number is in the sys.exit() in parser_error_mock(). Is there any way to test for the exit code?\n\nBTW, the test is more robust if I add:\n\nself.assertEquals(optionparser_mock.method_calls, [('add_option', ('--foo',), {'action': 'store', 'help': 'The foo option is self-explanatory'}), ('parse_args', (), {})])\n\n\nat the end.\n\nUpdate 2: I can test for the exit code by replacing \"self.assertRaises(SystemExit, sut.main)\" with:\n\ntry:\n    sut.main()\nexcept SystemExit, e:\n    self.assertEquals(type(e), type(SystemExit()))\n    self.assertEquals(e.code, 2)\nexcept Exception, e:\n    self.fail('unexpected exception: %s' % e)\nelse:\n    self.fail('SystemExit exception expected')\n\n", "completion": "Will this work instead of assertEquals?\n\nself.assertRaises(SystemExit, sut.main, 2)\n\n\nThis should catch the SystemExit exception and prevent the script from terminating.\n"}
{"prompt": "I'm especially interested in solutions with source code available (django independency is a plus, but I'm willing to hack my way through)\n", "completion": "You can, of course, write your own handler. Other than that, your options currently are limited to:\n\n\ngae-rest, which provides a RESTful interface to the datastore.\napprocket, a tool for replicating between MySQL and App Engine.\nThe amusingly named GAEBAR - Google App Engine Backup and Restore.\n\n"}
{"prompt": "I want to be able to run WSGI apps but my current hosting restricts it. Does anybody know a company that can accommodate my requirements?\n", "completion": "My automatic response would be WebFaction.  \n\nI haven't personally hosted with them, but they are primarily Python-oriented (founded by the guy who wrote CherryPy, for example, and as far as I know they were the first to roll out Python 3.0 support).\n"}
{"prompt": "By default when Django runs against sqlite backend it creates a new in memory database for a test. That means for every class that derives from unittest.TestCase, I get a new database. Can this be changed so that it is cleared before every test method is run?\n\nExample: I am testing a manager class that provides additional abstraction on top of Django persistent objects. The code looks more-less like that\n\nclass TestForManager(unittest.TestCase):\n  def testAddingBlah(self):\n    manager = Manager()\n    self.assertEquals(manager.getBlahs(), 0)\n    manager.addBlah(...)\n    self.assertEquals(manager.getBlahs(), 1)\n\n  def testAddingBlahInDifferentWay(self):\n    manager = Manager()\n    self.assertEquals(manager.getBlahs(), 0)\n    manager.addBlahInDifferentWay(...)\n    self.assertEquals(manager.getBlahs(), 1)\n\n\nNow, the first assertion of second test fails, because the state of the database is preserved between test calls and there already is an instance of Blah in the database.\n", "completion": "As always, solution is trivial: use django.test.TestCase not unittest.TestCase. And it works in all major versions of Django!\n"}
{"prompt": "I know there is something buried in here.  But I was just wondering if there is an actual way built into Python to determine text file encoding?\n\nThanks for your help :)\n\nEdit:  As a side question, it can be ignored if you want but why is the type of encoding not put into the file so it could be detected easier?\n", "completion": "Correctly detecting the encoding all times is impossible.\n\n(From chardet FAQ:)\n\n\n  However, some encodings are optimized\n  for specific languages, and languages\n  are not random. Some character\n  sequences pop up all the time, while\n  other sequences make no sense. A\n  person fluent in English who opens a\n  newspaper and finds \u00e2\u0080\u009ctxzqJv 2!dasd0a\n  QqdKjvz\u00e2\u0080\u009d will instantly recognize that\n  that isn't English (even though it is\n  composed entirely of English letters).\n  By studying lots of \u00e2\u0080\u009ctypical\u00e2\u0080\u009d text, a\n  computer algorithm can simulate this\n  kind of fluency and make an educated\n  guess about a text's language.\n\n\nThere is the chardet library that uses that study to try to detect encoding. chardet is a port of the auto-detection code in Mozilla. \n\nYou can also use UnicodeDammit. It will try the following methods:\n\n\nAn encoding discovered in the document itself: for instance, in an XML declaration or (for HTML documents) an http-equiv META tag. If Beautiful Soup finds this kind of encoding within the document, it parses the document again from the beginning and gives the new encoding a try. The only exception is if you explicitly specified an encoding, and that encoding actually worked: then it will ignore any encoding it finds in the document.\nAn encoding sniffed by looking at the first few bytes of the file. If an encoding is detected at this stage, it will be one of the UTF-* encodings, EBCDIC, or ASCII.\nAn encoding sniffed by the chardet library, if you have it installed.\nUTF-8\nWindows-1252 \n\n"}
{"prompt": "There seem to be many excellent web frameworks for Python.  Has anyone used any of these (Pylons, Web2Py, Django) with IronPython?\n", "completion": "Django has been run on IronPython before, but as a proof-of-concept. I know the IronPython team are interested in Django support as a metric for Python-compatibility.\n\nSomewhat related is the possibility to use IronPython with ASP.NET and ASP.NET MVC, which is probably more mature.\n"}
{"prompt": "Is there any reason to prefer unicode(somestring, 'utf8') as opposed to somestring.decode('utf8')?\n\nMy only thought is that .decode() is a bound method so python may be able to resolve it more efficiently, but correct me if I'm wrong.\n", "completion": "It's easy to benchmark it:\n\n>>> from timeit import Timer\n>>> ts = Timer(\"s.decode('utf-8')\", \"s = '\u00c3\u00a9\u00c3\u00a9\u00c3\u00a9'\")\n>>> ts.timeit()\n8.9185450077056885\n>>> tu = Timer(\"unicode(s, 'utf-8')\", \"s = '\u00c3\u00a9\u00c3\u00a9\u00c3\u00a9'\") \n>>> tu.timeit()\n2.7656929492950439\n>>> \n\n\nObviously, unicode() is faster.\n\nFWIW, I don't know where you get the impression that methods would be faster - it's quite the contrary.\n"}
{"prompt": "Is it possible to use the with statement directly with CSV files? It seems natural to be able to do something like this:\n\nimport csv\nwith csv.reader(open(\"myfile.csv\")) as reader:\n    # do things with reader\n\n\nBut csv.reader doesn't provide the __enter__ and __exit__ methods, so this doesn't work. I can however do it in two steps:\n\nimport csv\nwith open(\"myfile.csv\") as f:\n    reader = csv.reader(f)\n    # do things with reader\n\n\nIs this second way the ideal way to do it? Why wouldn't they make csv.reader directly compatible with the with statement?\n", "completion": "The primary use of with statement is an exception-safe cleanup of an object used in the statement. with makes sure that files are closed, locks are released, contexts are restored, etc.\n\nDoes csv.reader have things to cleanup in case of exception?\n\nI'd go with:\n\nwith open(\"myfile.csv\") as f:\n    for row in csv.reader(f):\n        # process row\n\n\nYou don't need to submit the patch to use csv.reader and with statement together.\n\nimport contextlib\n\n\nHelp on function contextmanager in module contextlib:\n\ncontextmanager(func)\n    @contextmanager decorator.\n\n\nTypical usage:\n\n    @contextmanager\n    def some_generator(<arguments>):\n        <setup>\n        try:\n            yield <value>\n        finally:\n            <cleanup>\n\n\nThis makes this:\n\n    with some_generator(<arguments>) as <variable>:\n        <body>\n\n\nequivalent to this:\n\n    <setup>\n    try:\n        <variable> = <value>\n        <body>\n    finally:\n        <cleanup>\n\n\nHere's a concrete example how I've used it: curses_screen.\n"}
{"prompt": "I'm working on an attendance entry form for a band.  My idea is to have a section of the form to enter event information for a performance or rehearsal.  Here's the model for the event table:\n\nclass Event(models.Model):\n    event_id = models.AutoField(primary_key=True)\n    date = models.DateField()\n    event_type = models.ForeignKey(EventType)\n    description = models.TextField()\n\n\nThen I'd like to have an inline FormSet that links the band members to the event and records whether they were present, absent, or excused:\n\nclass Attendance(models.Model):\n    attendance_id = models.AutoField(primary_key=True)\n    event_id = models.ForeignKey(Event)\n    member_id = models.ForeignKey(Member)\n    attendance_type = models.ForeignKey(AttendanceType)\n    comment = models.TextField(blank=True)\n\n\nNow, what I'd like to do is to pre-populate this inline FormSet with entries for all the current members and default them to being present (around 60 members).  Unfortunately, Django doesn't allow initial values in this case.\n\nAny suggestions?\n", "completion": "So, you're not going to like the answer, partly because I'm not yet done writing the code and partly because it's a lot of work.\n\nWhat you need to do, as I discovered when I ran into this myself, is:\n\n\nSpend a lot of time reading through the formset and model-formset code to get a feel for how it all works (not helped by the fact that some of the functionality lives on the formset classes, and some of it lives in factory functions which spit them out). You will need this knowledge in the later steps.\nWrite your own formset class which subclasses from BaseInlineFormSet and accepts initial. The really tricky bit here is that you must override __init__(), and you must make sure that it calls up to BaseFormSet.__init__() rather than using the direct parent or grandparent __init__() (since those are BaseInlineFormSet and BaseModelFormSet, respectively, and neither of them can handle initial data).\nWrite your own subclass of the appropriate admin inline class (in my case it was TabularInline) and override its get_formset method to return the result of inlineformset_factory() using your custom formset class.\nOn the actual ModelAdmin subclass for the model with the inline, override add_view and change_view, and replicate most of the code, but with one big change: build the initial data your formset will need, and pass it to your custom formset (which will be returned by your ModelAdmin's get_formsets() method).\n\n\nI've had a few productive chats with Brian and Joseph about improving this for future Django releases; at the moment, the way the model formsets work just make this more trouble than it's usually worth, but with a bit of API cleanup I think it could be made extremely easy.\n"}
{"prompt": "In C, we can find the size of an int, char, etc. I want to know how to get size of objects like a string, integer, etc. in Python.\n\nRelated question: How many bytes per element are there in a Python list (tuple)?\n\nI am using an XML file which contains size fields that specify the size of value. I must parse this XML and do my coding. When I want to change the value of a particular field, I will check the size field of that value. Here I want to compare whether the new value that I'm gong to enter is of the same size as in XML. I need to check the size of new value. In case of a string I can say its the length. But in case of int, float, etc. I am confused.\n", "completion": "Just use the sys.getsizeof function defined in the sys module.\n\n\n  sys.getsizeof(object[, default]):\n  \n  Return the size of an object in bytes.\n  The object can be any type of object.\n  All built-in objects will return\n  correct results, but this does not\n  have to hold true for third-party\n  extensions as it is implementation\n  specific.\n  \n  The default argument allows to define\n  a value which will be returned if the\n  object type does not provide means to\n  retrieve the size and would cause a\n  TypeError.\n  \n  getsizeof calls the object\u00e2\u0080\u0099s\n  __sizeof__ method and adds an additional garbage collector overhead\n  if the object is managed by the\n  garbage collector.\n\n\nUsage example, in python 3.0:\n\n>>> import sys\n>>> x = 2\n>>> sys.getsizeof(x)\n14\n>>> sys.getsizeof(sys.getsizeof)\n32\n>>> sys.getsizeof('this')\n38\n>>> sys.getsizeof('this also')\n48\n\n\nIf you are in python < 2.6 and don't have sys.getsizeof you can use this extensive module instead. Never used it though.\n"}
{"prompt": "I was looking at the builtin object methods in the Python documentation, and I was interested in the documentation for object.__repr__(self). Here's what it says:\n\n\n  Called by the repr() built-in function\n  and by string conversions (reverse\n  quotes) to compute the \u00e2\u0080\u009cofficial\u00e2\u0080\u009d\n  string representation of an object. If\n  at all possible, this should look like\n  a valid Python expression that could\n  be used to recreate an object with the\n  same value (given an appropriate\n  environment). If this is not possible,\n  a string of the form <...some useful\n  description...> should be returned.\n  The return value must be a string\n  object. If a class defines repr()\n  but not str(), then repr() is\n  also used when an \u00e2\u0080\u009cinformal\u00e2\u0080\u009d string\n  representation of instances of that\n  class is required.\n  \n  This is typically used for debugging,\n  so it is important that the\n  representation is information-rich and\n  unambiguous\n\n\nThe most interesting part to me, was...\n\n\n  If at all possible, this should look like a valid Python expression that could be used to recreate an object with the same value\n\n\n... but I'm not sure exactly what this means. It says it should look like an expression which can be used to recreate the object, but does that mean it should just be an example of the sort of expression you could use, or should it be an actual expression, that can be executed (eval etc..) to recreate the object? Or... should it be just a rehasing of the actual expression which was used, for pure information purposes?\n\nIn general I'm a bit confused as to exactly what I should be putting here.\n", "completion": ">>> from datetime import date\n>>>\n>>> repr(date.today())        # calls date.today().__repr__()\n'datetime.date(2009, 1, 16)'\n>>> eval(_)                   # _ is the output of the last command\ndatetime.date(2009, 1, 16)\n\n\nThe output is a string that can be parsed by the python interpreter and results in an equal object.\n\nIf that's not possible, it should return a string in the form of <...some useful description...>.\n"}
{"prompt": "I want to create a list of lambda objects from a list of constants in Python; for instance:\n\nlistOfNumbers = [1,2,3,4,5]\nsquare = lambda x: x * x\nlistOfLambdas = [lambda: square(i) for i in listOfNumbers]\n\n\nThis will create a list of lambda objects, however, when I run them:\n\nfor f in listOfLambdas:\n    print f(),\n\n\nI would expect that it would print\n\n1 4 9 16 25\n\n\nInstead, it prints:\n\n25 25 25 25 25\n\n\nIt seems as though the lambdas have all been given the wrong parameter. Have I done something wrong, and is there a way to fix it? I'm in Python 2.4 I think.\n\nEDIT: a bit more of trying things and such came up with this:\n\nlistOfLambdas = []\nfor num in listOfNumbers:\n    action = lambda: square(num)\n    listOfLambdas.append(action)\n    print action()\n\n\nPrints the expected squares from 1 to 25, but then using the earlier print statement:\n\nfor f in listOfLambdas:\n    print f(),\n\n\nstill gives me all 25s. How did the existing lambda objects change between those two print calls?\n\nRelated question: Why results of map() and list comprehension are different?\n", "completion": "You have:\n\nlistOfLambdas = [lambda: i*i for i in range(6)]\n\nfor f in listOfLambdas:\n    print f()\n\n\nOutput:\n\n25\n25\n25\n25\n25\n25\n\n\nYou need currying!  Aside from being delicious, use this default value \"hack\".\n\nlistOfLambdas = [lambda i=i: i*i for i in range(6)]\n\nfor f in listOfLambdas:\n    print f()\n\n\nOutput:\n\n0\n1\n4\n9\n16\n25\n\n\nNote the i=i.  That's where the magic happens.  \n"}
{"prompt": "I want to send a datetime.datetime object in serialized form from Python using JSON and de-serialize in JavaScript using JSON. What is the best way to do this?\n", "completion": "You can add the 'default' parameter to json.dumps to handle this:\n\ndate_handler = lambda obj: (\n    obj.isoformat()\n    if isinstance(obj, datetime.datetime)\n    or isinstance(obj, datetime.date)\n    else None\n)\njson.dumps(datetime.datetime.now(), default=date_handler)\n'\"2010-04-20T20:08:21.634121\"'\n\n\nWhich is ISO 8601 format. \n\nA more comprehensive default handler function:\n\ndef handler(obj):\n    if hasattr(obj, 'isoformat'):\n        return obj.isoformat()\n    elif isinstance(obj, ...):\n        return ...\n    else:\n        raise TypeError, 'Object of type %s with value of %s is not JSON serializable' % (type(obj), repr(obj))\n\n\nUpdate:  Added output of type as well as value.\nUpdate:  Also handle date \n"}
{"prompt": "I'm using Python and Qt 4.4 and I have to print some pages. Initially I thought I'd use HTML with CSS to produce those pages. But HTML has some limitations.\n\nNow the question is: is there anything that's better than HTML but just (or almost) as easy to use? Additionally, it should be GPL-compatible.\n\nEdit:\n\nkdgregory & Mark G: The most obvious limitation is that I can't specify the printer margins. There is another problem: How do I add page numbers?\n\nJeremy French: One thing I have to print is a list of all the products someone ordered which can spread over a few pages.\n", "completion": "I have been fighting with printed (or PDF) output from Python for 8 years now and so far I came across the following approaches (in order of personal preference):\n\n\nUsing JasperReports via pyJasper (written by me) or JasperServer. You can use the WYSIWYG design tool iReport to define your layout. Your Python code will contact the Java based Jasper engine via HTTP and make it render a PDF (pyJasper handles that). We use that for a few thousand pages a day.\nUse plain text output. You can't get any faster. We use that for a few hundred pages per day.\nUse XSLT-FO. You also have to call a Java based rendering engine like FOB. Might result in performance issues but can be mitigated by having a long running Java server process  - same approach than with Jasper. We use that for a few hundred pages per day but writing XSLT-FO documents made my head hurt. Not used for new code.\nGenerate LaTeX source and use a latex software package to render to PDF. Getting LaTeX to look like you like is quite difficult. But as long as you go with the provided LaTeX styles, you are fine. Not used in production at my shop.\nPDF generation with the ReportLab Toolkit. Somewhat low level. Even more low level: FPDF. We use FPDF-Ruby for a few hundred pages a day. Took a lot of fiddeling to get the layout we wanted.\nDirectly generate Postscript. Strange but you nearly can't get more in terms of speed and control. We used that to generate contact sheets with a few hundred thousand Jpegs per day. Takes fiddling but is fun.\nuse troff/groff to generate Postscript/PDF. Very low level bute nice to do simple, high volume things. Never used it thus in production.\n\n\nFor orders, invoices and the like I highly recommend JasperReports. The ability to use a visual editor to define the layout is a huge time saver.\n"}
{"prompt": "Is there a standard way to associate version string with a python package in such way that I could do the following?\n\nimport foo\nprint foo.version\n\n\nI would imagine there's some way to retrieve that data without any extra hardcoding, since minor/major strings are specified in setup.py already. Alternative solution that I found was to have import __version__ in my foo/__init__.py and then have __version__.py generated by setup.py.\n", "completion": "Here is how I do this. Advantages of the following method:\n\n\nIt provides a __version__ attribute.\nIt provides the standard metadata version. Therefore it will be detected by pkg_resources or other tools that parse the package metadata (EGG-INFO and/or PKG-INFO, PEP 0345).\nIt doesn't import your package (or anything else) when building your package, which can cause problems in some situations. (See the comments below about what problems this can cause.)\nThere is only one place that the version number is written down, so there is only one place to change it when the version number changes, and there is less chance of inconsistent versions.\n\n\nHere is how it works: the \"one canonical place\" to store the version number is a .py file, named \"_version.py\" which is in your Python package, for example in myniftyapp/_version.py. This file is a Python module, but your setup.py doesn't import it! (That would defeat feature 3.) Instead your setup.py knows that the contents of this file is very simple, something like:\n\n__version__ = \"3.6.5\"\n\n\nAnd so your setup.py opens the file and parses it, with code like:\n\nimport re\nVERSIONFILE=\"myniftyapp/_version.py\"\nverstrline = open(VERSIONFILE, \"rt\").read()\nVSRE = r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\"\nmo = re.search(VSRE, verstrline, re.M)\nif mo:\n    verstr = mo.group(1)\nelse:\n    raise RuntimeError(\"Unable to find version string in %s.\" % (VERSIONFILE,))\n\n\nThen your setup.py passes that string as the value of the \"version\" argument to setup(), thus satisfying feature 2.\n\nTo satisfy feature 1, you can have your package (at run-time, not at setup time!) import the _version file from myniftyapp/__init__.py like this:\n\nfrom _version import __version__\n\n\nHere is an example of this technique that I've been using for years.\n\nThe code in that example is a bit more complicated, but the simplified example that I wrote into this comment should be a complete implementation.\n\nHere is example code of importing the version.\n\nIf you see anything wrong with this approach, please let me know: zooko at zooko dot com. If you don't see anything wrong with this approach then use it! Because the more packages come with their version numbers in the expected places the better!\n"}
{"prompt": "I read in the manual of Numpy that there is function det(M) that can calculate the determinant. However, I can't find the det() method in Numpy.\n\nBy the way, I use Python 2.5. There should be no compatibility problems with Numpy.\n", "completion": "You can use numpy.linalg.det to compute the determinant of an array:\n\nIn [1]: import numpy\n\nIn [2]: M = [[1, 2], [3, 4]]\n\nIn [3]: print numpy.linalg.det(M)\nOut[3]: -2.0000000000000004\n\n"}
{"prompt": "In PHP you can just use $_POST for POST and $_GET for GET (Query string) variables. What's the equivalent in Python?\n", "completion": "suppose you're posting a html form with this:\n\n<input type=\"text\" name=\"username\">\n\n\nIf using raw cgi:\n\nimport cgi\nform = cgi.FieldStorage()\nprint form[\"username\"]\n\n\nIf using Django, Pylons, Flask or Pyramid: \n\nprint request.GET['username'] # for GET form method\nprint request.POST['username'] # for POST form method\n\n\nUsing Turbogears, Cherrypy:\n\nfrom cherrypy import request\nprint request.params['username']\n\n\nWeb.py:\n\nform = web.input()\nprint form.username\n\n\nWerkzeug:\n\nprint request.form['username']\n\n\nIf using Cherrypy or Turbogears, you can also define your handler function taking a parameter directly:\n\ndef index(self, username):\n    print username\n\n\nGoogle App Engine:\n\nclass SomeHandler(webapp2.RequestHandler):\n    def post(self):\n        name = self.request.get('username') # this will get the value from the field named username\n        self.response.write(name) # this will write on the document\n\n\nSo you really will have to choose one of those frameworks.\n"}
{"prompt": "In Python compiled regex patterns have a findall method that does the following:\n\n\n  Return all non-overlapping matches of\n  pattern in string, as a list of\n  strings. The string is scanned\n  left-to-right, and matches are\n  returned in the order found. If one or\n  more groups are present in the\n  pattern, return a list of groups; this\n  will be a list of tuples if the\n  pattern has more than one group. Empty\n  matches are included in the result\n  unless they touch the beginning of\n  another match.\n\n\nWhat's the canonical way of doing this in Perl? A naive algorithm I can think of is along the lines of \"while a search and replace with the empty string is successful, do [suite]\". I'm hoping there's a nicer way. :-)\n\nThanks in advance!\n", "completion": "Use the /g modifier in your match. From the perlop manual:\n\n\n  The \"/g\" modifier specifies global pattern matching--that is, matching as many times as possible within the string.  How it behaves depends on the context.  In list context, it returns a list of the substrings matched by any capturing parentheses in the regular expression.  If there are no parentheses, it returns a list of all the matched strings, as if there were parentheses around the whole pattern.\n  \n  In scalar context, each execution of \"m//g\" finds the next match, returning true if it matches, and false if there is no further match.  The position after the last match can be read or set using the pos() function; see \"pos\" in perlfunc.   A failed match normally resets the search position to the beginning of the string, but you can avoid that by adding the \"/c\" modifier (e.g. \"m//gc\").  Modifying the target string also resets the search position.\n\n"}
{"prompt": "I'm speaking of this module:\nhttp://docs.python.org/library/operator.html\n\nFrom the article:\n\n\n  The operator module exports a set of\n  functions implemented in C\n  corresponding to the intrinsic\n  operators of Python. For example,\n  operator.add(x, y) is equivalent to\n  the expression x+y. The function names\n  are those used for special class\n  methods; variants without leading and\n  trailing __ are also provided for\n  convenience.\n\n\nI'm not sure I understand the benefit or purpose of this module.\n", "completion": "Possibly the most popular usage is operator.itemgetter.  Given a list lst of tuples, you can sort by the ith element by: lst.sort(key=operator.itemgetter(i))\n\nCertainly, you could do the same thing without operator by defining your own key function, but the operator module makes it slightly neater.\n\nAs to the rest, python allows a functional style of programming, and so it can come up -- for instance, Greg's reduce example.\n\nYou might argue: \"Why do I need operator.add when I can just do: add = lambda x, y: x+y?\" The answers are:\n\n\noperator.add is (I think) slightly faster.\nIt makes the code easier to understand for you, or another person later, looking at it.  They don't need to look for the definition of add, because they know what the operator module does.\n\n"}
{"prompt": "I'm new to the MDX/OLAP and I'm wondering if there is any ORM similar like Django ORM for Python that would support OLAP.\n\nI'm a Python/Django developer and if there would be something that would have some level of integration with Django I would be much interested in learning more about it.\n", "completion": "Django has some OLAP features that are nearing release.\n\nRead http://www.eflorenzano.com/blog/post/secrets-django-orm/\n\nhttp://doughellmann.com/2007/12/30/using-raw-sql-in-django.html, also\n\nIf you have a proper star schema design in the first place, then one-dimensional results can have the following form.\n\nfrom myapp.models import SomeFact\nfrom collections import defaultdict\n\nfacts = SomeFact.objects.filter( dimension1__attribute=this, dimension2__attribute=that )\nmyAggregates = defaultdict( int )\nfor row in facts:\n    myAggregates[row.dimension3__attribute] += row.someMeasure\n\n\nIf you want to create a two-dimensional summary, you have to do something like the following.\n\nfacts = SomeFact.objects.filter( dimension1__attribute=this, dimension2__attribute=that )\nmyAggregates = defaultdict( int )\nfor row in facts:\n    key = ( row.dimension3__attribute, row.dimension4__attribute )\n    myAggregates[key] += row.someMeasure\n\n\nTo compute multiple SUM's and COUNT's and what-not, you have to do something like this.\n\nclass MyAgg( object ):\n    def __init__( self ):\n        self.count = 0\n        self.thisSum= 0\n        self.thatSum= 0\n\nmyAggregates= defaultdict( MyAgg )\nfor row in facts:\n    myAggregates[row.dimension3__attr].count += 1\n    myAggregates[row.dimension3__attr].thisSum += row.this\n    myAggregates[row.dimension3__attr].thatSum += row.that\n\n\nThis -- at first blush -- seems inefficient.  You're trolling through the fact table returning lots of rows which you are then aggregating in your application.\n\nIn some cases, this may be faster than the RDBMS's native sum/group_by.  Why?  You're using a simple mapping, not the more complex sort-based grouping operation that the RDBMS often has to use for this.  Yes, you're getting a lot of rows; but you're doing less to get them.\n\nThis has the disadvantage that it's not so declarative as we'd like.  It has the advantage that it's pure Django ORM.\n"}
{"prompt": "I've just started using the PythonInterpreter from within my Java classes, and it works great! However, if I try to include python modules (re, HTMLParser, etc.), I'm receiving the following exception (for re):\n\n\nException in thread \"main\" Traceback (innermost last):\n  File \"\", line 1, in ?\nImportError: no module named re\n\n\nHow could I make the classes from the jython jar \"see\" the modules python has available?\n", "completion": "You embed jython and you will use some Python-Modules somewere:\n\nif you want to set the path (sys.path) in your Java-Code :\n\npublic void init() {\n        interp = new PythonInterpreter(null, new PySystemState());\n\n        PySystemState sys = Py.getSystemState();\n        sys.path.append(new PyString(rootPath));\n        sys.path.append(new PyString(modulesDir));\n    }\n\n\nPy is in org.python.core.\n\nrootPath and modulesDir is where YOU want !\n\nlet rootPath point where you located the standard-jython-lib\n\nHave a look at src/org/python/util/PyServlet.java in the Jython-Source-Code for example\n"}
{"prompt": "I want to modify a few tiny details of Django's built-in django.contrib.auth module.  Specifically, I want a different form that makes username an email field (and email an alternate email address.   (I'd rather not modify auth any more than necessary -- a simple form change seems to be all that's needed.)\n\nWhen I use autodiscover with a customized ModelAdmin for auth I wind up conflicting with auth's own admin interface and get an \"already registered\" error.\n\nIt looks like I have to create my own admin site, enumerating all of my Models.  It's only 18 classes, but it seems like a DRY problem -- every change requires both adding to the Model and adding to the customized admin site.\n\nOr, should I write my own version of \"autodiscover with exclusions\" to essentially import all the admin modules except auth?\n", "completion": "None of the above.  Just use admin.site.unregister().  Here's how I recently added filtering Users on is_active in the admin (n.b. is_active filtering is now on the User model by default in Django core; still works here as an example), all DRY as can be:\n\nfrom django.contrib import admin\nfrom django.contrib.auth.admin import UserAdmin\nfrom django.contrib.auth.models import User\n\nclass MyUserAdmin(UserAdmin):\n    list_filter = UserAdmin.list_filter + ('is_active',)\n\nadmin.site.unregister(User)\nadmin.site.register(User, MyUserAdmin)\n\n"}
{"prompt": "What is the purpose of __slots__ in Python \u00e2\u0080\u0094 especially with respect to when would I want to use it and when not?\n", "completion": "Quoting Jacob Hallen:\n\n\n  The proper use of __slots__ is to save space in objects. Instead of having\n  a dynamic dict that allows adding attributes to objects at anytime,\n  there is a static structure which does not allow additions after creation.\n  This saves the overhead of one dict for every object that uses slots. While this is sometimes a useful optimization, it would be completely\n  unnecessary if the Python interpreter was dynamic enough so that it would\n  only require the dict when there actually were additions to the object.\n  \n  Unfortunately there is a side effect to slots. They change the behavior of\n  the objects that have slots in a way that can be abused by control freaks\n  and static typing weenies. This is bad, because the control freaks should\n  be abusing the metaclasses and the static typing weenies should be abusing\n  decorators, since in Python, there should be only one obvious way of doing something.\n  \n  Making CPython smart enough to handle saving space without __slots__ is a major\n  undertaking, which is probably why it is not on the list of changes for P3k (yet).\n\n"}
{"prompt": "Searching on Google reveals x2 code snippets. The first result is to this code recipe which has a lot of documentation and explanation, along with some useful discussion underneath. \n\nHowever, another code sample, whilst not containing so much documentation, includes sample code for passing commands such as start, stop and restart. It also creates a PID file which can be handy for checking if the daemon is already running etc.\n\nThese samples both explain how to create the daemon. Are there any additional things that need to be considered? Is one sample better than the other, and why?\n", "completion": "There are many fiddly things to take care of when becoming a well-behaved daemon process:\n\n\nprevent core dumps (many daemons run as root, and core dumps can contain sensitive information)\nbehave correctly inside a chroot gaol\nset UID, GID, working directory, umask, and other process parameters appropriately for the use case\nrelinquish elevated suid, sgid privileges\nclose all open file descriptors, with exclusions depending on the use case\nbehave correctly if started inside an already-detached context, such as init, inetd, etc.\nset up signal handlers for sensible daemon behaviour, but also with specific handlers determined by the use case\nredirect the standard streams stdin, stdout, stderr since a daemon process no longer has a controlling terminal\nhandle a PID file as a cooperative advisory lock, which is a whole can of worms in itself with many contradictory but valid ways to behave\nallow proper cleanup when the process is terminated\nactually become a daemon process without leading to zombies\n\n\nSome of these are standard, as described in canonical Unix literature (Advanced Programming in the UNIX Environment, by the late W. Richard Stevens, Addison-Wesley, 1992). Others, such as stream redirection and PID file handling, are conventional behaviour most daemon users would expect but that are less standardised.\n\nAll of these are covered by the PEP 3143 \u00e2\u0080\u009cStandard daemon process library\u00e2\u0080\u009d specification. The python-daemon reference implementation works on Python 2.7 or later, and Python 3.2 or later.\n"}
{"prompt": "http://docs.djangoproject.com/en/dev/ref/templates/builtins/#regroup\n\nI can think of a few ways of doing it with loops but I'd particularly like to know if there is a neat one-liner.\n", "completion": "Combine itertools.groupby with operator.itemgetter to get a pretty nice solution:\n\nfrom operator import itemgetter\nfrom itertools import groupby\n\nkey = itemgetter('gender')\niter = groupby(sorted(people, key=key), key=key)\n\nfor gender, people in iter:\n    print '===', gender, '==='\n    for person in people:\n        print person\n\n"}
{"prompt": "I would like to know how to convert a string containing digits to a double.\n", "completion": ">>> x = \"2342.34\"\n>>> float(x)\n2342.3400000000001\n\n\nThere you go.  Use float (which is almost always a C double).\n"}
{"prompt": "Does the Python shelve module have any protection built in to make sure two processes aren't writing to a file at the same time?\n", "completion": "The shelve module uses an underlying database package (such as dbm, gdbm or bsddb) .\n\nThe restrictions pragraph says (my emphasis):\n\n\n  The shelve module does not support concurrent read/write access to shelved objects. (Multiple simultaneous read accesses are safe.) When a program has a shelf open for writing, no other program should have it open for reading or writing. Unix file locking can be used to solve this, but this differs across Unix versions and requires knowledge about the database implementation used.\n\n\nConclusion: it depends on OS and the underlying DB. To keep things portable, do not build on concurrency.\n"}
{"prompt": "I have a range of dates and a measurement on each of those dates.  I'd like to calculate an exponential moving average for each of the dates.  Does anybody know how to do this?\n\nI'm new to python.  It doesn't appear that averages are built into the standard python library, which strikes me as a little odd.  Maybe I'm not looking in the right place.\n\nSo, given the following code, how could I calculate the moving weighted average of IQ points for calendar dates?\n\nfrom datetime import date\ndays = [date(2008,1,1), date(2008,1,2), date(2008,1,7)]\nIQ = [110, 105, 90]\n\n\n(there's probably a better way to structure the data, any advice would be appreciated)\n", "completion": "EDIT:\nIt seems that mov_average_expw() function from scikits.timeseries.lib.moving_funcs submodule from SciKits (add-on toolkits that complement SciPy) better suits the wording of your question.  \n\n\n\nTo calculate an exponential smoothing of your data with a smoothing factor alpha (it is (1 - alpha) in Wikipedia's terms):\n\n>>> alpha = 0.5\n>>> assert 0 < alpha <= 1.0\n>>> av = sum(alpha**n.days * iq \n...     for n, iq in map(lambda (day, iq), today=max(days): (today-day, iq), \n...         sorted(zip(days, IQ), key=lambda p: p[0], reverse=True)))\n95.0\n\n\nThe above is not pretty, so let's refactor it a bit:\n\nfrom collections import namedtuple\nfrom operator    import itemgetter\n\ndef smooth(iq_data, alpha=1, today=None):\n    \"\"\"Perform exponential smoothing with factor `alpha`.\n\n    Time period is a day.\n    Each time period the value of `iq` drops `alpha` times.\n    The most recent data is the most valuable one.\n    \"\"\"\n    assert 0 < alpha <= 1\n\n    if alpha == 1: # no smoothing\n        return sum(map(itemgetter(1), iq_data))\n\n    if today is None:\n        today = max(map(itemgetter(0), iq_data))\n\n    return sum(alpha**((today - date).days) * iq for date, iq in iq_data)\n\nIQData = namedtuple(\"IQData\", \"date iq\")\n\nif __name__ == \"__main__\":\n    from datetime import date\n\n    days = [date(2008,1,1), date(2008,1,2), date(2008,1,7)]\n    IQ = [110, 105, 90]\n    iqdata = list(map(IQData, days, IQ))\n    print(\"\\n\".join(map(str, iqdata)))\n\n    print(smooth(iqdata, alpha=0.5))\n\n\nExample:\n\n$ python26 smooth.py\nIQData(date=datetime.date(2008, 1, 1), iq=110)\nIQData(date=datetime.date(2008, 1, 2), iq=105)\nIQData(date=datetime.date(2008, 1, 7), iq=90)\n95.0\n\n"}
{"prompt": "While I like to think of myself as a reasonably competent Python coder, one aspect of the language I've never been able to grok is decorators.\n\nI know what they are (superficially), I've read tutorials, examples, questions on Stack Overflow, and I understand the syntax, can write my own, occasionally use @classmethod and @staticmethod, but it never occurs to me to use a decorator to solve a problem in my own Python code.  I never encounter a problem where I think, \"Hmm...this looks like a job for a decorator!\"\n\nSo, I'm wondering if you guys might offer some examples of where you've used decorators in your own programs, and hopefully I'll have an \"A-ha!\" moment and get them.\n", "completion": "I use decorators mainly for timing purposes\n\ndef time_dec(func):\n\n  def wrapper(*arg):\n      t = time.clock()\n      res = func(*arg)\n      print func.func_name, time.clock()-t\n      return res\n\n  return wrapper\n\n\n@time_dec\ndef myFunction(n):\n    ...\n\n"}
{"prompt": "i have:\n\nfor i in range(2,n):\n    if(something):\n       do something\n    else:\n       do something else\n       i = 2 **restart the loop\n\n\nBut that doesn't seem to work. Is there a way to restart that loop? \n\nThanks\n", "completion": "You may want to consider using a different type of loop where that logic is applicable, because it is the most obvious answer.\n\nperhaps a:\n\ni=2\nwhile i < n:\n    if something:\n       do something\n       i += 1\n    else: \n       do something else  \n       i = 2 #restart the loop\n\n"}
{"prompt": "Is Django a good choice for a security critical application?\n\nI am asking this because most of the online banking software is built using Java. Is there any real reason for this?\n", "completion": "Actually, the security in Java and Python is the same.  Digest-only password handling, cookies that timeout rapidly, careful deletion of sessions, multi-factor authentication.  None of this is unique to a Java framework or a Python framework like Django.\n\nDjango, indeed, has a security backend architecture that allows you to add your own LDAP (or AD) connection, possibly changing the digest technique used.  \n\nDjango has a Profile model where you can keep additional authentication factors.\n\nDjango offers a few standard decorators for view function authorization checking.  Since Python is so flexible, you can trivially write your own decorator functions to layer in different or additional authentication checking.\n\nSecurity is a number of first-class features in Django.\n"}
{"prompt": "First of all,I'm not into web programming. I bumped into django and read a bit about models. I was intrigued by the following code ( from djangoproject.com ) :\n\n\nclass Person(models.Model):\n    first_name = models.CharField(max_length=50)\n    last_name = models.CharField(max_length=50)\n\n    def __str__(self):\n        # Note use of django.utils.encoding.smart_str() here because\n        # first_name and last_name will be unicode strings.\n        return smart_str('%s %s' % (self.first_name, self.last_name))\n\n\nBy my understanding of python , first_name and last_name are class variables , right ? How is that used in code ( because I guess that setting Person.first_name or Person.last_name will affect all Person instances ) ? Why is it used that way ?\n", "completion": "The essence of your question is \"how come these class variables (which I assign Field objects to) suddenly become instance variables (which I assign data to) in Django's ORM\"?  The answer to that is the magic of Python metaclasses.\n\nA metaclass allows you to hook into and modify the process of creating a Python class (not the creation of an instance of that class, the creation of the class itself).\n\nDjango's Model object (and thus also your models, which are subclasses) has a ModelBase metaclass.  It looks through all the class attributes of your model, and any that are instances of a Field subclass it moves into a fields list.  That list is assigned as an attribute of the _meta object, which is a class attribute of the model.  Thus you can always get to the actual Field objects via MyModel._meta.fields, or MyModel._meta.get_field('field_name').\n\nThe Model.__init__ method is then able to use the _meta.fields list to determine what instance attributes should be initialized when a model instance is created.\n\nDon't be afraid to dive into the Django source code; it's a great source of education!\n"}
{"prompt": "I would like to know similar, concrete simulations, as the simulation about watering a field here.\n\nWhat is your favorite library/internet page for such simulations in Python?\n\nI know little Simpy, Numpy and Pygame. I would like to get examples about them.\n", "completion": "If you are looking for some game physics (collisions, deformations, gravity, etc.) which looks real and is reasonably fast consider re-using some physics engine libraries.\n\nAs a first reference, you may want to look into pymunk, a Python wrapper of Chipmunk 2D physics library. You can find a list of various Open Source physics engines (2D and 3D) in Wikipedia.\n\nIf you are looking for physically correct simulations, no matter what language you want to use, it will be much slower (almost never real-time), and you need to use some numerical analysis software (and probably to write something yourself). Exact answer depends on the problem you want to solve. It is a fairly complicated field (of math).\n\nFor example, if you need to do simulations in continuum mechanics or electromagnetism, you probably need Finite Difference, Finite Volume or Finite Element methods. For Python, there are some ready-to-use libraries, for example: FiPy (FVM), GetFem++ (FEM), FEniCS/DOLFIN (FEM), and some other.\n"}
{"prompt": "I am looking for a Python3.0 version of \"py2exe\". I tried running 2to3 on the source for py2exe but the code remained broken.\n\nAny ideas?\n", "completion": "Update 2014-05-15\n\npy2exe for Python 3.x is now released! Get it on PyPI.\n\nOld information\n\nHave a look at the py2exe SourceForge project SVN repository at:\n\nhttp://py2exe.svn.sourceforge.net/\n\nThe last I looked at it, it said the last update was August 2009. But keep an eye on that to see if there's any Python 3 work in-progress.\n\nI've also submitted two feature requests on the py2exe tracker. So far, no feedback on them:\n\n\nSupport Python 3.x\nProject roadmap\n\n"}
{"prompt": "Does anyone know where information about writing gedit plugins can be found ? I'm interested in writing them in Python. I know of Gedit/PythonPluginHowTo\n , but it isn't very good . Besides the code of writing a plugin that does nothing , I can't seem to find more information . I started to look at other people's code , but I think this shouldn't be the natural way of writing plugins . Can someone help ?\n", "completion": "When I started working on my gedit plugin, I used the howto you gave a link to, also startign with this URL. Then it was looking at other plugins code... I'm sorry to say that, but for me this topic is poorly documented and best and fastest way is to get a pluging done that actually does something.\n"}
{"prompt": "I found the following behavior at least weird:\n\ndef errors():\n    try:\n        ErrorErrorError\n    finally:\n        return 10\n\nprint errors()\n# prints: 10\n# It should raise: NameError: name 'ErrorErrorError' is not defined\n\n\nThe exception disappears when you use return inside a finally clause. Is that a bug? Is that documented anywhere?\n\nBut the real question (and the answer I will mark as correct) is:\nWhat is the python developers' reason to allow that odd behavior?\n", "completion": "\n  The exception disappears when you use return inside a finally clause. .. Is that documented anywhere?\n\n\nIt is:\n\n\n  If finally is present, it specifies a \u00e2\u0080\u0098cleanup\u00e2\u0080\u0099 handler. The try clause is executed, including any except and else clauses. If an exception occurs in any of the clauses and is not handled, the exception is temporarily saved. The finally clause is executed. If there is a saved exception, it is re-raised at the end of the finally clause. If the finally clause raises another exception or executes a return or break statement, the saved exception is lost.\n\n"}
{"prompt": "Like most Python developers, I typically keep a console window open with the Python interpreter running to test commands, dir() stuff, help() stuff, etc.\n\nLike any console, after a while the visible backlog of past commands and prints gets to be cluttered, and sometimes confusing when re-running the same command several times.  I'm wondering if, and how, to clear the Python interpreter console.\n\nI've heard about doing a system call and either calling cls on Windows or clear on Linux, but I was hoping there was something I could command the interpreter itself to do.\n\nNote: I'm running on Windows, so Ctrl+L doesn't work.\n", "completion": "As you mentioned, you can do a system call:\n\n>>> import os\n>>> clear = lambda: os.system('cls')\n>>> clear()\n\n\nI am not sure of any other way in Windows.\n"}
{"prompt": "I have downloaded the Pyscripter and learning Python. But I have no Idea if it has any job value , especially in India. I am learning Python as a Hobby. But it would be comforting to know if Python programmers are in demand in India.\n", "completion": "Everywhere. It's used extensively by google for one.\n\nSee list of python software for more info, and also who uses python on the web?\n"}
{"prompt": "Today I was thinking about a Python project I wrote about a year back where I used logging pretty extensively. I remember having to comment out a lot of logging calls in inner-loop-like scenarios (the 90% code) because of the overhead (hotshot indicated it was one of my biggest bottlenecks).\n\nI wonder now if there's some canonical way to programmatically strip out logging calls in Python applications without commenting and uncommenting all the time. I'd think you could use inspection/recompilation or bytecode manipulation to do something like this and target only the code objects that are causing bottlenecks. This way, you could add a manipulator as a post-compilation step and use a centralized configuration file, like so:\n\n[Leave ERROR and above]\nmy_module.SomeClass.method_with_lots_of_warn_calls\n\n[Leave WARN and above]\nmy_module.SomeOtherClass.method_with_lots_of_info_calls\n\n[Leave INFO and above]\nmy_module.SomeWeirdClass.method_with_lots_of_debug_calls\n\n\nOf course, you'd want to use it sparingly and probably with per-function granularity -- only for code objects that have shown logging to be a bottleneck. Anybody know of anything like this?\n\nNote: There are a few things that make this more difficult to do in a performant manner because of dynamic typing and late binding. For example, any calls to a method named debug may have to be wrapped with an if not isinstance(log, Logger). In any case, I'm assuming all of the minor details can be overcome, either by a gentleman's agreement or some run-time checking. :-)\n", "completion": "What about using logging.disable?\n\nI've also found I had to use logging.isEnabledFor if the logging message is expensive to create.\n"}
{"prompt": "I'm trying to add two images together using NumPy and PIL. The way I would do this in MATLAB would be something like:\n\n>> M1 = imread('_1.jpg');\n>> M2 = imread('_2.jpg');\n>> resM = M1 + M2;\n>> imwrite(resM, 'res.jpg');\n\n\nI get something like this:\n\n\n\nUsing a compositing program and adding the images the MATLAB result seems to be right.\n\nIn Python I'm trying to do the same thing like this:\n\nfrom PIL import Image\nfrom numpy import *\n\nim1 = Image.open('/Users/rem7/Desktop/_1.jpg')\nim2 = Image.open('/Users/rem7/Desktop/_2.jpg')\n\nim1arr = asarray(im1)\nim2arr = asarray(im2)\n\naddition = im1arr + im2arr\n\nresultImage = Image.fromarray(addition)\nresultImage.save('/Users/rem7/Desktop/a.jpg')\n\n\nand I get something like this:\n\n\n\nWhy am I getting all those funky colors? I also tried using ImageMath.eval(\"a+b\", a=im1, b=im2), but I get an error about RGB unsupported.\n\nI also saw that there is an Image.blend() but that requires an alpha.\n\nWhat's the best way to achieve what I'm looking for?\n\nSource Images (images have been removed):\n\n\n\n\nHumm, OK, well I added the source images using the add image icon and they show up when I'm editing the post, but for some reason the images don't show up in the post. \n\n(images have been removed) 2013 05 09\n", "completion": "As everyone suggested already, the weird colors you're observing are overflow. And as you point out in the comment of schnaader's answer you still get overflow if you add your images like this:\n\naddition=(im1arr+im2arr)/2\n\n\nThe reason for this overflow is that your NumPy arrays (im1arr  im2arr) are of the uint8 type (i.e. 8-bit). This means each element of the array can only hold values up to 255, so when your sum exceeds 255, it loops back around 0:\n\n>>>array([255,10,100],dtype='uint8') +  array([1,10,160],dtype='uint8')\narray([ 0, 20,  4], dtype=uint8)\n\n\nTo avoid overflow, your arrays should be able to contain values beyond 255. You need to convert them to floats for instance, perform the blending operation and convert the result back to uint8:\n\nim1arrF = im1arr.astype('float')\nim2arrF = im2arr.astype('float')\nadditionF = (im1arrF+im2arrF)/2\naddition = additionF.astype('uint8')\n\n\nYou should not do this:\n\naddition = im1arr/2 + im2arr/2\n\n\nas you lose information, by squashing the dynamic of the image (you effectively make the images 7-bit) before you perform the blending information.\n\nMATLAB note: the reason you don't see this problem in MATLAB, is probably because MATLAB takes care of the overflow implicitly in one of its functions.\n"}
{"prompt": "Background: I have a small Python application that makes life for developers releasing software in our company a bit easier. I build an executable for Windows using py2exe. The application as well as the binary are checked into Subversion. Distribution happens by people just checking out the directory from SVN. The program has about 6 different Python library dependencies (e.g. ElementTree, Mako)\n\nThe situation: Developers want to hack on the source of this tool and then run it without having to build the binary. Currently this means that they need a python 2.6 interpreter (which is fine) and also have the 6 libraries installed locally using easy_install.\n\nThe Problem\n\n\nThis is not a public, classical open source environment: I'm inside a corporate network, the tool will never leave the \"walled garden\" and we have seriously inconvenient barriers to getting to the outside internet (NTLM authenticating proxies and/or machines without direct internet access).\nI want the hurdles to starting to hack on this tool to be minimal: nobody should have to hunt for the right dependency in the right version, they should have to execute as little setup as possible. Optimally the prerequisites would be having a Python installation and just checking out the program from Subversion.\n\n\nAnecdote: The more self-contained the process is the easier it is to repeat it. I had my machine swapped out for a new one and went through the unpleasant process of having to reverse engineer the dependencies, reinstall distutils, hunting down the libraries online and getting them to install (see corporate internet restrictions above). \n", "completion": "Just use virtualenv - it is a tool to create isolated Python environments. You can create a set-up script and distribute the whole bunch if you want.\n"}
{"prompt": "I know that you can prepopulate admin form fields based on other fields. For example, I have a slug field that is automatically populated based on the title field.\n\nHowever, I would also like to make other automatic prepopulations based on the date. For example, I have an URL field, and I want it to automatically be set to http://example.com/20090209.mp3 where 20090209 is YYYYMMDD. \n\nI would also like to have a text field that automatically starts with something like \"Hello my name is author\" where author is the current user's name. Of course, I also want the person to be able to edit the field. The point is to just make it so the user can fill out the admin form more easily, and not just to have fields that are completely automatic.\n", "completion": "I know that you can prepopulate some values via GET, it will be something like this\n\nhttp://localhost:8000/admin/app/model/add/?model_field=hello\n\n\nI got some problems with date fields but, maybe this could help you.\n"}
{"prompt": "The purpose of my question is to strengthen my knowledge base with Python and get a better picture of it, which includes knowing its faults and surprises.  To keep things specific, I'm only interested in the CPython interpreter.\n\nI'm looking for something similar to what learned from my PHP landmines\nquestion where some of the answers were well known to me but a couple were borderline horrifying.\n\nUpdate:\n   Apparently one maybe two people are upset that I asked a question that's already partially answered outside of Stack Overflow.  As some sort of compromise here's the URL\nhttp://www.ferg.org/projects/python_gotchas.html\n\nNote that one or two answers here already are original from what was written on the site referenced above.\n", "completion": "Expressions in default arguments are calculated when the function is defined, not when it\u00e2\u0080\u0099s called. \n\nExample: consider defaulting an argument to the current time:\n\n>>>import time\n>>> def report(when=time.time()):\n...     print when\n...\n>>> report()\n1210294387.19\n>>> time.sleep(5)\n>>> report()\n1210294387.19\n\n\nThe when argument doesn't change. It is evaluated when you define the function. It won't change until the application is re-started.\n\nStrategy: you won't trip over this if you default arguments to None and then do something useful when you see it:\n\n>>> def report(when=None):\n...     if when is None:\n...         when = time.time()\n...     print when\n...\n>>> report()\n1210294762.29\n>>> time.sleep(5)\n>>> report()\n1210294772.23\n\n\nExercise: to make sure you've understood: why is this happening?\n\n>>> def spam(eggs=[]):\n...     eggs.append(\"spam\")\n...     return eggs\n...\n>>> spam()\n['spam']\n>>> spam()\n['spam', 'spam']\n>>> spam()\n['spam', 'spam', 'spam']\n>>> spam()\n['spam', 'spam', 'spam', 'spam']\n\n"}
{"prompt": "I thought I was starting to get a grip on \"the Python way\" of programming. Methods of a class accept self as the first parameter to refer to the instance of the class whose context the method is being called in. The @classmethod decorator refers to a method whose functionality is associated with the class, but which doesn't reference a specific instance.\n\nSo, what does the first parameter of a @classmethod (canonically 'self') refer to if the method is meant to be called without an instance reference?\n", "completion": "class itself:\n\n\n  A class method receives the class as implicit first argument, just like an instance method receives the instance.\n\n\nclass C:\n    @classmethod\n    def f(cls):\n        print(cls.__name__, type(cls))\n\n>>> C.f()\nC <class 'type'>\n\n\nand it's cls canonically, btw\n"}
{"prompt": "Is there a function to extract the extension from a filename?\n", "completion": "Yes. Use os.path.splitext:\n\n>>> import os\n>>> filename, file_extension = os.path.splitext('/path/to/somefile.ext')\n>>> filename\n'/path/to/somefile'\n>>> file_extension\n'.ext'\n\n"}
{"prompt": "I used to be a java developer and we used tools like ant or maven to manage our development/testing/UAT environments in a standardized way. This allowed us to handle library dependencies, setting OS variables, compiling, deploying, running unit tests, and all the required tasks. Also, the scripts generated guaranteed that all the environments were almost equally configured, and all the task were performed in the same way by all the members of the team. \n\nI'm starting to work in Python now and I'd like your advice in which tools should I use to accomplish the same as described for java.\n", "completion": "\nvirtualenv to create a contained virtual environment (prevent different versions of Python or Python packages from stomping on each other). There is increasing buzz from people moving to this tool.  The author is the same as the older working-env.py mentioned by Aaron.\npip to install packages inside a virtualenv.  The traditional is easy_install as answered by S. Lott, but pip works better with virtualenv.  easy_install still has features not found in pip though.\nscons as a build tool, although you won't need this if you stay purely Python.\nFabric paste, or paver for deployment.\nbuildbot for continuous integration.\nBazaar, mercurial, or git for version control.\nNose as an extension for unit testing.\nPyFit for FIT testing.\n\n"}
{"prompt": "Does anybody know how to make the icon not show up? I'm looking for a way to have no icon at all.\n", "completion": "On Windows\n\nStep One:\n\nCreate a transparent icon using either an icon editor, or a site like rw-designer. Save it as transparent.ico.\n\nStep Two:\n\nfrom tkinter import *\n\ntk = Tk()\ntk.iconbitmap(default='transparent.ico')\nlab = Label(tk, text='Window with transparent icon.')\nlab.pack()\ntk.mainloop()\n\n\nOn Unix\n\nSomething similar, but using an xbm icon. \n"}
{"prompt": "Am i able to overload the print function? and call the normal function? What i want to do is after a specific line i want print to call my print which will call the normal print and write a copy to file.\n\nAlso i dont know how to overload print. I dont know how to do variable length arguments. i'll look it up soon but  http://stackoverflow.com/questions/550470/overload-print-python/550477#550477 just told me i cant overload print in 2.x which is what i am using.\n", "completion": "For those reviewing the previously dated answers, as of version release \"Python 2.6\" there is a new answer to the original poster's question.\n\nIn Python 2.6 and up, you can disable the print statement in favor of the print function, and then override the print function with your own print function:\n\nfrom __future__ import print_function\n# This must be the first statement before other statements.\n# You may only put a quoted or triple quoted string, \n# Python comments, other future statements, or blank lines before the __future__ line.\n\ntry:\n    import __builtin__\nexcept ImportError:\n    # Python 3\n    import builtins as __builtin__\n\ndef print(*args, **kwargs):\n    \"\"\"My custom print() function.\"\"\"\n    # Adding new arguments to the print function signature \n    # is probably a bad idea.\n    # Instead consider testing if custom argument keywords\n    # are present in kwargs\n    __builtin__.print('My overridden print() function!')\n    return __builtin__.print(*args, **kwargs)\n\n\nOf course you'll need to consider that this print function is only module wide at this point. You could choose to override __builtin__.print, but you'll need to save the original __builtin__.print; likely mucking with the __builtin__ namespace.\n"}
{"prompt": "I need to load (de-serialize) a pre-computed list of integers from a file in a Python script (into a Python list). The list is large (upto millions of items), and I can choose the format I store it in, as long as loading is fastest.\n\nWhich is the fastest method, and why?\n\n\nUsing import on a .py file that just contains the list assigned to a variable\nUsing cPickle's load\nSome other method (perhaps numpy?)\n\n\nAlso, how can one benchmark such things reliably?\n\nAddendum: measuring this reliably is difficult, because import is cached so it can't be executed multiple times in a test. The loading with pickle also gets faster after the first time probably because page-precaching by the OS. Loading 1 million numbers with cPickle takes 1.1 sec the first time run, and 0.2 sec on subsequent executions of the script.\n\nIntuitively I feel cPickle should be faster, but I'd appreciate numbers (this is quite a challenge to measure, I think). \n\nAnd yes, it's important for me that this performs quickly.\n\nThanks\n", "completion": "I would guess cPickle will be fastest if you really need the thing in a list.\n\nIf you can use an array, which is a built-in sequence type, I timed this at a quarter of a second for 1 million integers:\n\nfrom array import array\nfrom datetime import datetime\n\ndef WriteInts(theArray,filename):\n    f = file(filename,\"wb\")\n    theArray.tofile(f)\n    f.close()\n\ndef ReadInts(filename):\n    d = datetime.utcnow()\n    theArray = array('i')\n    f = file(filename,\"rb\")\n    try:\n        theArray.fromfile(f,1000000000)\n    except EOFError:\n        pass\n    print \"Read %d ints in %s\" % (len(theArray),datetime.utcnow() - d)\n    return theArray\n\nif __name__ == \"__main__\":\n    a = array('i')\n    a.extend(range(0,1000000))\n    filename = \"a_million_ints.dat\"\n    WriteInts(a,filename)\n    r = ReadInts(filename)\n    print \"The 5th element is %d\" % (r[4])\n\n"}
{"prompt": "Alright, this is probably a really silly question but I am new to Python/Django so I can't really wrap my head around its scoping concepts just yet. Right now I am writing a middleware class to handle some stuff, and I want to set 'global' variables that my views and templates can access. What is the \"right\" way of doing this? I considered doing something like this:\n\nmiddleware.py\n\nfrom django.conf import settings\n\nclass BeforeFilter(object):\n    def process_request(self, request):\n        settings.my_var = 'Hello World'\n        return None\n\n\nviews.py\n\nfrom django.conf import settings\nfrom django.http import HttpResponse\n\ndef myview(request):\n    return HttpResponse(settings.my_var)\n\n\nAlthough this works, I am not sure if it is the \"Django way\" or the \"Python way\" of doing this.\n\nSo, my questions are:\n1. Is this the right way?\n2. If it is the right way, what is the right way of adding variables that can be used in the actual template from the middleware? Say I want to evaluate something and I want to set a variable headername as 'My Site Name' in the middleware, and I want to be able to do {{ headername }} in all templates. Doing it the way I have it now I'd have to add headername to the context inside every view. Is there anyway to bypass this? I am thinking something along the lines of CakePHP's $this->set('headername','My Site Name');\n3. I am using the middleware class as an equivalent of CakePHP's beforeFilter that runs before every view (or controller in CakePHP) is called. Is this the right way of doing this?\n4. Completely unrelated but it is a small question, what is a nice way of printing out the contents of a variable to the browser ala print_r? Say I want to see all the stuff inside the request that is passed into the view? Is pprint the answer?\n", "completion": "\nIt's not the best way. You could set my_var on the request rather than on the settings. Settings are global and apply to the whole site. You don't want to modify it for every request. There could be concurrency issues with multiple request updating/reading the variable at the same time.\nTo access request.my_var in your templates you could do {{ request.my_var }}. To get access to the request variable in your template you will have to add django.core.context_processors.request to your TEMPLATE_CONTEXT_PROCESSORS setting.\nYes. Other terminology to describe request middleware would be request pre-processor/filter/interceptor. \n\n\nAlso, if you want to use a common Site name for the header in your templates, you might want to check out the Django Sites application which provides a site name variable for your use.\n"}
{"prompt": "How to do conditional compilation in Python ?\n\nIs it using DEF ?\n", "completion": "Python isn't compiled in the same sense as C or C++ or even Java, python files are compiled \"on the fly\", you can think of it as being similar to a interpreted language like Basic or Perl.1\n\nYou can do something equivalent to conditional compile by just using an if statement. For example:\n\nif FLAG:\n    def f():\n        print \"Flag is set\"\nelse:\n    def f():\n        print \"Flag is not set\"\n\n\nYou can do the same for the creation classes, setting of variables and pretty much everything.\n\nThe closest way to mimic IFDEF would be to use the hasattr function. E.g.:\n\nif hasattr(aModule, 'FLAG'):\n    # do stuff if FLAG is defined in the current module.\n\n\nYou could also use a try/except clause to catch name errors, but the idiomatic way would be to set a variable to None at the top of your script.\n\n\nPython code is byte compiled into an intermediate form like Java, however there generally isn't a separate compilation step. The \"raw\" source files that end in .py are executable.\n\n"}
{"prompt": "I am using iPython to run my code. I wonder if there is any module or command which would allow me to check the memory usage of an object. For instance:\n\nIn [1]: a = range(10000)\nIn [2]: %memusage a\nOut[2]: 1MB\n\n\nSomething like %memusage <object> and return the memory used by the object.\n\nDuplicate\n\n\n  Find out how much memory is being used by an object in Python\n\n", "completion": "Unfortunately this is not possible, but there are a number of ways of approximating the answer:\n\n\nfor very simple objects (e.g. ints, strings, floats, doubles) which are represented more or less as simple C-language types you can simply calculate the number of bytes as with John Mulder's solution.\nFor more complex objects a good approximation is to serialize the object to a string using cPickle.dumps. The length of the string is a good approximation of the amount of memory required to store an object. \n\n\nThere is one big snag with solution 2, which is that objects usually contain references to other objects. For example a dict contains string-keys and other objects as values. Those other objects might be shared. Since pickle always tries to do a complete serialization of the object it will always over-estimate the amount of memory required to store an object.\n"}
{"prompt": "Questions:\n\n\nWhat is the best practice for \nkeeping track of a tread's \nprogress without locking the GUI \n(\"Not Responding\")?\nGenerally, what are the best practices for\nthreading as it applies to GUI\ndevelopment?\n\n\nQuestion Background:\n\n\nI have a PyQt GUI for Windows.\nIt is used to process sets of HTML\ndocuments.\nIt takes anywhere from three seconds\nto three hours to process a set of\ndocuments.\nI want to be able to process\nmultiple sets at the same time.\nI don't want the GUI to lock.\nI'm looking at the threading module\nto achieve this.\nI am relatively new to threading.\nThe GUI has one progress bar.\nI want it to display the progress of\nthe selected thread.\nDisplay results of the selected\nthread if it's finished.\nI'm using Python 2.5.\n\n\nMy Idea: Have the threads emit a QtSignal when the progress is updated that triggers some function that updates the progress bar.  Also signal when finished processing so results can be displayed.\n\n#NOTE: this is example code for my idea, you do not have\n#      to read this to answer the question(s).\n\nimport threading\nfrom PyQt4 import QtCore, QtGui\nimport re\nimport copy\n\nclass ProcessingThread(threading.Thread, QtCore.QObject):\n\n    __pyqtSignals__ = ( \"progressUpdated(str)\",\n                        \"resultsReady(str)\")\n\n    def __init__(self, docs):\n        self.docs = docs\n        self.progress = 0   #int between 0 and 100\n        self.results = []\n        threading.Thread.__init__(self)\n\n    def getResults(self):\n        return copy.deepcopy(self.results)\n\n    def run(self):\n        num_docs = len(self.docs) - 1\n        for i, doc in enumerate(self.docs):\n            processed_doc = self.processDoc(doc)\n            self.results.append(processed_doc)\n            new_progress = int((float(i)/num_docs)*100)\n\n            #emit signal only if progress has changed\n            if self.progress != new_progress:\n                self.emit(QtCore.SIGNAL(\"progressUpdated(str)\"), self.getName())\n            self.progress = new_progress\n            if self.progress == 100:\n                self.emit(QtCore.SIGNAL(\"resultsReady(str)\"), self.getName())\n\n    def processDoc(self, doc):\n        ''' this is tivial for shortness sake '''\n        return re.findall('<a [^>]*>.*?</a>', doc)\n\n\nclass GuiApp(QtGui.QMainWindow):\n\n    def __init__(self):\n        self.processing_threads = {}  #{'thread_name': Thread(processing_thread)}\n        self.progress_object = {}     #{'thread_name': int(thread_progress)}\n        self.results_object = {}      #{'thread_name': []}\n        self.selected_thread = ''     #'thread_name'\n\n    def processDocs(self, docs):\n        #create new thread\n        p_thread = ProcessingThread(docs)\n        thread_name = \"example_thread_name\"\n        p_thread.setName(thread_name)\n        p_thread.start()\n\n        #add thread to dict of threads\n        self.processing_threads[thread_name] = p_thread\n\n        #init progress_object for this thread\n        self.progress_object[thread_name] = p_thread.progress  \n\n        #connect thread signals to GuiApp functions\n        QtCore.QObject.connect(p_thread, QtCore.SIGNAL('progressUpdated(str)'), self.updateProgressObject(thread_name))\n        QtCore.QObject.connect(p_thread, QtCore.SIGNAL('resultsReady(str)'), self.updateResultsObject(thread_name))\n\n    def updateProgressObject(self, thread_name):\n        #update progress_object for all threads\n        self.progress_object[thread_name] = self.processing_threads[thread_name].progress\n\n        #update progress bar for selected thread\n        if self.selected_thread == thread_name:\n            self.setProgressBar(self.progress_object[self.selected_thread])\n\n    def updateResultsObject(self, thread_name):\n        #update results_object for thread with results\n        self.results_object[thread_name] = self.processing_threads[thread_name].getResults()\n\n        #update results widget for selected thread\n        try:\n            self.setResultsWidget(self.results_object[thread_name])\n        except KeyError:\n            self.setResultsWidget(None)\n\n\nAny commentary on this approach (e.g. drawbacks, pitfalls, praises, etc.) will be appreciated.\n\nResolution:\n\nI ended up using the QThread class and associated signals and slots to communicate between threads.  This is primarily because my program already uses Qt/PyQt4 for the GUI objects/widgets.  This solution also required fewer changes to my existing code to implement.\n\nHere is a link to an applicable Qt article that explains how Qt handles threads and signals, http://www.linuxjournal.com/article/9602. Excerpt below:\n\n\n  Fortunately, Qt permits\n  signals and slots to be connected\n  across threads\u00e2\u0080\u0094as long as the threads\n  are running their own event loops.\n  This is a much cleaner method of\n  communication compared to sending and\n  receiving events, because it avoids\n  all the bookkeeping and intermediate\n  QEvent-derived classes that become\n  necessary in any nontrivial\n  application. Communicating between\n  threads now becomes a matter of\n  connecting signals from one thread to\n  the slots in another, and the mutexing\n  and thread-safety issues of exchanging\n  data between threads are handled by\n  Qt.\n  \n  Why is it necessary to run an event\n  loop within each thread to which you\n  want to connect signals? The reason\n  has to do with the inter-thread\n  communication mechanism used by Qt\n  when connecting signals from one\n  thread to the slot of another thread.\n  When such a connection is made, it is\n  referred to as a queued connection.\n  When signals are emitted through a\n  queued connection, the slot is invoked\n  the next time the destination object's\n  event loop is executed. If the slot\n  had instead been invoked directly by a\n  signal from another thread, that slot\n  would execute in the same context as\n  the calling thread. Normally, this is\n  not what you want (and especially not\n  what you want if you are using a\n  database connection, as the database\n  connection can be used only by the\n  thread that created it). The queued\n  connection properly dispatches the\n  signal to the thread object and\n  invokes its slot in its own context by\n  piggy-backing on the event system.\n  This is precisely what we want for\n  inter-thread communication in which\n  some of the threads are handling\n  database connections. The Qt\n  signal/slot mechanism is at root an\n  implementation of the inter-thread\n  event-passing scheme outlined above,\n  but with a much cleaner and\n  easier-to-use interface.\n\n\nNOTE: eliben also has a good answer, and if I weren't using PyQt4, which handles thread-safety and mutexing, his solution would have been my choice.\n", "completion": "If you want to use signals to indicate progress to the main thread then you should really be using PyQt's QThread class instead of the Thread class from Python's threading module.\n\nA simple example which uses QThread, signals and slots can be found on the PyQt Wiki:\n\nhttps://wiki.python.org/moin/PyQt/Threading,_Signals_and_Slots\n"}
{"prompt": "Is it possible to append elements to a python generator?\n\nI'm currently trying to get all images from a set of disorganized folders and write them to a new directory. To get the files, I'm using os.walk() which returns a list of image files in a single directory. While I can make a generator out of this single list, I don't know how to combine all these lists into one single generator. Any help would be much appreciated.\n\nRelated:\n\n\nFlattening a shallow list in python\n\n", "completion": "You are looking for itertools.chain. It will combine multiple iterables into a single one, like this:\n\n>>> for i in itertools.chain([1,2,3], [4,5,6]):\n...  print i\n... \n1\n2\n3\n4\n5\n6\n\n"}
{"prompt": "I have a try...except block in my code and When an exception is throw. I really just want to continue with the code because in that case, everything is still able to run just fine. The problem is if you leave the except: block empty or with a #do nothing, it gives you a syntax error. I can't use continue because its not in a loop. Is there a keyword i can use that tells the code to just keep going?\n", "completion": "except:\n    pass\n\n"}
{"prompt": "Look at this simple function\n\ndef prime_factors(n):\n    for i in range(2,n):\n      if n % i == 0:\n        return i, prime_factors(n / i)\n    return n\n\n\nHere's the result of prime_factors(120)\n\n(2, (2, (2, (3, 5))))\n\n\nInstead of nested tuples, I want it to return one flat tuple or list.\n\n(2, 2, 2, 3, 5)\n\n\nIs there a simple way to do that?\n", "completion": "def prime_factors(n):\n  for i in range(2,n):\n    if n % i == 0:\n      return [i] + prime_factors(n / i)\n  return [n]\n\n"}
{"prompt": "What is an easy way in Python to format integers into strings representing thousands with K, and millions with M, and leaving just couple digits after comma?\n\nI'd like to show 7436313 as 7.44M, and 2345 as 2,34K.\n\nIs there some % string formatting operator available for that? Or that could be done only by actually dividing by 1000 in a loop and constructing result string step by step?\n", "completion": "I don't think there's a built-in function that does that.  You'll have to roll your own, e.g.:\n\ndef human_format(num):\n    magnitude = 0\n    while abs(num) >= 1000:\n        magnitude += 1\n        num /= 1000.0\n    # add more suffixes if you need them\n    return '%.2f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n\nprint('the answer is %s' % human_format(7436313))  # prints 'the answer is 7.44M'\n\n"}
{"prompt": "I am doing some prototyping for a new desktop app i am writing in Python, and i want to use SQLite and an ORM to store data.\n\nMy question is, are there any ORM libraries that support auto-generating/updating the database schema and work with SQLite?\n", "completion": "SQLAlchemy is a great choice in the Python ORM space that supports SQLite.\n"}
{"prompt": "Is there any way of writing a decorator such that the following would work?\n\nassert 'z' not in globals()\n\n@my_decorator\ndef func(x, y):\n   print z\n\n\n\n\nEDIT: moved from anwser\n\nIn answer to hop's \"why?\": syntax sugar / DRY.\n\nIt's not about caching, it's about calculating z (and z1, z2, z3, ...) based upon the values of x & y.\n\nI have lots of functions which do related things, and I don't want to do have to write\n\nz1, z2, z3=calculate_from(x, y)\n\n\nat the beginning of every single function - I'll get it wrong somewhere. If this were c I'd do this with cpp (if this were lisp, I'd do this with macros ...), but I wanted to see if decorators could do the same thing.\n\nIf it helps, I'd almost certainly call the decorator \"precalculate_z\", and it certainly wouldn't be part of any public API.\n\nI could probably get a similar effect from using the class infrastructure as well, but I wanted to see if it was doable with raw functions.\n", "completion": "Echoing Hop's answer\n\n\nDon't do it.\nSeriously, don't do this.  Lisp and Ruby are more appropriate languages for writing your own custom syntax.  Use one of those.  Or find a cleaner way to do this\nIf you must, you want dynamic scoped variables, not lexically scoped.\n\n\nPython doesn't have dynamically scoped variables, but you can simulate it.  Here's an example that simulates it by creating a global binding, but restores the previous value on exit:\n\nhttp://codepad.org/6vAY8Leh\n\ndef adds_dynamic_z_decorator(f):\n  def replacement(*arg,**karg):\n    # create a new 'z' binding in globals, saving previous\n    if 'z' in globals():\n      oldZ = (globals()['z'],)\n    else:\n      oldZ = None\n    try:\n      globals()['z'] = None\n      #invoke the original function\n      res = f(*arg, **karg)\n    finally:\n      #restore any old bindings\n      if oldZ:\n        globals()['z'] = oldZ[0]\n      else:\n        del(globals()['z'])\n    return res\n  return replacement\n\n@adds_dynamic_z_decorator\ndef func(x,y):\n  print z\n\ndef other_recurse(x):\n  global z\n  print 'x=%s, z=%s' %(x,z)\n  recurse(x+1)\n  print 'x=%s, z=%s' %(x,z)\n\n@adds_dynamic_z_decorator\ndef recurse(x=0):\n  global z\n  z = x\n  if x < 3:\n    other_recurse(x)\n\nprint 'calling func(1,2)'\nfunc(1,2)\n\nprint 'calling recurse()'\nrecurse()\n\n\nI make no warranties on the utility or sanity of the above code.  Actually, I warrant that it is insane, and you should avoid using it unless you want a flogging from your Python peers.\n\nThis code is similar to both eduffy's and John Montgomery's code, but ensures that 'z' is created and properly restored \"like\" a local variable would be -- for instance, note how 'other_recurse' is able to see the binding for 'z' specified in the body of 'recurse'.  \n"}
{"prompt": "If you invoke the cpython interpreter with the -i option, it will enter the interactive mode upon completing any commands or scripts it has been given to run.  Is there a way, within a program to get the interpreter to do this even when it has not been given -i? The obvious use case is in debugging by interactively inspecting the state when an exceptional condition has occurred.\n", "completion": "You want the code module.\n\n#!/usr/bin/env python\n\nimport code    \ncode.interact(\"Enter Here\")\n\n"}
{"prompt": "I have a view that accepts a form submission and updates a model.\n\nAfter updating the model, I want to redirect to another page, and I want a message such as \"Field X successfully updated\" to appear on this page.\n\nHow can I \"pass\" this message to the other page? HttpResponseRedirect only accepts a URL. I've seen this done before on other sites. How is this accomplished?\n", "completion": "This is a built-in feature of Django, called \"messages\"\n\nSee http://docs.djangoproject.com/en/dev/topics/auth/#messages\n\nFrom the documentation:\n\n\n  A message is associated with a User.\n  There's no concept of expiration or\n  timestamps.\n  \n  Messages are used by the Django admin\n  after successful actions. For example,\n  \"The poll Foo was created\n  successfully.\" is a message.\n\n"}
{"prompt": "I have a project where i'm trying to use weakrefs with callbacks, and I don't understand what I'm doing wrong. I have created simplified test that shows the exact behavior i'm confused with. \n\nWhy is it that in this test test_a works as expected, but the weakref for self.MyCallbackB disappears between the class initialization and calling test_b? I thought like as long as the instance (a) exists, the reference to self.MyCallbackB should exist, but it doesn't.\n\nimport weakref\n\nclass A(object):\n    def __init__(self):\n\n        def MyCallbackA():\n            print 'MyCallbackA'\n        self.MyCallbackA = MyCallbackA\n\n        self._testA = weakref.proxy(self.MyCallbackA)\n        self._testB = weakref.proxy(self.MyCallbackB)\n\n    def MyCallbackB(self):\n        print 'MyCallbackB'\n\n    def test_a(self):\n        self._testA()\n\n    def test_b(self):\n        self._testB()\n\nif __name__ == '__main__':\n    a = A()    \n    a.test_a()\n    a.test_b()\n\n", "completion": "You want a WeakMethod. \n\nAn explanation why your solution doesn't work can be found in the discussion of the recipe:\n\n\n  Normal weakref.refs to bound methods don't quite work the way one expects, because bound methods are first-class objects; weakrefs to bound methods are dead-on-arrival unless some other strong reference to the same bound method exists.\n\n"}
{"prompt": "In my application I have to maintain some global application state and global application wide methods like currently connected users, total number of answers, create an application config file etc. There are two options:\n\n\nMake a separate appstate.py file with global variables with functions over them. It looks fine initially but it seems that I am missing something in clarity of my code.\nCreate a class AppState with class functions in a appstate.py file, all other modules have been defined by their specific jobs. This looks fine. But now I have to write longer line like appstate.AppState.get_user_list(). Moreover, the methods are not so much related to each other. I can create separate classes but that would be too many classes.\n\n\nEDIT: If I use classes I will be using classmethods. I don't think there is a need to instantiate the class to an object.\n", "completion": "Sounds like the classic conundrum :-)  In Python, there's nothing dirty or shameful about choosing to use a module if that's the best approach. After all, modules, functions, and the like are, in fact, first-class citizens in the language, and offer introspection and other properties that many other languages use objects to obtain.\n\nThe way you've described your options, it kinda sounds like you're not too crazy about a class-based approach in this case.\n\nI don't know if you've used the Django framework, but if not have a look at their documentation on how they handle settings. These are app-wide, they are defined in a module, and they are available gobally. The way they parse the options and expose them globally is quite elegant, and you may find such an approach inspiring for your needs.\n"}
{"prompt": "What, in Your opinion is a meaningful docstring? What do You expect to be described there?\n\nFor example, consider this Python class's __init__:\n\ndef __init__(self, name, value, displayName=None, matchingRule=\"strict\"):\n    \"\"\"\n    name - field name\n    value - field value\n    displayName - nice display name, if empty will be set to field name\n    matchingRule - I have no idea what this does, set to strict by default\n    \"\"\"\n\n\nDo you find this meaningful? Post Your good/bad examples for all to know (and a general answer so it can be accepted).\n", "completion": "I agree with \"Anything that you can't tell from the method's signature\". It might also mean to explain what a method/function returns.\n\nYou might also want to use Sphinx (and reStructuredText syntax) for documentation purposes inside your docstrings. That way you can include this in your documentation easily. For an example check out e.g. repoze.bfg which uses this extensively (example file, documentation example).\n\nAnother thing one can put in docstrings is also doctests. This might make sense esp. for module or class docstrings as you can also show that way how to use it and have this testable at the same time.\n"}
{"prompt": "I have implemented a class in C++. I want to use it with Python.\nPlease suggest step by step method and elaborate each step.\nSomthing like this...\n\nclass Test{\n     private:\n        int n;\n     public:\n        Test(int k){\n            n=k;\n        }\n        void setInt(int k){\n            n = k; \n        }\n        int getInt(){\n            return n;\n        }\n};\n\n\nNow, in Python \n\n>>> T1 = Test(12)\n>>> T1.getInt()\n12\n>>> T1.setInt(32)\n>>> T1.getInt()\n32\n\n\nPlease suggest.How can I do this ?\nNOTE: I would like to know manual way to do that. I don't want any third party library dependency.\n", "completion": "Look into Boost.Python. It's a library to write python modules with C++.\n\nAlso look into SWIG which can also handle modules for other scripting languages. I've used it in the past to write modules for my class and use them within python. Works great.\n\nYou can do it manually by using the Python/C API, writing the interface yourself. It's pretty lowlevel, but you will gain a lot of additional knowledge of how Python works behind the scene (And you will need it when you use SWIG anyway).\n"}
{"prompt": "I have a string that can be a hex number prefixed with \"0x\" or a decimal number without a special prefix except for possibly a minus sign. \"0x123\" is in base 16 and \"-298\" is in base 10.\n\nHow do I convert this to an int or long in Python?\n\nI don't want to use eval() since it's unsafe and overkill.\n", "completion": "int(\"0x123\", 0)\n\n\n(why doesn't int(\"0x123\") do that?)\n"}
{"prompt": "Is there a way to debug a regular expression in Python? And I'm not referring to the process of trying and trying till they work :)\n\nEDIT: here is how regexes can be debugged in perl :\n\n\nuse re 'debug';\n\nmy $str = \"GET http://some-site.com HTTP/1.1\";\nif($str =~/get\\s+(\\S+)/i) {\n    print \"MATCH:$1\\n\";\n}\n\n\nThe code above produces the following output on my computer when ran :\n\n\nCompiling REx \"get\\s+(\\S+)\"\nFinal program:\n   1: EXACTF  (3)\n   3: PLUS (5)\n   4:   SPACE (0)\n   5: OPEN1 (7)\n   7:   PLUS (9)\n   8:     NSPACE (0)\n   9: CLOSE1 (11)\n  11: END (0)\nstclass EXACTF  minlen 5\nMatching REx \"get\\s+(\\S+)\" against \"GET http://some-site.com HTTP/1.1\"\nMatching stclass EXACTF  against \"GET http://some-site.com HTTP/1.1\" (33 chars)\n   0           |  1:EXACTF (3)\n   3        |  3:PLUS(5)\n                                  SPACE can match 1 times out of 2147483647...\n   4       |  5:  OPEN1(7)\n   4       |  7:  PLUS(9)\n                                    NSPACE can match 20 times out of 2147483647...\n  24       |  9:    CLOSE1(11)\n  24       | 11:    END(0)\nMatch successful!\nMATCH:http://some-site.com\nFreeing REx: \"get\\s+(\\S+)\"\n\n\n", "completion": "\n\n>>> p = re.compile('.*', re.DEBUG)\nmax_repeat 0 65535\n  any None\n>>>                         \n\n\n\nhttp://stackoverflow.com/questions/580993/regex-operator-vs-separate-runs-for-each-sub-expression/582227#582227\n"}
{"prompt": "I`m currently studying C++ and want to learn another language.\nFor work I use C# + ASP (just started learning it, actually), but I want something \"less Microsoft\" and powerful.\nI have heard Python is a popular and powerful language, not so complicated as C++. But many people mentioned it was hard for them to get back to C++/Java from Python because they started thinking in it, get used to absence of memory management, etc.\nWhat do you recommend?\n", "completion": "There's no right or wrong answer, really.  But I think you'll benefit more from learning Python.  Given the similarities between C# and C++, you'll learn a different way of thinking from Python.  The more ways you learn to think about a problem, the better it makes you as a programmer, regardless of the language.\n"}
{"prompt": "I've been hacking classes in Python like this:\n\ndef hack(f,aClass) :\n  class MyClass(aClass) :\n     def f(self) :\n       f()\n  return MyClass\n\nA = hack(afunc,A)\n\n\nWhich looks pretty clean to me. It takes a class, A, creates a new class derived from it that has an extra method, calling f, and then reassigns the new class to A.\n\nHow does this differ from metaclass hacking in Python? What are the advantages of using a metaclass over this?\n", "completion": "The definition of a class in Python is an instance of type (or an instance of a subclass of type). In other words, the class definition itself is an object. With metaclasses, you have the ability to control the type instance that becomes the class definition.\n\nWhen a metaclass is invoked, you have the ability to completely re-write the class definition. You have access to all the proposed attributes of the class, its ancestors, etc. More than just injecting a method or removing a method, you can radically alter the inheritance tree, the type, and pretty much any other aspect. You can also chain metaclasses together for a very dynamic and totally convoluted experience.\n\nI suppose the real benefit, though is that the class's type remains the class's type. In your example, typing:\n\na_inst = A()\ntype(a_inst)\n\n\nwill show that it is an instance of MyClass. Yes, isinstance(a_inst, aClass) would return True, but you've introduced a subclass, rather than a dynamically re-defined class. The distinction there is probably the key.\n\nAs rjh points out, the anonymous inner class also has performance and extensibility implications. A metaclass is processed only once, and the moment that the class is defined, and never again. Users of your API can also extend your metaclass because it is not enclosed within a function, so you gain a certain degree of extensibility.\n\nThis slightly old article actually has a good explanation that compares exactly the \"function decoration\" approach you used in the example with metaclasses, and shows the history of the Python metaclass evolution in that context: http://www.ibm.com/developerworks/linux/library/l-pymeta.html\n"}
{"prompt": "When I have created a table with an auto-incrementing primary key, is there a way to obtain what the primary key would be (that is, do something like reserve the primary key) without actually committing?\n\nI would like to place two operations inside a transaction however one of the operations will depend on what primary key was assigned in the previous operation.\n", "completion": "You don't need to commit, you just need to flush.  Here's some sample code.  After the call to flush you can access the primary key that was assigned.  Note this is with SA 0.4.8.\n\nfrom sqlalchemy import *\nfrom sqlalchemy.databases.mysql import *\nimport sqlalchemy.ext.declarative\n\nBase = sqlalchemy.ext.declarative.declarative_base()\n\nclass User(Base):\n    __tablename__ = 'user'\n    user_id = Column('user_id', Integer, primary_key=True)\n    name = Column('name', String)\n\nif __name__ == '__main__':\n    import unittest\n    from sqlalchemy.orm import *\n    import datetime\n\n    class Blah(unittest.TestCase):\n        def setUp(self):\n            self.engine = create_engine('sqlite:///:memory:', echo=True)\n            self.sessionmaker = scoped_session(sessionmaker(bind=self.engine))\n            Base.metadata.bind = self.engine\n            Base.metadata.create_all()\n            self.now = datetime.datetime.now()\n\n        def test_pkid(self):\n            user = User(name=\"Joe\")\n            session = self.sessionmaker()\n            session.save(user)\n            session.flush()\n            print 'user_id', user.user_id\n            session.commit()\n            session.close()\n\n    unittest.main()\n\n"}
{"prompt": "I have this code:\n\n>>> class G:\n...   def __init__(self):\n...     self.x = 20\n...\n>>> gg = G()\n>>> gg.x\n20\n>>> gg.y = 2000\n\n\nAnd this code:\n\n>>> from datetime import datetime\n>>> my_obj = datetime.now()\n>>> my_obj.interesting = 1\n*** AttributeError: 'datetime.datetime' object has no attribute 'interesting'\n\n\nFrom my Python knowledge, I would say that datetime overrides setattr/getattr, but I am not sure. Could you shed some light here?\n\nEDIT: I'm not specifically interested in datetime. I was wondering about objects in general.\n", "completion": "My guess, is that the implementation of datetime uses __slots__ for better performance.\n\nWhen using __slots__, the interpreter reserves storage for just the attributes listed, nothing else. This gives better performance and uses less storage, but it also means you can't add new attributes at will.\n\nRead more here: http://docs.python.org/reference/datamodel.html\n"}
{"prompt": "I have been playing around with writing some simple card games in Python for fun and I would like to add a graphical user interface (GUI) to the games. Which library would you recommend for writing the GUI for a simple card game?\n", "completion": "If all you want is a GUI, wxPython should do the trick.\n\nIf you're looking to add sound, controller input, and take it beyond a simple card game, then you may want to use pygame.\n"}
{"prompt": "\nI'm trying to add a simple \"page x of y\" to a report made with ReportLab.. I found this old post about it, but maybe six years later something more straightforward has emerged? ^^;\nI found this recipe too, but when I use it, the resulting PDF is missing the images..\n", "completion": "I was able to implement the NumberedCanvas approach from ActiveState.  It was very easy to do and did not change much of my existing code. All I had to do was add that NumberedCanvas class and add the canvasmaker attribute when building my doc.  I also changed the measurements of where the \"x of y\" was displayed:\n\nself.doc.build(pdf)\n\n\nbecame \n\nself.doc.build(pdf, canvasmaker=NumberedCanvas)\n\n\ndoc is a BaseDocTemplate and pdf is my list of flowable elements.\n"}
{"prompt": "I am in the field of data crunching and very soon might make a move to the world of web programming. Although I am fascinated both by Python and Ruby as both of them seem to be having every similar styles when it comes to writing business logic or data crunching logic.\n\nBut when I start googling for web development I start inclining towards Ruby on Rails my question is why is the web world obsessed with ruby on rails and active records so much?\n\nThere seem to be so many screencasts to learn Ruby on Rails and plethora of good books too \nwhy is Python not able to pull the crowd  when it comes to creating screencasts or ORM's like active record.\n", "completion": "Ruby and Python are languages.\n\nRails is a framework.\n\nSo it is not really sensible to compare Ruby on Rails vs Python.\n\nThere are Python Frameworks out there you should take a look at for a more direct comparison - http://wiki.python.org/moin/WebFrameworks (e.g. I know Django gets a lot of love, but there are others)\n\nEdit: I've just had a google, there seem to be loads of Django Screencasts.\n"}
{"prompt": "I was wondering if someone could help me out.  In some web application, the app will send out emails, say when a new message has been posted.  Then instead of signing into the application to post a reply you can just simply reply to the email and it will automatically update the web app with your response.\n\nMy question is, how is this done and what is it called?\n\nThanks\n", "completion": "Generally:\n\n1) Set up a dedicated email account for the purpose.\n\n2) Have a programm monitor the mailbox (let's say fetchmail, since that's what I do).\n\n3) When an email arrives at the account, fetchmail downloads the email, writes it to disk, and calls script or program you have written with the email file as an argument.\n\n4) Your script or program parses the email and takes an appropriate action.\n\nThe part that's usually mysterious to people is the fetchmail part (#2).\n\nSpecifically on Mail Servers (iff you control the mailserver enough to redirect emails to scripts):\n\n1-3) Configure an address to be piped to a script you have written.\n\n4) Same as above.\n"}
{"prompt": "Right now I have a central module in a framework that spawns multiple processes using the Python 2.6 multiprocessing module. Because it uses multiprocessing, there is module-level multiprocessing-aware log, LOG = multiprocessing.get_logger(). Per the docs, this logger has process-shared locks so that you don't garble things up in sys.stderr (or whatever filehandle) by having multiple processes writing to it simultaneously.\n\nThe issue I have now is that the other modules in the framework are not multiprocessing-aware. The way I see it, I need to make all dependencies on this central module use multiprocessing-aware logging. That's annoying within the framework, let alone for all clients of the framework. Are there alternatives I'm not thinking of?\n", "completion": "I just now wrote a log handler of my own that just feeds everything to the parent process via a pipe.  I've only been testing it for ten minutes but it seems to work pretty well. \n\n(Note: This is hardcoded to RotatingFileHandler, which is my own use case.)\n\n\n\nUpdate: Implementation!\n\nThis now uses a queue for correct handling of concurrency, and also recovers from errors correctly.  I've now been using this in production for several months, and the current version below works without issue.\n\nfrom logging.handlers import RotatingFileHandler\nimport multiprocessing, threading, logging, sys, traceback\n\nclass MultiProcessingLog(logging.Handler):\n    def __init__(self, name, mode, maxsize, rotate):\n        logging.Handler.__init__(self)\n\n        self._handler = RotatingFileHandler(name, mode, maxsize, rotate)\n        self.queue = multiprocessing.Queue(-1)\n\n        t = threading.Thread(target=self.receive)\n        t.daemon = True\n        t.start()\n\n    def setFormatter(self, fmt):\n        logging.Handler.setFormatter(self, fmt)\n        self._handler.setFormatter(fmt)\n\n    def receive(self):\n        while True:\n            try:\n                record = self.queue.get()\n                self._handler.emit(record)\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except EOFError:\n                break\n            except:\n                traceback.print_exc(file=sys.stderr)\n\n    def send(self, s):\n        self.queue.put_nowait(s)\n\n    def _format_record(self, record):\n        # ensure that exc_info and args\n        # have been stringified.  Removes any chance of\n        # unpickleable things inside and possibly reduces\n        # message size sent over the pipe\n        if record.args:\n            record.msg = record.msg % record.args\n            record.args = None\n        if record.exc_info:\n            dummy = self.format(record)\n            record.exc_info = None\n\n        return record\n\n    def emit(self, record):\n        try:\n            s = self._format_record(record)\n            self.send(s)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\n    def close(self):\n        self._handler.close()\n        logging.Handler.close(self)\n\n"}
{"prompt": "I am planning to write an simple 3d(isometric view) game in Java using jMonkeyEngine - nothing to fancy, I just want to learn something about OpenGL and writing efficient algorithms (random map generating ones). \n\nWhen I was planning what to do, I started wondering about switching to Python. I know that Python didn't come into existence to be a tool to write 3d games, but is it possible to write good looking games with this language? \n\nI have in mind 3d graphics, nice effects and free CPU time to power to rest of game engine? I had seen good looking java games - and too be honest, I was rather shocked when I saw level of detail achieved in Runescape HD. \n\nOn the other hand, pygame.org has only 2d games, with some starting 3d projects. Are there any efficient 3d game engines for python? Is pyopengl the only alternative? Good looking games in python aren't popular or possible to achieve? \n\nI would be grateful for any information / feedback.\n", "completion": "If you are worried about 3D performance: Most of the performance-critical parts will be handled by OpenGL (in a C library or even in hardware), so the language you use to drive it should not matter too much.\n\nTo really find out if performance is a problem, you'd have to try it. But there is no reason why it cannot work in principle.\n\nAt any rate, you could still optimize the critical parts, either in Python or by dropping to C. You still gain Python's benefit for most of the game engine which is less performance-critical.\n"}
{"prompt": "Empirically, it seems that Python's default list sorter, when passed a list of tuples, will sort by the first element in each tuple. Is that correct? If not, what's the right way to sort a list of tuples by their first elements?\n", "completion": "It automatically sorts a list of tuples by the first elements in the tuples, then by the second elements and so on tuple([1,2,3]) will go before tuple([1,2,4]). If you want to override this behaviour pass a callable as the second argument to the sort method. This callable should return 1, -1, 0.\n"}
{"prompt": "I know about unittest Python module.\n\nI know about assertRaises() method of TestCase class.\n\nI would like to write a test that succeeds when an exception is not raised.\n\nAny hints please?\n", "completion": "def runTest(self):\n    try:\n        doStuff()\n    except:\n        self.fail(\"Encountered an unexpected exception.\")\n\n\nUPDATE: As liw.fi mentions, the default result is a success, so the example above is something of an antipattern. You should probably only use it if you want to do something special before failing. You should also catch the most specific exceptions possible.\n"}
{"prompt": "I have a list of integers.  I want to know whether the number 13 appears in it and, if so, where.  Do I have to search the list twice, as in the code below?\n\nif 13 in intList:\n   i = intList.index(13)\n\n\nIn the case of dictionaries, there's a get function which will ascertain membership and perform look-up with the same search.  Is there something similar for lists?\n", "completion": "You answered it yourself, with the index() method. That will throw an exception if the index is not found, so just catch that:\n\ndef getIndexOrMinusOne(a, x):\n  try:\n    return a.index(x)\n  except ValueError:\n    return -1\n\n"}
{"prompt": "This really looks like something I should be able to find on Google, but for some reason I can't make heads or tails of it. There's the EDITOR environment variable, the ipy_user_conf.py file, the ipythonrc file, some weird thing about running gvim in server mode and a bunch of other stuff I can't wrap my head around (probably because of lack of sleep).\n\nIs there a guide somewhere I can follow, or maybe someone can just outline the steps I need to take?\n", "completion": "Setting the EDITOR environment variable to 'gvim -f' seems to work.\n\nset EDITOR=gvim -f\n\n"}
{"prompt": "I want to get the full command line as it was typed. \n\nThis: \n\n\" \".join(sys.argv[:]) \n\ndoesn't work here (deletes double quotes). Also I prefer not to rejoin something that was parsed and splited.\n\nAny ideas? \n\nThank you in advance.\n", "completion": "You're too late. By the time that the typed command gets to Python your shell has already worked its magic. For example, quotes get consumed (as you've noticed), variables get interpolated, etc.\n"}
{"prompt": "I want my python application to be able to tell when the socket on the other side has been dropped. Is there a method for this?\n", "completion": "It depends on what you mean by \"dropped\".  For TCP sockets, if the other end closes the connection either through \nclose() or  the process terminating, you'll find out by reading an end of file, or getting a read error, usually the errno being set to whatever 'connection reset by peer' is by your operating system.   For python, you'll read a zero length string, or a socket.error will be thrown when you try to read or write from the socket.\n"}
{"prompt": "Several months ago, I wrote a blog post detailing how to achieve tab-completion in the standard Python interactive interpreter--a feature I once thought only available in IPython. I've found it tremendously handy given that I sometimes have to switch to the standard interpreter due to IPython unicode issues.\n\nRecently I've done some work in OS X. To my discontent, the script doesn't seem to work for OS X's Terminal application. I'm hoping some of you with experience in OS X might be able to help me trouble-shoot it so it can work in Terminal, as well.\n\nI am reproducing the code below\n\nimport atexit\nimport os.path\n\ntry:\n    import readline\nexcept ImportError:\n    pass\nelse:\n    import rlcompleter\n\n    class IrlCompleter(rlcompleter.Completer):\n        \"\"\"\n        This class enables a \"tab\" insertion if there's no text for\n        completion.\n\n        The default \"tab\" is four spaces. You can initialize with '\\t' as\n        the tab if you wish to use a genuine tab.\n\n        \"\"\"\n\n        def __init__(self, tab='    '):\n            self.tab = tab\n            rlcompleter.Completer.__init__(self)\n\n\n        def complete(self, text, state):\n            if text == '':\n                readline.insert_text(self.tab)\n                return None\n            else:\n                return rlcompleter.Completer.complete(self,text,state)\n\n\n    #you could change this line to bind another key instead tab.\n    readline.parse_and_bind('tab: complete')\n    readline.set_completer(IrlCompleter('\\t').complete)\n\n\n# Restore our command-line history, and save it when Python exits.\nhistory_path = os.path.expanduser('~/.pyhistory')\nif os.path.isfile(history_path):\n    readline.read_history_file(history_path)\natexit.register(lambda x=history_path: readline.write_history_file(x))\n\n\nNote that I have slightly edited it from the version on my blog post so that the IrlCompleter is initialized with a true tab, which seems to be what is output by the Tab key in Terminal.\n", "completion": "This should work under Leopard's python:\n\nimport rlcompleter\nimport readline\nreadline.parse_and_bind (\"bind ^I rl_complete\")\n\n\nWhereas this one does not:\n\nimport readline, rlcompleter\nreadline.parse_and_bind(\"tab: complete\")\n\n\nSave it in ~/.pythonrc.py and execute in .bash_profile\n\nexport PYTHONSTARTUP=$HOME/.pythonrc.py\n\n"}
{"prompt": "I'm currently trying to optimize my website, which run on the google's appengine. It's not an easy task, because I'm not using any powerful tool.\n\nDoes anyone have experience in optimizing python code for this purpose?\nHave you find a good python profiler?\n", "completion": "I have found Gprof2Dot extremely useful. The output of the profiling modules I've tried as pretty unintuitive to interpret.\n\nGprof2Dot turns the cProfile output into a pretty looking graph, with the slowest chain(?) highlighted, and a bit of information on each function (function name, percentage of time spend on this function, and number of calls).\n\nAn example graph (1429x1896px)\n\nI've not done much with the App Engine, but when profiling non-webapp scripts, I tend to profile the script that runs all the unittests, which may not be very accurate to real-world situations\n\nOne (better?) method would be to have a script that does a fake WSGI request, then profile that.\n\nWSGI is really simple protocol, it's basically a function that takes two arguments, one with request info and the second with a callback function (which is used for setting headers, among other things). Perhaps something like the following (which is possible-working pseudo code)...\n\nclass IndexHandler(webapp.RequestHandler):\n    \"\"\"Your site\"\"\"\n    def get(self):\n        self.response.out.write(\"hi\")\n\nif __name__ == '__main__':\n    application = webapp.WSGIApplication([\n        ('.*', IndexHandler),\n    ], debug=True)\n\n    # Start fake-request/profiling bit\n    urls = [\n        \"/\",\n        \"/blog/view/hello\",\n        \"/admin/post/edit/hello\",\n        \"/makeanerror404\",\n        \"/makeanerror500\"\n    ]\n\n    def fake_wsgi_callback(response, headers):\n        \"\"\"Prints heads to stdout\"\"\"\n        print(\"\\n\".join([\"%s: %s\" % (n, v) for n, v in headers]))\n        print(\"\\n\")\n\n    for request_url in urls:\n        html = application({\n        'REQUEST_METHOD': 'GET',\n        'PATH_INFO': request_url},\n        fake_wsgi_callback\n        )\n        print html\n\n\nActually, the App Engine documentation explains a better way of profiling your application:\n\nFrom http://code.google.com/appengine/kb/commontasks.html#profiling:\n\n\n  To profile your application's performance, first rename your application's main() function to real_main(). Then, add a new main function to your application, named profile_main() such as the one below:\n\ndef profile_main():\n    # This is the main function for profiling \n    # We've renamed our original main() above to real_main()\n    import cProfile, pstats\n    prof = cProfile.Profile()\n    prof = prof.runctx(\"real_main()\", globals(), locals())\n    print \"<pre>\"\n    stats = pstats.Stats(prof)\n    stats.sort_stats(\"time\")  # Or cumulative\n    stats.print_stats(80)  # 80 = how many to print\n    # The rest is optional.\n    # stats.print_callees()\n    # stats.print_callers()\n    print \"</pre>\"\n\n  \n  [...]\n  \n  To enable the profiling with your application, set main = profile_main. To run your application as normal, simply set main = real_main.\n\n"}
{"prompt": "I just got my first Jython (and Python) project, and I was wondering what documentation, IDEs, etc. are best suited to a Java buff like me. \n\nI know there are a lot of questions about starting out with Python, so I'm asking for things that might be specific to Jython. Where should I start? If it helps, I'm running Linux and Solaris only.\n", "completion": "For starters, I'd read python is not java.  It'll give you a good idea of some habits you may have to break to program in Python effectively.  As has been mentioned, pydev is a pretty good development environment.  Although I would say that you would eventually want to learn emacs (but you may just want to learn one thing at a time).\n"}
{"prompt": "How to make sure that code is still working after refactoring ( i.e, after variable name change)?\n\nIn static language, if a class is renamed but other referring class is not, then I will get a compilation error.  \n\nBut in dynamic language there is no such safety net, and your code can break during refactoring if you are not careful enough. You can use unit test,  but when you are using mocks it's pretty hard to know the name changes and as a consequence, it may not help.\n\nHow to solve this problem?\n", "completion": "Before you start refactoring you should create tests that will be able to test what you're going to change - if you say unit tests will not be enought, or they will be hard to create, then by all means create higher level tests possibly even excersising the whole of your product. \n\nIf you have code coverage tools for your language use them to measure the quality of the tests that you've created - after it's reached a reasonably high value and if the tests are kept up to date and extended you'll be able to do anything with your code very efficiently and be rather sure things are not going in the wrong direction.\n"}
{"prompt": "Ex.\n\nIf I have something like this:\n\nclass C(object):\n    @classmethod\n    def f(cls, x):\n       return x + x\n\n\nThis will work:\n\nc = C()\n\nc.f(2)\n4\n\n\nBut is that bad form?\nShould I only call\n\nC.f()\n\n\nor \n\nc.__class__.f()\n\n\nObviously, this would only make sense in cases where f doesn't interact with self/cls expecting it to be class.\n\n?\n", "completion": "If you are tempted to call a class method from an instance you probably don't need a class method.\n\nIn the example you gave a static method would be more appropriate precisely because of your last remark (no self/cls interaction).\n\nclass C(object):\n    @staticmethod\n    def f(x):\n       return x + x\n\n\nthis way it's \"good form\" to do both\n\nc = C()\nc.f(2)\n\n\nand\n\nC.f(2)\n\n"}
{"prompt": "So you've got some legacy code lying around in a fairly hefty project. How can you find and delete dead functions?\n\nI've seen these two references: Find unused code and Tool to find unused functions in php project, but they seem specific to C# and PHP, respectively.\n\nIs there a Python tool that'll help you find functions that aren't referenced anywhere else in the source code (notwithstanding reflection/etc.)?\n", "completion": "In python you can find unused code by using dynamic or static code analyzers. Two examples for dynamic analyzers are coverage and figleaf. They have the drawback that you have to run all possible branches of your code in order to find unused parts, but they also have the advantage that you get very reliable results.\n\nAlternatively, you can use static code analyzers, that just look at your code, but don't actually run it. This has the advantage that they run much faster, but due to python's dynamic nature the results are not 100% percent accurate and you might want to double-check them.\nTwo tools that come to mind here are pyflakes and vulture. They are complementary: Pyflakes finds unused imports and unused local variables while vulture finds unused functions, methods, classes, variables and attributes.\n\nThe tools are all available at the Python Package Index http://pypi.python.org/pypi.\n"}
{"prompt": "Google is sponsoring an Open Source project to increase the speed of Python by 5x.\n\nUnladen-Swallow seems to have a good project plan\n\nWhy is concurrency such a hard problem? \nIs LLVM going to solve the concurrency problem? \nAre there solutions other than Multi-core for Hardware advancement?\n", "completion": "LLVM is several things together - kind of a virtual machine/optimizing compiler, combined with different frontends that take the input in a particular language and output the result in an intermediate language. This intermediate output can be run with the virtual machine, or can be used to generate a standalone executable. \n\nThe problem with concurrency is that, although it was used for a long time in scientific computing, it has just recently has become common in consumer apps. So while it's widely known how to program a scientific calculation program to achieve great performance, it is completely different thing to write a mail user agent/word processor that can be good at concurrency. Also, most of the current OS's were being designed with a single processor in mind, and they may not be fully prepared for multicore processors.\n\nThe benefit of LLVM with respect to concurrency is that you have an intermediate output, and if in the future there are advances in concurrency, then by updating your interpreter you instantly gain those benefits in all LLVM-compiled programs.  This is not so easy if you had compiled to a standalone executable. So LLVM doesn't solve the concurrency problem per se but it leaves an open door for future enhancements.\n\nSure there are more possible advances for the hardware like quantum computers, genetics computers, etc. But we have to wait for them to become a reality.\n"}
{"prompt": "Given a class C in Python, how can I determine which file the class was defined in? I need something that can work from either the class C, or from an instance off C.\n\nThe reason I am doing this, is because I am generally a fan off putting files that belong together in the same folder. I want to create a class that uses a Django template to render itself as HTML. The base implementation should infer the filename for the template based on the filename that the class is defined in.\n\nSay I put a class LocationArtifact in the file \"base/artifacts.py\", then I want the default behaviour to be that the template name is \"base/LocationArtifact.html\".\n", "completion": "You can use the inspect module, like this:\n\nimport inspect\ninspect.getfile(C.__class__)\n\n"}
{"prompt": "I want a program to do one thing if executed like this:\n\ncat something | my_program.py\n\n\nand do another thing if run like this\n\nmy_program.py\n\n\nBut if I read from stdin, then it will wait for user input, so I want to see if there is anything to read before trying to read from stdin.\n", "completion": "If you want to detect if someone is piping data into your program, or running it interactively you can use isatty to see if stdin is a terminal:\n\n$ python -c 'import sys; print sys.stdin.isatty()'\nTrue\n$ echo | python -c 'import sys; print sys.stdin.isatty()'\nFalse\n\n"}
{"prompt": "I know I can use AWK but I am on a Windows box.  I am making a function for others that may not have AWK.  I also know I can write a C program but I would love not have to create maintain and compile something for a little Vim utility I am making.  \n\nThe original file might be\n\nTHE DAY WAS LONG \nTHE WAY WAS FAST\n\n\nand it would become\n\nTT\nHH\nEE\n\nDW\nAA\nYY\n\nWW\nAA\nSS\n\nLF\nOA\nNS\nGT\n\n\nUPDATE:  Golf rules apply to selecting correct answer.\n\nUPDATE:  Python fans should check out Mr. Duffy's answer below.\n", "completion": "Here is a command in Vim language. So you don't have to compile Vim with +python support.\n\nfunction! s:transpose()\n    let maxcol = 0\n    let lines = getline(1, line('$'))\n\n    for line in lines\n        let len = len(line)\n        if len > maxcol \n            let maxcol = len\n        endif\n    endfor\n\n    let newlines = []\n    for col in range(0, maxcol - 1)\n        let newline = ''\n        for line in lines\n            let line_with_extra_spaces = printf('%-'.maxcol.'s', line)\n            let newline .= line_with_extra_spaces[col]\n        endfor\n        call add(newlines, newline)\n    endfor\n\n    1,$\"_d\n    call setline(1, newlines)\nendfunction\n\ncommand! TransposeBuffer call s:transpose()\n\n\nPut this in newly created .vim file inside vim/plugin dir or put this to your [._]vimrc.\nExecute :TransposeBuffer to transpose current buffer \n"}
{"prompt": "If I have the following python code:\n\nclass Foo(object):\n    bar = 1\n    def bah(self):\n\n        print bar\n\nf = Foo()\nf.bah()\n\n\nIt complains \n\nNameError: global name 'bar' is not defined\n\n\nHow can I access class/static variable 'bar' within method 'bah'?\n", "completion": "Instead of bar use self.bar or Foo.bar. Assigning to Foo.bar will create a static variable, and assigning to self.bar will create an instance variable.\n"}
{"prompt": "I'm trying out pylint to check my source code for conventions. Somehow some variable names are matched with the regex for constants (const-rgx) instead of the variable name regex (variable-rgx). How to match the variable name with variable-rgx? Or should I extend const-rgx with my variable-rgx stuff?\n\ne.g.\nC0103: 31: Invalid name \"settings\" (should match (([A-Z_][A-Z1-9_]*)|(__.*__))$)\n", "completion": "\n  Somehow some variable names are matched with the regex for constants (const-rgx) instead of the variable name regex (variable-rgx).\n\n\nAre those variables declared on module level? Maybe that's why they are treated as constants (at least that's how they should be declared, according to PEP-8).\n"}
{"prompt": "I use nosetests to run my unittests and it works well. I want to get a list of all the tests nostests finds without actually running them. Is there a way to do that?\n", "completion": "Version 0.11.1 is currently available.  You can get a list of tests without running them as follows:\n\nnosetests -v --collect-only\n\n"}
{"prompt": "Are there better alternatives to PIL (Python Imaging Library) for basic image file I/O and processing in Python?\n", "completion": "Try Pillow: http://pypi.python.org/pypi/Pillow\n\nIt's a fork of PIL but maintained by Plone community. Which is great as it is being maintained (in comparison to it's predecessor) and it is backward compatible with PIL. Existing code will work out of the box with Pillow. \n\nThey mostly focus on packaging issues and AFAIK this was the biggest pain with PIL.\n\nGood luck!\n"}
{"prompt": "In python I have a dictionary that maps tuples to a list of tuples. e.g. \n\n{(1,2): [(2,3),(1,7)]}\n\nI want to be able to encode this data use it with javascript, so I looked into json but it appears keys must be strings so my tuple does not work as a key.\n\nIs the best way to handle this is encode it as \"1,2\" and then parse it into something I want on the javascript? Or is there a more clever way to handle this.\n", "completion": "You might consider saying\n\n{\"[1,2]\": [(2,3),(1,7)]}\n\n\nand then when you need to get the value out, you can just parse the keys themselves as JSON objects, which all modern browsers can do with the built-in JSON.parse method (I'm using jQuery.each to iterate here but you could use anything):\n\nvar myjson = JSON.parse('{\"[1,2]\": [[2,3],[1,7]]}');\n$.each(myjson, function(keystr,val){\n    var key = JSON.parse(keystr);\n    // do something with key and val\n});\n\n\nOn the other hand, you might want to just structure your object differently, e.g.\n\n{1: {2: [(2,3),(1,7)]}}\n\n\nso that instead of saying\n\nmyjson[1,2] // doesn't work\n\n\nwhich is invalid Javascript syntax, you could say\n\nmyjson[1][2] // returns [[2,3],[1,7]]\n\n"}
{"prompt": "http://pypi.python.org/pypi/simplejson\n\nI am just diving into the Python world and want to make a simple twitter application which requires the installation of simplejson but not sure how I can set it up and get it working..\n\nI am on a Windows System\n", "completion": "I would recommend EasyInstall, a package management application for Python.\n\nOnce you've installed EasyInstall, you should be able to go to a command window and type:\n\neasy_install simplejson\n\n\nThis may require putting easy_install.exe on your PATH first, I don't remember if the EasyInstall setup does this for  you (something like C:\\Python25\\Scripts).\n"}
{"prompt": "I'm using site wide caching with memcached as the backend. I would like to invalidate pages in the cache when the underlying database object changes. \n\nIf the page name changes then I would invalidate the whole cache (as it affects navigation on every page. Clumsy but sufficient for my needs.\n\nIf just the page content changes then I'd like to invalidate the cache of just that page.\n\nIs there an easy way to do this?  \n", "completion": "I haven't done a lot of caching with Django, but I think what you want here are signals.\n\nYou can set up a post_save signal on the underlying object, and have the callback function invalidate that page in the cache.\n\nfrom django.core.signals import post_save\nfrom django.core.cache import cache\n\ndef invalidate_cache(sender, **kwargs):\n    # invalidate cache\n    cache.delete(sender.get_absolute_url()) # or any other pertinent keys\n\npost_save.connect(invalidate_cache, sender=UnderlyingModel)\n\n\nThis should properly remove the item from the cache when it is updated.\n"}
{"prompt": "Often when I am coding I just like to print little things (mostly the current value of variables) out to console. I don't see anything like this for Google App Engine, although I note that the Google App Engine Launcher does have a Log terminal. Is there any way to write to said Terminal, or to some other terminal, using Google App Engine? \n", "completion": "You'll want to use the Python's standard logging module.\n\nimport logging\n\nlogging.info(\"hello\")\nlogging.debug(\"hi\") # this won't show up by default\n\n\nTo see calls to logging.debug() in the GoogleAppEngineLauncher Logs console, you have to first add the flag --dev_appserver_log_level=debug to your app. However, beware that you're going to see a lot of debug noise from the App Engine SDK itself. The full set of levels are:\n\n\ndebug\ninfo\nwarning\nerror\ncritical\n\n\nYou can add the flag by double clicking the app and then dropping it into the Extra Flags field.\n\n\n"}
{"prompt": "I have a django application using mod_python, fairly typical configuration except that media files are being served by a (I know, not recommended) 'media' directory in the document root.  I would like to test and maybe deploy with mod_wsgi but I cannot figure out how to create something simple to serve static files.  mod_python allows the use of Apache directives like:\n\n<Location '/'>\n    SetHandler MyApplication.xyz.....\n</Location>\n\n<Location '/media'>\n    SetHandler None\n</Location>\n\n\nThe django docs seem to point to the second block above as the correct way to make a similar exception for mod_wsgi, but in my tests everything below root is still being sent to the wsgi app.  Is there a good way set a static media directory with mod_wsgi, or is what I am trying to do intentionally unsupported for compelling technical reasons?  Answers that point to entirely different approaches are welcome.\n", "completion": "I run a a dozen or so Django sites on the same server and here's how I configure the media URL's.\n\nEach VirtualHost has the following configuration:\n\nAlias /media /path/to/media/\n<Directory /path/to/media>\n    Include /etc/apache2/vhosts.d/media.include\n</Directory>\n\n\nThis way I can make any changes to the media handling in one file.\n\nThen, my media.include file looks like this:\n\nOrder allow,deny\nAllow from all\nSetHandler None\nFileETag none\nOptions FollowSymLinks\n\n<IfModule mod_expires.c>\n    ExpiresActive On\n    ExpiresByType image/gif \"access plus 30 days\"\n    ExpiresByType image/jpg \"access plus 30 days\"\n    ExpiresByType image/png \"access plus 30 days\"\n    ExpiresByType image/jpeg \"access plus 30 days\"\n    ExpiresByType text/css \"access plus 30 days\"\n    ExpiresByType application/x-javascript \"modification plus 2 years\"\n</IfModule>\n\n<IfModule mod_headers.c>\n    Header append Vary Accept-Encoding\n</IfModule>\n\nAddOutputFilterByType DEFLATE text/html text/css text/plain\n\n\nThis has worked very well for me, and gets an A grade from YSlow (also see Jeff Atwood on YSlow).\n\nAlso note, for the root dir I use the following configuration:\n\nWSGIScriptAlias / /path/to/app.wsgi\n<Directory /path/to>\n    Options +ExecCGI\n    Allow from all\n</Directory>\n\n\n... which should be after the Alias /media in your configuration file (because Apache looks at the aliases in order)\n"}
{"prompt": "I'm using Django 1.0.2.  I've written a ModelForm backed by a Model.  This model has a ForeignKey where blank=False.  When Django generates HTML for this form it creates a select box with one option for each row in the table referenced by the ForeignKey.  It also creates an option at the top of the list that has no value and displays as a series of dashes:\n\n<option value=\"\">---------</option>\n\n\nWhat I'd like to know is:\n\n\nWhat is the cleanest way to remove this auto-generated option from the select box?  \nWhat is the cleanest way to customize it so that it shows as:\n\n<option value=\"\">Select Item</option>\n\n\n\nIn searching for a solution I came across Django ticket 4653 which gave me the impression that others had the same question and that the default behavior of Django may have been modified.  This ticket is over a year old so I was hoping there might be a cleaner way to accomplish these things.\n\nThanks for any help,\n\nJeff\n\nEdit: I've configured the ForeignKey field as such: \n\nverb = models.ForeignKey(Verb, blank=False, default=get_default_verb)\n\n\nThis does set the default so that it's no longer the empty/dashes option but unfortunately it doesn't seem to resolve either of my questions.  That is, the empty/dashes option still appears in the list.\n", "completion": "Haven't tested this, but based on reading Django's code here and here I believe it should work:\n\nclass ThingForm(models.ModelForm):\n  class Meta:\n    model = Thing\n\n  def __init__(self, *args, **kwargs):\n    super(ThingForm, self).__init__(*args, **kwargs)\n    self.fields['verb'].empty_label = None\n\n\nEDIT: This is documented, though you wouldn't necessarily know to look for ModelChoiceField if you're working with an auto-generated ModelForm.\n\nEDIT: As jlpp notes in his answer, this isn't complete - you have to re-assign the choices to the widgets after changing the empty_label attribute.  Since that's a bit hacky, the other option that might be easier to understand is just overriding the entire ModelChoiceField:\n\nclass ThingForm(models.ModelForm):\n  verb = ModelChoiceField(Verb.objects.all(), empty_label=None)\n\n  class Meta:\n    model = Thing\n\n"}
{"prompt": "Does anyone have any good information aside from the Google App Engine docs provided by Google that gives a good overview for people with MS SQL background to porting their knowledge and using Google App Engine Data Store API effectively.\n\nFor Example, if you have a self created Users Table and a Message Table\n\nWhere there is a relationship between Users and Message (connected by the UserID), how would this structure be represented in Google App Engine?\n\nSELECT * FROM Users INNER JOIN Message ON Users.ID = Message.UserID\n\n", "completion": "Here is a good link: One to Many Join using Google App Engine.\n\nhttp://blog.arbingersys.com/2008/04/google-app-engine-one-to-many-join.html\n\nHere is another good link: Many to Many Join using Google App Engine:\n\nhttp://blog.arbingersys.com/2008/04/google-app-engine-many-to-many-join.html\n\nHere is a good discussion regarding the above two links:\n\nhttp://groups.google.com/group/google-appengine/browse_thread/thread/e9464ceb131c726f/6aeae1e390038592?pli=1\n\nPersonally I find this comment in the discussion very informative about the Google App Engine Data Store:\n\nhttp://groups.google.com/group/google-appengine/msg/ee3bd373bd31e2c7\n\n\n  At scale you wind up doing a bunch of\n  things that seem wrong, but that are\n  required by the numbers we are\n  running. Go watch the EBay talks. Or\n  read the posts about how many database\n  instances FaceBook is running.\n  \n  The simple truth is, what we learned\n  about in uni was great for the\n  business automation apps of small to\n  medium enterprise applications, where\n  the load was predictable, and there\n  was money enough to buy the server\n  required to handle the load of 50\n  people doing data entry into an\n  accounts or business planning and\n  control app....\n\n\nSearched around a bit more and came across this Google Doc Article:\n\nhttp://code.google.com/appengine/articles/modeling.html\n\n\n  App Engine allows the creation of easy\n  to use relationships between datastore\n  entities which can represent\n  real-world things and ideas. Use\n  ReferenceProperty when you need to\n  associate an arbitrary number of\n  repeated types of information with a\n  single entity. Use key-lists when you\n  need to allow lots of different\n  objects to share other instances\n  between each other. You will find that\n  these two approaches will provide you\n  with most of what you need to create\n  the model behind great applications.\n\n"}
{"prompt": "I'm trying to use SQLAlchemy to implement a basic users-groups model where users can have multiple groups and groups can have multiple users.\n\nWhen a group becomes empty, I want the group to be deleted, (along with other things associated with the group.  Fortunately, SQLAlchemy's cascade works fine with these more simple situations).\n\nThe problem is that cascade='all, delete-orphan' doesn't do exactly what I want; instead of deleting the group when the group becomes empty, it deletes the group when any member leaves the group.\n\nAdding triggers to the database works fine for deleting a group when it becomes empty, except that triggers seem to bypass SQLAlchemy's cascade processing so things associated with the group don't get deleted.\n\nWhat is the best way to delete a group when all of its members leave and have this deletion cascade to related entities.\n\nI understand that I could do this manually by finding every place in my code where a user can leave a group and then doing the same thing as the trigger however, I'm afraid that I would miss places in the code (and I'm lazy).\n", "completion": "The way I've generally handled this is to have a function on your user or group called leave_group.  When you want a user to leave a group, you call that function, and you can add any side effects you want into there.  In the long term, this makes it easier to add more and more side effects.  (For example when you want to check that someone is allowed to leave a group).\n"}
{"prompt": "Is there a way to write a string directly to a tarfile? From http://docs.python.org/library/tarfile.html it looks like only files already written to the file system can be added.\n", "completion": "I would say it's possible, by playing with TarInfo e TarFile.addfile passing a StringIO as a fileobject. \n\nVery rough, but works\n\nimport tarfile\nimport StringIO\n\ntar = tarfile.TarFile(\"test.tar\",\"w\")\n\nstring = StringIO.StringIO()\nstring.write(\"hello\")\nstring.seek(0)\ninfo = tarfile.TarInfo(name=\"foo\")\ninfo.size=len(string.buf)\ntar.addfile(tarinfo=info, fileobj=string)\n\ntar.close()\n\n"}
{"prompt": "I  was creating a simple command line utility and using a dictionary as a sort of case statement with key words linking to their apropriate function.  The functions all have different amount of arguments required so currently to check if the user entered the correct amount of arguments needed for each function I placed the required amount inside the dictionary case statement in the form {Keyword:(FunctionName, AmountofArguments)}.\n\nThis current setup works perfectly fine however I was just wondering in the interest of self improval if there was a way to determine the required number of arguments in a function and my google attempts have returned so far nothing of value but I see how args and kwargs could screw such a command up because of the limitless amount of arguments they allow.\n\nThanks for any help.\n", "completion": "inspect.getargspec():\n\n\n  Get the names and default values of a function\u00e2\u0080\u0099s arguments. A tuple of four things is returned: (args, varargs, varkw, defaults). args is a list of the argument names (it may contain nested lists). varargs and varkw are the names of the * and ** arguments or None. defaults is a tuple of default argument values or None if there are no default arguments; if this tuple has n elements, they correspond to the last n elements listed in args.\n\n"}
{"prompt": "I'm interested in testing the performance of my django apps as I go, what is the best way to get line by line performance data?\n\nnote: Googling this returns lots of people benchmarking django itself. I'm not looking for a benchmarks of django, I'm trying to test the performance of the django apps that I'm writing :)\n\nThanks!\n\nedit: By \"line by line\" I just mean timing individual functions, db calls, etc to find out where the bottlenecks are on a very granular level\n", "completion": "There's two layers to this.  We have most of #1 in place for our testing.  We're about to start on #2.\n\n\nDjango in isolation.  The ordinary Django unit tests works well here.  Create some tests that cycle through a few (less than 6) \"typical\" use cases.  Get this, post that, etc.  Collect timing data.  This isn't real web performance, but it's an easy-to-work with test scenario that you can use for tuning.\nYour whole web stack.  In this case, you need a regular server running Squid, Apache, Django, MySQL, whatever.  You need a second computer(s) to act a client exercise your web site through urllib2, doing a few (less than 6) \"typical\" use cases.  Get this, post that, etc.  Collect timing data.  This still isn't \"real\" web performance, because it isn't through the internet, but it's as close as you're going to get without a really elaborate setup.\n\n\nNote that the #2 (end-to-end) includes a great deal of caching for performance.   If your client scripts are doing similar work, caching will be really beneficial.  if your client scripts do unique things each time, caching will be less beneficial.\n\nThe hardest part is determining what the \"typical\" workload is.  This isn't functional testing, so the workload doesn't have to include everything.  Also, the more concurrent sessions your client is running, the slower it becomes.  Don't struggle trying to optimize your server when your test client is the slowest part of the processing.\n\n\n\nEdit\n\nIf \"line-by-line\" means \"profiling\", well, you've got to get a Python profiler running.\n\nhttps://docs.python.org/library/profile.html\n\nNote that there's plenty of caching in the Django ORM layer.  So running a view function a  half-dozen times to get a meaningful set of measurements isn't sensible.  You have to run a \"typical\" set of operations and then find hot-spots in the profile.  \n\nGenerally, your application is easy to optimize -- you shouldn't be doing much.  Your view functions should be short and have no processing to speak of.  Your form and model method functions, similarly, should be very short.\n"}
{"prompt": "In Python, the assignment operator can unpack list or tuple into variables, like this:\n\nl = (1, 2)\na, b = l # Here goes auto unpack\n\n\nBut I need to specify exactly the same amount of names to the left as an items count in the list to the right. But sometimes I don't know a size of the list to the right, for example if I use split(). Example:\n\na, b = \"length=25\".split(\"=\") # This will result in a=\"length\" and b=25\n\n\nBut the following code will lead to an error:\n\na, b = \"DEFAULT_LENGTH\".split(\"=\") # Error, list has only one item\n\n\nIs it possible to somehow unpack list in the example above so I get a = \"DEFAULT_LENGTH\" and b equals to 'None' or not set? A straightforward way looks kind of long:\n\na = b = None\nif \"=\" in string :\n  a, b = string.split(\"=\")\nelse :\n  a = string\n\n", "completion": "This may be of no use to you unless you're using Python 3. However, for completeness, it's worth noting that the extended tuple unpacking introduced there allows you to do things like:\n\n>>> a, *b = \"length=25\".split(\"=\")\n>>> a,b\n(\"length\", ['25'])\n>>> a, *b = \"DEFAULT_LENGTH\".split(\"=\")\n>>> a,b\n(\"DEFAULT_LENGTH\", [])\n\n\nI.e. tuple unpacking now works similarly to how it does in argument unpacking, so you can denote \"the rest of the items\" with *, and get them as a (possibly empty) list.\n\nPartition is probably the best solution for what you're doing however.\n"}
{"prompt": "I have a series of images that I want to create a video from.  Ideally I could specify a frame duration for each frame but a fixed frame rate would be fine too.  I'm doing this in wxPython, so I can render to a wxDC or I can save the images to files, like PNG.  Is there a Python library that will allow me to create either a video (AVI, MPG, etc) or an animated GIF from these frames?\n\nEdit: I've already tried PIL and it doesn't seem to work.  Can someone correct me with this conclusion or suggest another toolkit?  This link seems to backup my conclusion regarding PIL: http://www.somethinkodd.com/oddthinking/2005/12/06/python-imaging-library-pil-and-animated-gifs/\n", "completion": "As of June 2009 the originally cited blog post has a method to create animated GIFs in the comments.  Download the script images2gif.py (formerly images2gif.py, update courtesy of @geographika).\n\nThen, to reverse the frames in a gif, for instance:\n\n#!/usr/bin/env python\n\nfrom PIL import Image, ImageSequence\nimport sys, os\nfilename = sys.argv[1]\nim = Image.open(filename)\noriginal_duration = im.info['duration']\nframes = [frame.copy() for frame in ImageSequence.Iterator(im)]    \nframes.reverse()\n\nfrom images2gif import writeGif\nwriteGif(\"reverse_\" + os.path.basename(filename), frames, duration=original_duration/1000.0, dither=0)\n\n"}
{"prompt": "I was reading 'Dive Into Python' and in the chapter on classes it gives this example:\n\nclass FileInfo(UserDict):\n    \"store file metadata\"\n    def __init__(self, filename=None):\n        UserDict.__init__(self)\n        self[\"name\"] = filename\n\n\nThe author then says that if you want to override the __init__ method, you must explicitly call the parent __init__ with the correct parameters.  \n\n\nWhat if that FileInfo class had more than one ancestor class?\n\nDo I have to explicitly call all of the ancestor classes' __init__ methods?  \n\nAlso, do I have to do this to any other method I want to override?\n\n", "completion": "The book is a bit dated with respect to subclass-superclass calling.  It's also a little dated with respect to subclass built-in classes.\n\nIt looks like this nowadays.\n\nclass FileInfo(dict):\n    \"\"\"store file metadata\"\"\"\n    def __init__(self, filename=None):\n        super( FileInfo, self ).__init__()\n        self[\"name\"] = filename\n\n\nNote the following.\n\n\nWe can directly subclass built-in classes, like dict, list, tuple, etc.\nThe super function handles tracking down this class's superclasses and calling functions in them appropriately.\n\n"}
{"prompt": "I'm unclear on why the sub-interpreter API exists and why it's used in modules such as the mod_wsgi apache module.  Is it mainly used for creating a security sandbox for different applications running within the same process, or is it a way to allow concurrency with multiple threads?  Maybe both? Are there other purposes?\n", "completion": "I imagine the purpose is to create separate python execution environments. For instance, mod_wsgi (Apache Python module) hosts a single python interpreter and then hosts multiple applications within sub-interpreters (in the default configuration).\n\nSome key points from the documentation:\n\n\nThis is an (almost) totally separate environment for the execution of Python code. In particular, the new interpreter has separate, independent versions of all imported modules, including the fundamental modules __builtin__, __main__ and sys.\nThe table of loaded modules (sys.modules) and the module search path (sys.path) are also separate.\nBecause sub-interpreters (and the main interpreter) are part of the same process, the insulation between them isn\u00e2\u0080\u0099t perfect \u00e2\u0080\u0094 for example, using low-level file operations like os.close() they can (accidentally or maliciously) affect each other\u00e2\u0080\u0099s open files. \nBecause of the way extensions are shared between (sub-)interpreters, some extensions may not work properly; this is especially likely when the extension makes use of (static) global variables, or when the extension manipulates its module\u00e2\u0080\u0099s dictionary after its initialization.\n\n"}
{"prompt": "For a block like this:\n\ntry:\n    #some stuff\nexcept Exception:\n    pass\n\n\npylint raises warning W0703 'Catch \"Exception\"'. Why?\n", "completion": "It's considered good practice to not normally catch the root Exception object, but instead to catch more specific ones - for example IOException.\n\nConsider if an out of memory exception occurred - simply using \"pass\" isn't going to leave your programme in a good state.\n\nPretty much the only time you should catch Exception is at the top level of your programme, where you can (try to) log it, display an error, and exit as gracefully as you can.\n"}
{"prompt": "What is the nicest way of splitting this:\n\ntuple = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')\n\n\ninto this:\n\ntuples = [('a', 'b'), ('c', 'd'), ('e', 'f'), ('g', 'h')]\n\n\nAssuming that the input always has an even number of values.\n", "completion": "zip() is your friend:\n\nt = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')\nzip(t[::2], t[1::2])\n\n"}
{"prompt": "I'm keeping track of a user status on a model. For the model 'Lesson' I have the status 'Finished', 'Learning', 'Viewed'. In a view for a list of models I want to add the user status. What is the best way to do this?\n\nOne idea: Adding the request to a models method would do the trick. Is that possible?\n\nEdit: I meant in templatecode: {{ lesson.get_status }}, with get_status(self, request). Is it possible? It does not work (yet).\n", "completion": "If your status is a value that changes, you have to break this into two separate parts.\n\n\nUpdating the status.  This must be called in a view function.  The real work, however, belongs in the model.  The view function calls the model method and does the save.\nDisplaying the status.  This is just some string representation of the status.\n\n\nModel\n\nclass MyStatefulModel( models.Model ):\n    theState = models.CharField( max_length=64 )\n    def changeState( self ):\n        if theState is None:\n            theState= \"viewed\"\n        elif theState is \"viewed\":\n            theState= \"learning\"\n        etc.\n\n\nView Function\n\n def show( request, object_id ):\n     object= MyStatefulModel.objects.get( id=object_id )\n     object.changeState()\n     object.save()\n     render_to_response( ... )\n\n\nTemplate\n\n <p>Your status is {{object.theState}}.</p>\n\n"}
{"prompt": "I started to use the csv.reader in Python 2.6 but you can't use len on it, or slice it, etc. What's the reason behind this? It certainly feels very limiting.\n\nOr is this just an abandoned module in later versions?\n", "completion": "I'm pretty sure you can't use len or slice because it is an iterator. Try this instead.\n\nimport csv\nr = csv.reader(...)\nlines = [line for line in r]\nprint len(lines) #number of lines\nfor odd in lines[1::2]: print odd # print odd lines\n\n"}
{"prompt": "Is there a far shorter way to write the following code?\n\nmy_string = my_string.replace('A', '1')\nmy_string = my_string.replace('B', '2')\nmy_string = my_string.replace('C', '3')\nmy_string = my_string.replace('D', '4')\nmy_string = my_string.replace('E', '5')\n\n\nNote that I don't need those exact values replaced; I'm simply looking for a way to turn 5+ lines into fewer than 5\n", "completion": "Looks like a good opportunity to use a loop:\n\nmapping = { 'A':'1', 'B':'2', 'C':'3', 'D':'4', 'E':'5'}\nfor k, v in mapping.iteritems():\n    my_string = my_string.replace(k, v)\n\n\nA faster approach if you don't mind the parentheses would be:\n\nmapping = [ ('A', '1'), ('B', '2'), ('C', '3'), ('D', '4'), ('E', '5') ]\nfor k, v in mapping:\n    my_string = my_string.replace(k, v)\n\n"}
{"prompt": "Why is :memory: in sqlite so slow?\n\nI've been trying to see if there are any performance improvements gained by using in-memory sqlite vs. disk based sqlite. Basically I'd like to trade startup time and memory to get extremely rapid queries which do not hit disk during the course of the application. \n\nHowever, the following benchmark gives me only a factor of 1.5X in improved speed. Here, I'm generating 1M rows of random data and loading it into both a disk and memory based version of the same table. I then run random queries on both dbs, returning sets of size approx 300k. I expected the memory based version to be considerably faster, but as mentioned I'm only getting 1.5X speedups. \n\nI experimented with several other sizes of dbs and query sets; the advantage of :memory: does seem to go up as the number of rows in the db increases. I'm not sure why the advantage is so small, though I had a few hypotheses: \n\n\nthe table used isn't big enough (in rows) to make :memory: a huge winner\nmore joins/tables would make the :memory: advantage more apparent\nthere is some kind of caching going on at the connection or OS level such that the previous results are accessible somehow, corrupting the benchmark\nthere is some kind of hidden disk access going on that I'm not seeing (I haven't tried lsof yet, but I did turn off the PRAGMAs for journaling)\n\n\nAm I doing something wrong here? Any thoughts on why :memory: isn't producing nearly instant lookups? Here's the benchmark: \n\n==> sqlite_memory_vs_disk_benchmark.py <==\n\n#!/usr/bin/env python\n\"\"\"Attempt to see whether :memory: offers significant performance benefits.\n\n\"\"\"\nimport os\nimport time\nimport sqlite3\nimport numpy as np\n\ndef load_mat(conn,mat):\n    c = conn.cursor()\n\n    #Try to avoid hitting disk, trading safety for speed.\n    #http://stackoverflow.com/questions/304393\n    c.execute('PRAGMA temp_store=MEMORY;')\n    c.execute('PRAGMA journal_mode=MEMORY;')\n\n    # Make a demo table\n    c.execute('create table if not exists demo (id1 int, id2 int, val real);')\n    c.execute('create index id1_index on demo (id1);')\n    c.execute('create index id2_index on demo (id2);')\n    for row in mat:\n        c.execute('insert into demo values(?,?,?);', (row[0],row[1],row[2]))\n    conn.commit()\n\ndef querytime(conn,query):\n    start = time.time()\n    foo = conn.execute(query).fetchall()\n    diff = time.time() - start\n    return diff\n\n#1) Build some fake data with 3 columns: int, int, float\nnn   = 1000000 #numrows\ncmax = 700    #num uniques in 1st col\ngmax = 5000   #num uniques in 2nd col\n\nmat = np.zeros((nn,3),dtype='object')\nmat[:,0] = np.random.randint(0,cmax,nn)\nmat[:,1] = np.random.randint(0,gmax,nn)\nmat[:,2] = np.random.uniform(0,1,nn)\n\n#2) Load it into both dbs & build indices\ntry: os.unlink('foo.sqlite')\nexcept OSError: pass\n\nconn_mem = sqlite3.connect(\":memory:\")\nconn_disk = sqlite3.connect('foo.sqlite')\nload_mat(conn_mem,mat)\nload_mat(conn_disk,mat)\ndel mat\n\n#3) Execute a series of random queries and see how long it takes each of these\nnumqs = 10\nnumqrows = 300000 #max number of ids of each kind\nresults = np.zeros((numqs,3))\nfor qq in range(numqs):\n    qsize = np.random.randint(1,numqrows,1)\n    id1a = np.sort(np.random.permutation(np.arange(cmax))[0:qsize]) #ensure uniqueness of ids queried\n    id2a = np.sort(np.random.permutation(np.arange(gmax))[0:qsize])\n    id1s = ','.join([str(xx) for xx in id1a])\n    id2s = ','.join([str(xx) for xx in id2a])\n    query = 'select * from demo where id1 in (%s) AND id2 in (%s);' % (id1s,id2s)\n\n    results[qq,0] = round(querytime(conn_disk,query),4)\n    results[qq,1] = round(querytime(conn_mem,query),4)\n    results[qq,2] = int(qsize)\n\n#4) Now look at the results\nprint \"  disk | memory | qsize\"\nprint \"-----------------------\"\nfor row in results:\n    print \"%.4f | %.4f | %d\" % (row[0],row[1],row[2])\n\n\nHere's the results. Note that disk takes about 1.5X as long as memory for a fairly wide range of query sizes. \n\n[ramanujan:~]$python -OO sqlite_memory_vs_disk_clean.py\n  disk | memory | qsize\n-----------------------\n9.0332 | 6.8100 | 12630\n9.0905 | 6.6953 | 5894\n9.0078 | 6.8384 | 17798\n9.1179 | 6.7673 | 60850\n9.0629 | 6.8355 | 94854\n8.9688 | 6.8093 | 17940\n9.0785 | 6.6993 | 58003\n9.0309 | 6.8257 | 85663\n9.1423 | 6.7411 | 66047\n9.1814 | 6.9794 | 11345\n\n\nShouldn't RAM be almost instant relative to disk? What's going wrong here? \n\nEdit\n\nSome good suggestions here. \n\nI guess the main takehome point for me is that **there's probably no way to make :memory: absolutely faster, but there is a way to make disk access relatively slower. ** \n\nIn other words, the benchmark is adequately measuring the realistic performance of memory, but not the realistic performance of disk (e.g. because the cache_size pragma is too big or because I'm not doing writes). I'll mess around with those parameters and post my findings when I get a chance.  \n\nThat said, if there is anyone who thinks I can squeeze some more speed out of the in-memory db (other than by jacking up the cache_size and default_cache_size, which I will do), I'm all ears...\n", "completion": "It has to do with the fact that SQLite has a page cache. According to the Documentation, the default page cache is 2000 1K pages or about 2Mb. Since this is about 75% to 90% of your data, it isn't surprising that the two number are very similar. My guess is that in addition to the SQLite page cache, the rest of the data is still in the OS disk cache. If you got SQLite to flush the page cache (and the disk cache) you would see some really significant differences.\n"}
{"prompt": "Can anyone suggest any good payment processing libraries for python/django?\n", "completion": "The most developed Django solution is Satchmo with support for Authorize.Net, TrustCommerce, CyberSource, PayPal, Google Checkout, and Protx.\n\nThe new kid on the Django block is django-lfs which looks like only support for PayPal at the moment, and even that may not be complete.\n\nFor general Python the main player is getpaid\n"}
{"prompt": "Surely a 0d array is scalar, but Numpy does not seem to think so... am I missing something or am I just misunderstanding the concept? \n\n>>> foo = numpy.array(1.11111111111, numpy.float64)\n>>> numpy.ndim(foo)\n0\n>>> numpy.isscalar(foo)\nFalse\n>>> foo.item()\n1.11111111111\n\n", "completion": "One should not think too hard about it. It's ultimately better for the mental health and longevity of the individual.\n\nThe curious situation with Numpy scalar-types was bore out of the fact that there is no graceful and consistent way to degrade the 1x1 matrix to scalar types. Even though mathematically they are the same thing, they are handled by very different code.\n\nIf you've been doing any amount of scientific code, ultimately you'd want things like max(a) to work on matrices of all sizes, even scalars. Mathematically, this is a perfectly sensible thing to expect. However for programmers this means that whatever presents scalars in Numpy should have the .shape and .ndim attirbute, so at least the ufuncs don't have to do explicit type checking on its input for the 21 possible scalar types in Numpy. \n\nOn the other hand, they should also work with existing Python libraries that does do explicit type-checks on scalar type. This is a dilemma, since a Numpy ndarray have to individually change its type when they've been reduced to a scalar, and there is no way of knowing whether that has occurred without it having do checks on all access. Actually going that route would probably make bit ridiculously slow to work with by scalar type standards.\n\nThe Numpy developer's solution is to inherit from both ndarray and Python scalars for its own scalary type, so that all scalars also have .shape, .ndim, .T, etc etc. The 1x1 matrix will still be there, but its use will be discouraged if you know you'll be dealing with a scalar. While this should work fine in theory, occasionally you could still see some places where they missed with the paint roller, and the ugly innards are exposed for all to see:\n\n>>> from numpy import *\n>>> a = array(1)\n>>> b = int_(1)\n>>> a.ndim\n0\n>>> b.ndim\n0\n>>> a[...]\narray(1)\n>>> a[()]\n1\n>>> b[...]\narray(1)\n>>> b[()]\n1\n\n\nThere's really no reason why a[...] and a[()] should return different things, but it does. There are proposals in place to change this, but looks like they forgot to finish the job for 1x1 arrays.\n\nA potentially bigger, and possibly non-resolvable issue, is the fact that Numpy scalars are immutable. Therefore \"spraying\" a scalar into a ndarray, mathematically the adjoint operation of collapsing an array into a scalar, is a PITA to implement. You can't actually grow a Numpy scalar, it cannot by definition be cast into an ndarray, even though newaxis mysteriously works on it:\n\n>>> b[0,1,2,3] = 1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'numpy.int32' object does not support item assignment\n>>> b[newaxis]\narray([1])\n\n\nIn Matlab, growing the size of a scalar is a perfectly acceptable and brainless operation. In Numpy you have to stick jarring a = array(a) everywhere you think you'd have the possibility of starting with a scalar and ending up with an array. I understand why Numpy has to be this way to play nice with Python, but that doesn't change the fact that many new switchers are deeply confused about this. Some have explicit memory of struggling with this behaviour and eventually persevering, while others who are too far gone are generally left with some deep shapeless mental scar that frequently haunts their most innocent dreams. It's an ugly situation for all.\n"}
{"prompt": "I'm currently writing a telnet server in Python. It's a content server. People would connect to the server via telnet, and be presented with text-only content.\n\nMy problem is that the server would obviously need to support more than one simultaneous connection. The current implementation I have now supports only one. \n\nThis is the basic, proof-of-concept server I began with (while the program has changed greatly over time, the basic telnet framework hasn't):\n\nimport socket, os\n\nclass Server:\n    def __init__(self):\n    \tself.host, self.port = 'localhost', 50000\n    \tself.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \tself.socket.bind((self.host, self.port))\n\n    def send(self, msg):\n    \tif type(msg) == str: self.conn.send(msg + end)\n    \telif type(msg) == list or tuple: self.conn.send('\\n'.join(msg) + end)\n\n    def recv(self):\n    \tself.conn.recv(4096).strip()\n\n    def exit(self):\n    \tself.send('Disconnecting you...'); self.conn.close(); self.run()\n    \t# closing a connection, opening a new one\n\n    # main runtime\n    def run(self):\n    \tself.socket.listen(1)\n    \tself.conn, self.addr = self.socket.accept()\n    \t# there would be more activity here\n    \t# i.e.: sending things to the connection we just made\n\n\nS = Server()\nS.run()\n\n\nThanks for your help. \n", "completion": "Implemented in twisted:\n\nfrom twisted.internet.protocol import Factory, Protocol\nfrom twisted.internet import reactor\n\nclass SendContent(Protocol):\n    def connectionMade(self):\n        self.transport.write(self.factory.text)\n        self.transport.loseConnection()\n\nclass SendContentFactory(Factory):\n    protocol = SendContent\n    def __init__(self, text=None):\n        if text is None:\n            text = \"\"\"Hello, how are you my friend? Feeling fine? Good!\"\"\"\n        self.text = text\n\nreactor.listenTCP(50000, SendContentFactory())\nreactor.run()\n\n\nTesting:\n\n$ telnet localhost 50000\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nHello, how are you my friend? Feeling fine? Good!\nConnection closed by foreign host.\n\n\nSeriously, when it comes to asynchronous network, twisted is the way to go. It handles multiple connections in a single-thread single-process approach.\n"}
{"prompt": "What's the magic \"python setup.py some_incantation_here\" command to upload a package to PyPI, in a form that can be downloaded to get the original package in its original form?\n\nI have a package with some source and a few image files (as package_data).  If I do \"setup.py sdist register upload\", the .tar.gz has the image files excluded.  If I do \"setup.py bdist_egg register upload\", the egg contains the images but excludes the setup.py file.  I want to be able to get a file uploaded that is just the entirety of my project -- aka \"setup.py the_whole_freaking_thing register upload\".\n\nPerhaps the best way to do this is to manually tar.gz my project directory and upload it using the PyPI web interface?\n\nCaveat: I'm trying to avoid having to store a simple project I just created in my SVN repo as well as on PyPI -- it seems like a waste of work to keep track of its history and files in two places.\n", "completion": "When you perform an \"sdist\" command, then what controls the list of included files is your \"MANIFEST.in\" file sitting next to \"setup.py\", not whatever you have listed in \"package_data\".  This has something to do with the schizophrenic nature of the Python packaging solutions today; \"sdist\" is powered by the distutils in the standard library, while \"bdist_egg\" is controlled by the setuptools module.\n\nTo solve the problem, try creating a MANIFEST.in next to your setup.py file, and give it contents like this:\n\ninclude *.jpg\n\n\nOf course, I'm imaging that your \"image files\" are actual pictures rather than disk images or ISO images or something; you might have to adjust the above line if I've guessed wrong!  But check out the Specifying which files to distribute section of the distutils docs, and see whether you can't get those files appearing in your .tar.gz source distribution!  Good luck.\n"}
{"prompt": "How do I write the magic function below?\n\n>>> num = 123\n>>> lst = magic(num)\n>>>\n>>> print lst, type(lst)\n[1, 2, 3], <type 'list'>\n\n", "completion": "You mean this?\n\nnum = 1234\nlst = [int(i) for i in str(num)]\n\n"}
{"prompt": "The sqlite3 module is included in Python version 2.5+. However, I am stuck with version 2.4. I uploaded the sqlite3 module files, added the directory to sys.path, but I get the following error when I try to import it:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in ?\n  File \"sqlite3/__init__.py\", line 23, in ?\n    from dbapi2 import *\n  File \"sqlite3/dbapi2.py\", line 26, in ?\n    from _sqlite3 import *\nImportError: No module named _sqlite3\n\n\nThe file '_sqlite3' is in lib-dynload, but if I include this in the sqlite3 directory, I get additional errors.\n\nAny suggestions? I am working in a limited environment; I don't have access to GCC, among other things.\n", "completion": "I had same problem with CentOS and python 2.4\n\nMy solution:\n\nyum install python-sqlite2\n\n\nand try following python code\n\ntry:\n    import sqlite3\nexcept:\n    from pysqlite2 import dbapi2 as sqlite3\n\n"}
{"prompt": "I'd like to produce sounds that would resemble audio from real instruments. The problem is that I have very little clue how to get that.\n\nWhat I know this far from real instruments is that sounds they output are rarely clean. But how to produce such unclean sounds?\n\nThis far I've gotten to do this, it produces quite plain sound from which I'm not sure it's even using the alsa correctly.\n\nimport numpy\nfrom numpy.fft import fft, ifft\nfrom numpy.random import random_sample\nfrom alsaaudio import PCM, PCM_NONBLOCK, PCM_FORMAT_FLOAT_LE\n\npcm = PCM()#mode=PCM_NONBLOCK)\npcm.setrate(44100)\npcm.setformat(PCM_FORMAT_FLOAT_LE)\npcm.setchannels(1)\npcm.setperiodsize(4096)\n\ndef sine_wave(x, freq=100):\n    sample = numpy.arange(x*4096, (x+1)*4096, dtype=numpy.float32)\n    sample *= numpy.pi * 2 / 44100\n    sample *= freq\n    return numpy.sin(sample)\n\nfor x in xrange(1000):\n    sample = sine_wave(x, 100)\n    pcm.write(sample.tostring())\n\n", "completion": "Sound synthesis is a complex topic which requires many years of study to master. \n\nIt is also not an entirely solved problem, although relatively recent developments (such as physical modelling synthesis) have made progress in imitating real-world instruments.\n\nThere are a number of options open to you. If you are sure that you want to explore synthesis further, then I suggest you start by learning about FM synthesis. It is relatively easy to learn and implement in software, at least in basic forms, and produces a wide range of interesting sounds. Also, check out the book \"The Computer Music Tutorial\" by Curtis Roads. It's a bible for all things computer music, and although it's a few years old it is the book of choice for learning the fundamentals.\n\nIf you want a quicker way to produce life-like sound, consider using sampling techniques: that is, record the instruments you want to reproduce (or use a pre-existing sample bank), and just play back the samples. It's a much more straightforward (and often more effective) approach. \n"}
{"prompt": "I am wondering if there are any django based, or even Python Based Reporting Services ala JasperReports or SQL Server Reporting Services?\n\nBasically, I would love to be able to create reports, send them out as emails as CSV or HTML or PDF without having to code the reports. Even if I have to code the report I wouldn't mind, but the whole framework with schedules and so on would be nice!\n\nPS. I know I could use Django Apps to do it, but I was hoping if there was any integrated solutions or even projects such as Pinax or Satchmo which brings together the apps needed.\n\nPPS: It would have to work off Postgres\n\nThanks and Regards\n\nMark\n", "completion": "\"I would love to be able to create reports ... without having to code the reports\"  \n\nSo would I.  Sadly, however, each report seems to be unique and require custom code.\n\nFrom Django model to CSV is easy.  Start there with a few of your reports.\n\nimport csv\nfrom myApp.models import This, That, TheOther\ndef parseCommandLine():\n    # setup optparse to get report query parameters\ndef main():\n    wtr= csv.DictWriter( sys.stdout, [\"Col1\", \"Col2\", \"Col3\"] )\n    this, that = parseCommandLine()\n    thisList= This.objects.filter( name=this, that__name=that )\n    for object in thisList:\n        write.writerow( object.col1, object.that.col2, object.theOther.col3 )\nif __name__ == \"__main__\":\n    main()\n\n\nHTML is pretty easy -- Django has an HTML template language.  Rather than render_to_response, you simply render your template and write it to stdout.  And the core of the algorithm, interestingly, is very similar to writing a CSV.  Similar enough that -- without much cleverness -- you should have a design pattern that does both.\n\nOnce you have the CSV working, add the HTML using Django's templates.\n\nPDF's are harder, because you have to actually work out the formatting in some detail.  There are a lot of Python libraries for this.  Interestingly, however, the overall pattern for PDF writing is very similar to CSV and HTML writing.\n\nEmailing means using Python's smtplib directly or Django's email package.  This isn't too hard.  All the pieces are there, you just need to email the output files produced above to some distribution list.\n\nScheduling takes a little thinking to make best use of crontab.  This -- perhaps -- is the hardest part of the job.\n"}
{"prompt": "I have a django model like this:\n\nclass Player(models.Model):\n    name = models.CharField()\n    batting = models.IntegerField()\n    bowling = models.IntegerField()\n\n\nWhat would be the Django QuerySet equivalent of the following SQL?\n\nSELECT * FROM player WHERE batting > bowling;\n\n", "completion": "In django 1.1 you can do the following:\n\nplayers = Player.objects.filter(batting__gt=F('bowling'))\n\n\nSee the other question for details\n"}
{"prompt": "Of course an HTML page can be parsed using any number of python parsers, but I'm surprised that there don't seem to be any public parsing scripts to extract meaningful content (excluding sidebars, navigation, etc.) from a given HTML doc. \n\nI'm guessing it's something like collecting DIV and P elements and then checking them for a minimum amount of text content, but I'm sure a solid implementation would include plenty of things that I haven't thought of.\n", "completion": "Try the Beautiful Soup library for Python. It has very simple methods to extract information from an html file.\n\nTrying to generically extract data from webpages would require people to write their pages in a similar way... but there's an almost infinite number of ways to convey a page that looks identical let alone all the conbinations you can have to convey the same information.\n\nWas there a particular type of information you were trying to extract or some other end goal?\n\nYou could try extracting any content in 'div' and 'p' markers and compare the relative sizes of all the information in the page. The problem then is that people probably group information into collections of 'div's and 'p's (or at least they do if they're writing well formed html!).\n\nMaybe if you formed a tree of how the information is related (nodes would be the 'p' or 'div or whatever and each node would contain the associated text) you could do some sort of analysis to identify the smallest 'p' or 'div' that encompases what appears to be the majority of the information.. ?\n\n[EDIT] Maybe if you can get it into the tree structure I suggested, you could then use a similar points system to spam assassin. Define some rules that attempt to classify the information. Some examples:\n\n+1 points for every 100 words\n+1 points for every child element that has > 100 words\n-1 points if the section name contains the word 'nav'\n-2 points if the section name contains the word 'advert'\n\n\nIf you have a lots of low scoring rules which add up when you find more relevent looking sections, I think that could evolve into a fairly powerful and robust technique.\n\n[EDIT2] Looking at the readability, it seems to be doing pretty much exactly what I just suggested! Maybe it could be improved to try and understand tables better?\n"}
{"prompt": "I'd like to reformat some SQL statements that are a single string with newlines in to something that's much easier to read.\n\nI don't personally know of a good coding style for indenting SQL - how should nested queries / where clauses / left joins / etc by represented to maximise readability?\n\nHas anyone seen a pretty-printing algorithm that does this already? In Python would be even better.\n", "completion": "You can try sqlparse. It's a Python module that provides simple SQL formatting. A online demo is available here.\n"}
{"prompt": "I am looking for RAD like environment for PHP and/or Python free or not does not matter.\n\nIt should have a visual environment where one can use a point and click interface so that it is possible to select objects with mouse and move them around.\n\nI have looked at Delphi4PHP. The RAD part is fantastic, but I don't like the framework on which it is based VCL4PHP (vcl4php.sourceforge.net) is crappy. Just to deploy a simple Hello world application we will have to deploy 40MB of that framework. That is just stupid.....\n\nI looked at Eclipse but it is only a code IDE. Does not have a visual way of designing a page/window. Did I miss any plugin that supports this feature?\n\nI was suggested to give NetBeans IDE a close look so I also looked that up, but did not find what I wanted.\n\nI have also looked up following but none of these are true RAD:\n\n\nNuSphere PHPEd\nVS PHP for Visual Studio \nPHP Designer (not a designer by any means just a plain old IDE)\n\n\nI have not been able to find any descent Python RAD tool also.\n\nI have looked up Yes Software's Code Charge Studio (www.yessoftware.com) but it cannot be used to develop complicated applications like say for example an Accounting System or an Inventory Management App, etc.. It is useful but for very simple apps. Making changes to Visual part (referred as components by this people) is a nightmare. Finally it does not support Python.\n", "completion": "Python and PHP both have a binding for GTK :\n\n\nPyGTK for Python;\nPHP GTK for PHP;\n\n\nYou can use Glade as a RADD to draw a GTK app.\n\nGTK runs on Windows, Linux and Mac OS.\n\nIf you are looking for something embedding everything, I can think of SPE for Python, but you really can run Glade as stand alone. \n\nSince any picture is worth 1000 words :\n\n\n\nI let the PHP lovers choosing their language equivalent.\n"}
{"prompt": "I'm a newbie to programming. I had 1 semester of computer science (we used java).  I got an A in the course and was able to do everything assigned, however I'm not sure I really understood it. I ignored the text and learned by looking at sample programs and then trial and error. I was ahead of the class except for two guys who came in knowing java or another OOP language.\n\nI'd like to learn Python. I'm also going to build a second PC from extra parts I have and use linux. Basically, I want to enhance my knowledge of computers. Thats my motivation.\n\nNow on learning python are there any good programming theory books that would be useful? Or should I read up on more on how computers operate on the lowest levels? I don't think I know enough to ask the question I want. \n\nI guess to make it simple, I am asking what should I know to make the most of learning python. This is not for a career. This is from a desire to know. I am no longer a computer science major (it also would not have any direct applications to my anticipated career.)\n\nI'm not looking to learn in \"30 days\" or \"1 week\" or whatever. So, starting from a very basic level is fine with me. \n\nThanks in advance. I did a search and didn't quite find what I was looking for. \n\nUPDATE: Thanks for all the great advice. I found this site at work and couldn't find it on my home computer, so I am just getting to read now. \n", "completion": "My recommendation is always to start at the high level of abstraction.  You don't need to know how logic gates work and how you can use them to build a CPU -- it's cool stuff, but it's cool stuff that makes a lot more sense once you've messed around at the higher levels.  Python is therefore an excellent choice as a learning aid.\n\nHow to Think Like A Computer Scientist:  Learning With Python is available on the Internet and is an excellent introduction to the high-level concepts that make computers go.  And it's even Python-specific.\n\nIf you're looking to have your brain turned inside-out, SICP will do a good job of it.  I don't recommend it as a first text, though; it's heavy going.\n\nBoth of these books are high-level.  They won't teach you anything about the low-level details like memory structures or what a CPU actually does, but that's something I would reserve for later anyway.\n\nD'A\n"}
{"prompt": "I was wondering if it was possible (and, if so, how) to chain together multiple managers to produce a query set that is affected by both of the individual managers.  I'll explain the specific example that I'm working on:\n\nI have multiple abstract model classes that I use to provide small, specific functionality to other models.  Two of these models are a DeleteMixin and a GlobalMixin.\n\nThe DeleteMixin is defined as such:\n\nclass DeleteMixin(models.Model):\n    deleted = models.BooleanField(default=False)\n    objects = DeleteManager()\n\n    class Meta:\n        abstract = True\n\n    def delete(self):\n        self.deleted = True\n        self.save()\n\n\nBasically it provides a pseudo-delete (the deleted flag) instead of actually deleting the object.\n\nThe GlobalMixin is defined as such:\n\nclass GlobalMixin(models.Model):\n    is_global = models.BooleanField(default=True)\n\n    objects = GlobalManager()\n\n    class Meta:\n        abstract = True\n\n\nIt allows any object to be defined as either a global object or a private object (such as a public/private blog post).\n\nBoth of these have their own managers that affect the queryset that is returned.  My DeleteManager filters the queryset to only return results that have the deleted flag set to False, while the GlobalManager filters the queryset to only return results that are marked as global.  Here is the declaration for both:\n\nclass DeleteManager(models.Manager):\n    def get_query_set(self):\n        return super(DeleteManager, self).get_query_set().filter(deleted=False)\n\nclass GlobalManager(models.Manager):\n    def globals(self):\n        return self.get_query_set().filter(is_global=1)\n\n\nThe desired functionality would be to have a model extend both of these abstract models and grant the ability to only return the results that are both non-deleted and global.  I ran a test case on a model with 4 instances: one was global and non-deleted, one was global and deleted, one was non-global and non-deleted, and one was non-global and deleted.  If I try to get result sets as such: SomeModel.objects.all(), I get instance 1 and 3 (the two non-deleted ones - great!).  If I try SomeModel.objects.globals(), I get an error that DeleteManager doesn't have a globals (this is assuming my model declaration is as such: SomeModel(DeleteMixin, GlobalMixin).  If I reverse the order, I don't get the error, but it doesn't filter out the deleted ones).  If I change GlobalMixin to attach GlobalManager to globals instead of objects (so the new command would be SomeModel.globals.globals()), I get instances 1 and 2 (the two globals), while my intended result would be to only get instance 1 (the global, non-deleted one).\n\nI wasn't sure if anyone had run into any situation similar to this and had come to a result.  Either a way to make it work in my current thinking or a re-work that provides the functionality I'm after would be very much appreciated.  I know this post has been a little long-winded.  If any more explanation is needed, I would be glad to provide it.\n\nEdit:\n\nI have posted the eventual solution I used to this specific problem below.  It is based on the link to Simon's custom QuerySetManager.\n", "completion": "See this snippet on Djangosnippets: http://djangosnippets.org/snippets/734/\n\nInstead of putting your custom methods in a manager, you subclass the queryset itself. It's very easy and works perfectly. The only issue I've had is with model inheritance, you always have to define the manager in model subclasses (just: \"objects = QuerySetManager()\" in the subclass), even though they will inherit the queryset. This will make more sense once you are using QuerySetManager.\n"}
{"prompt": "Consider the following:\n\n@property\ndef name(self):\n\n    if not hasattr(self, '_name'):\n\n    \t# expensive calculation\n    \tself._name = 1 + 1\n\n    return self._name\n\n\nI'm new, but I think the caching could be factored out into a decorator. Only I didn't find one like it ;)\n\nPS the real calculation doesn't depend on mutable values\n", "completion": "Starting from Python 3.2 there is a built-in decorator:\n\n@functools.lru_cache(maxsize=100, typed=False)\n\n\nDecorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls. It can save time when an expensive or I/O bound function is periodically called with the same arguments.\n\n\nExample of an LRU cache for computing Fibonacci numbers:\n\n@lru_cache(maxsize=None)\ndef fib(n):\n    if n < 2:\n        return n\n    return fib(n-1) + fib(n-2)\n\n>>> print([fib(n) for n in range(16)])\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]\n\n>>> print(fib.cache_info())\nCacheInfo(hits=28, misses=16, maxsize=None, currsize=16)\n\n\n\n\nIf you are stuck with Python 2.x, here's a list of other compatible memoization libraries:\n\n\nfunctools32 | PyPI | Source code\nrepoze.lru | PyPI | Source code\npylru | PyPI | Source code\nbackports.functools_lru_cache | PyPI | Source code\n\n"}
{"prompt": "What's the pythonic way to designate unreachable code in python as in:\n\ngender = readFromDB(...) # either 'm' or 'f'\nif gender == 'm':\n    greeting = 'Mr.'\nelif gender == 'f':\n    greeting = 'Ms.'\nelse:\n    # What should this line say?\n\n", "completion": "raise ValueError('invalid gender %r' % gender)\n\n"}
{"prompt": "I have a mapping of catalog numbers to product names:\n\n35  cozy comforter\n35  warm blanket\n67  pillow\n\n\nand need a search that would find misspelled, mixed names like \"warm cmfrter\".\n\nWe have code using edit-distance (difflib), but it probably won't scale to the 18000 names.\n\nI achieved something similar with Lucene, but as PyLucene only wraps Java that would complicate deployment to end-users.\n\nSQLite doesn't usually have full-text or scoring compiled in.\n\nThe Xapian bindings are like C++ and have some learning curve.\n\nWhoosh is not yet well-documented but includes an abusable spell-checker.\n\nWhat else is there?\n", "completion": "Apparently the only way to make fuzzy comparisons fast is to do less of them ;)\n\nInstead of writing another n-gram search or improving the one in Whoosh we now keep a word index, retrieve all entries that have at least one (correctly spelled) word in common with the query, and use difflib to rank those. Works well enough in this case.\n"}
{"prompt": "I'm trying to write an __init__ function for one of my models so that I can create an object by doing\n\np = User('name','email')\n\n\nWhen I write the model, I have \n\n    def __init__(self, name, email, house_id, password):\n            models.Model.__init__(self)\n            self.name = name\n            self.email = email\n\n\nThis works, and I can save the object to the database, but when I do 'User.objects.all()', it doesn't pull anything up unless I take out my __init__ function.  Any ideas?\n", "completion": "Relying on Django's built-in functionality and passing named parameters would be the simplest way to go.\n\np = User(name=\"Fred\", email=\"fred@example.com\")\n\n\nBut if you're set on saving some keystrokes, I'd suggest adding a static convenience method to the class instead of messing with the initializer.\n\n# In User class declaration\n@classmethod\ndef create(cls, name, email):\n  return cls(name=name, email=email)\n\n# Use it\np = User.create(\"Fred\", \"fred@example.com\")\n\n"}
{"prompt": "I would like to emulate the pass-by-value behaviour in python. In other words, I would like to make absolutely sure that the function I write do not modify user supplied data. \n\nOne possible way is to use deep copy:\n\nfrom copy import deepcopy\ndef f(data):\n    data = deepcopy(data)\n    #do stuff\n\n\nis there more efficient or more pythonic way to achieve this goal, making as few assumptions as possible about the object being passed (such as .clone() method)\n\nEdit \n\nI'm aware that technically everything in python is passed by value. I was interested in emulating the behaviour, i.e. making sure I don't mess with the data that was passed to the function. I guess the most general way is to clone the object in question either with its own clone mechanism or with deepcopy.\n", "completion": "You can make a decorator and put the cloning behaviour in that. \n\n>>> def passbyval(func):\ndef new(*args):\n\tcargs = [deepcopy(arg) for arg in args]\n\treturn func(*cargs)\nreturn new\n\n>>> @passbyval\ndef myfunc(a):\n    print a\n\n>>> myfunc(20)\n20\n\n\nThis is not the most robust way, and doesn't handle key-value arguments or class methods (lack of self argument), but you get the picture.\n\nNote that the following statements are equal:\n\n@somedecorator\ndef func1(): pass\n# ... same as ...\ndef func2(): pass\nfunc2 = somedecorator(func2)\n\n\nYou could even have the decorator take some kind of function that does the cloning and thus allowing the user of the decorator to decide the cloning strategy. In that case the decorator is probably best implemented as a class with __call__ overridden.\n"}
{"prompt": "I want my Python script to be able to read Unicode command line arguments in Windows. But it appears that sys.argv is a string encoded in some local encoding, rather than Unicode. How can I read the command line in full Unicode?\n\nExample code: argv.py\n\nimport sys\n\nfirst_arg = sys.argv[1]\nprint first_arg\nprint type(first_arg)\nprint first_arg.encode(\"hex\")\nprint open(first_arg)\n\n\nOn my PC set up for Japanese code page, I get:\n\nC:\\temp>argv.py \"PC\u00e3\u0083\u00bb\u00e3\u0082\u00bd\u00e3\u0083\u0095\u00e3\u0083\u0088\u00e7\u0094\u00b3\u00e8\u00ab\u008b\u00e6\u009b\u00b808.09.24.doc\"\nPC\u00e3\u0083\u00bb\u00e3\u0082\u00bd\u00e3\u0083\u0095\u00e3\u0083\u0088\u00e7\u0094\u00b3\u00e8\u00ab\u008b\u00e6\u009b\u00b808.09.24.doc\n<type 'str'>\n50438145835c83748367905c90bf8f9130382e30392e32342e646f63\n<open file 'PC\u00e3\u0083\u00bb\u00e3\u0082\u00bd\u00e3\u0083\u0095\u00e3\u0083\u0088\u00e7\u0094\u00b3\u00e8\u00ab\u008b\u00e6\u009b\u00b808.09.24.doc', mode 'r' at 0x00917D90>\n\n\nThat's Shift-JIS encoded I believe, and it \"works\" for that filename. But it breaks for filenames with characters that aren't in the Shift-JIS character set\u00e2\u0080\u0094the final \"open\" call fails:\n\nC:\\temp>argv.py J\u00c3\u00b6rgen.txt\nJorgen.txt\n<type 'str'>\n4a6f7267656e2e747874\nTraceback (most recent call last):\n  File \"C:\\temp\\argv.py\", line 7,\nin <module>\n    print open(first_arg)\nIOError: [Errno 2] No such file or directory: 'Jorgen.txt'\n\n\nNote\u00e2\u0080\u0094I'm talking about Python 2.x, not Python 3.0. I've found that Python 3.0 gives sys.argv as proper Unicode. But it's a bit early yet to transition to Python 3.0 (due to lack of 3rd party library support).\n\nUpdate:\n\nA few answers have said I should decode according to whatever the sys.argv is encoded in. The problem with that is that it's not full Unicode, so some characters are not representable.\n\nHere's the use case that gives me grief: I have enabled drag-and-drop of files onto .py files in Windows Explorer. I have file names with all sorts of characters, including some not in the system default code page. My Python script doesn't get the right Unicode filenames passed to it via sys.argv in all cases, when the characters aren't representable in the current code page encoding.\n\nThere is certainly some Windows API to read the command line with full Unicode (and Python 3.0 does it). I assume the Python 2.x interpreter is not using it.\n", "completion": "Here is a solution that is just what I'm looking for, making a call to the Windows GetCommandLineArgvW  function:\nGet sys.argv with Unicode characters under Windows (from ActiveState)\n\nBut I've made several changes, to simplify its usage and better handle certain uses. Here is what I use:\n\nwin32_unicode_argv.py\n\n\"\"\"\nwin32_unicode_argv.py\n\nImporting this will replace sys.argv with a full Unicode form.\nWindows only.\n\nFrom this site, with adaptations:\n      http://code.activestate.com/recipes/572200/\n\nUsage: simply import this module into a script. sys.argv is changed to\nbe a list of Unicode strings.\n\"\"\"\n\n\nimport sys\n\ndef win32_unicode_argv():\n    \"\"\"Uses shell32.GetCommandLineArgvW to get sys.argv as a list of Unicode\n    strings.\n\n    Versions 2.x of Python don't support Unicode in sys.argv on\n    Windows, with the underlying Windows API instead replacing multi-byte\n    characters with '?'.\n    \"\"\"\n\n    from ctypes import POINTER, byref, cdll, c_int, windll\n    from ctypes.wintypes import LPCWSTR, LPWSTR\n\n    GetCommandLineW = cdll.kernel32.GetCommandLineW\n    GetCommandLineW.argtypes = []\n    GetCommandLineW.restype = LPCWSTR\n\n    CommandLineToArgvW = windll.shell32.CommandLineToArgvW\n    CommandLineToArgvW.argtypes = [LPCWSTR, POINTER(c_int)]\n    CommandLineToArgvW.restype = POINTER(LPWSTR)\n\n    cmd = GetCommandLineW()\n    argc = c_int(0)\n    argv = CommandLineToArgvW(cmd, byref(argc))\n    if argc.value > 0:\n        # Remove Python executable and commands if present\n        start = argc.value - len(sys.argv)\n        return [argv[i] for i in\n                xrange(start, argc.value)]\n\nsys.argv = win32_unicode_argv()\n\n\nNow, the way I use it is simply to do:\n\nimport sys\nimport win32_unicode_argv\n\n\nand from then on, sys.argv is a list of Unicode strings. The Python optparse module seems happy to parse it, which is great.\n"}
{"prompt": "Many python libraries, even recently written ones, use httplib2 or the socket interface to perform networking tasks.\n\nThose are obviously easier to code on than Twisted due to their blocking nature, but I think this is a drawback when integrating them with other code, especially GUI one. If you want scalability, concurrency or GUI integration while avoiding multithreading, Twisted is then a natural choice.\n\nSo I would be interested in opinions in those matters:\n\n\nShould new networking code (with the exception of small command line tools) be written with Twisted?\nWould you mix Twisted, http2lib or socket code in the same project?\nIs Twisted pythonic for most libraries (it is more complex than alternatives, introduce a dependency to a non-standard package...)?\n\n\nEdit: please let me phrase this in another way. Do you feel writing new library code with Twisted may add a barrier to its adoption? Twisted has obvious benefits (especially portability and scalability as stated by gimel), but the fact that it is not a core python library may be considered by some as a drawback.\n", "completion": "See asychronous-programming-in-python-twisted, you'll have to decide if depending on a non-standard (external) library fits your needs. Note the answer by @Glyph, he is the founder of the Twisted project, and can authoritatively answer any Twisted related question.\n\n\n  At the core of libraries like Twisted, the function in the main loop is not sleep, but an operating system call like select() or poll(), as exposed by a module like the Python select module. I say \"like\" select, because this is an API that varies a lot between platforms, and almost every GUI toolkit has its own version. Twisted currently provides an abstract interface to 14 different variations on this theme. The common thing that such an API provides is provide a way to say \"Here are a list of events that I'm waiting for. Go to sleep until one of them happens, then wake up and tell me which one of them it was.\"\n\n"}
{"prompt": "Is there a cross-platform way of getting the path to the temp directory in Python 2.6?  \n\nFor example, under Linux that would be /tmp, while under XP C:\\Documents and settings\\[user]\\Application settings\\Temp.\n\nThanks!\n", "completion": "That would be the tempfile module.\n\nIt has functions to get the temporary directory, and also has some shortcuts to create temporary files and directories in it, either named or unnamed.\n\nExample:\n\nimport tempfile\n\nprint tempfile.gettempdir() # prints the current temporary directory\n\nf = tempfile.TemporaryFile()\nf.write('something on temporaryfile')\nf.seek(0) # return to beginning of file\nprint f.read() # reads data back from the file\nf.close() # temporary file is automatically deleted here\n\n\nFor completeness, here's how it searches for the temporary directory, according to the documentation:\n\n\nThe directory named by the TMPDIR environment variable.\nThe directory named by the TEMP environment variable.\nThe directory named by the TMP environment variable.\nA platform-specific location:\n\nOn RiscOS, the directory named by the Wimp$ScrapDir environment variable.\nOn Windows, the directories C:\\TEMP, C:\\TMP, \\TEMP, and \\TMP, in that order.\nOn all other platforms, the directories /tmp, /var/tmp, and /usr/tmp, in that order.\n\nAs a last resort, the current working directory.\n\n"}
{"prompt": "To squeeze into the limited amount of filesystem storage available in an embedded system I'm currently playing with, I would like to eliminate any files that could reasonably be removed without significantly impacting functionality or performance.  The *.py, *.pyo, and *.pyc files in the Python library account for a sizable amount of space, I'm wondering which of these options would be most reasonable for a Python 2.6 installation in a small embedded system:\n\n\nKeep *.py, eliminate *.pyc and *.pyo (Maintain ability to debug, performance suffers?)\nKeep *.py and *.pyc, eliminate *.pyo (Does optimization really buy anything?)\nKeep *.pyc, eliminate *.pyo and *.py (Will this work?)\nKeep *.py, *.pyc, and *.pyo (All are needed?)\n\n", "completion": "http://www.network-theory.co.uk/docs/pytut/CompiledPythonfiles.html\n\n\n  When the Python interpreter is invoked with the -O flag, optimized code is generated and stored in \u00e2\u0080\u0098.pyo\u00e2\u0080\u0099 files. The optimizer currently doesn't help much; it only removes assert statements.\n  \n  Passing two -O flags to the Python interpreter (-OO) will cause the bytecode compiler to perform optimizations that could in some rare cases result in malfunctioning programs. Currently only doc strings are removed from the bytecode, resulting in more compact \u00e2\u0080\u0098.pyo\u00e2\u0080\u0099 files.\n\n\nMy suggestion to you? \n\nUse -OO to compile only .pyo files if you don't need assert statements and __doc__ strings.\n\nOtherwise, go with .pyc only.\n\nEdit\n\nI noticed that you only mentioned the Python library. Much of the python library can be removed if you only need part of the functionality. \n\nI also suggest that you take a look at tinypy which is large subset of Python in about 64kb.\n"}
{"prompt": "How can I open __init__.pyc here?\n\n    >>> import stompservice\n    <module 'stompservice' from 'C:\\Python25\\lib\\site-packages\\stompservice-0.1.0-py2.5.egg\\stompservice\\__init__.pyc'>\n\n\nAll I see in C:\\Python25\\lib\\site-packages\\ is the .egg file, but where are the internal files of the package?\n", "completion": "http://peak.telecommunity.com/DevCenter/PythonEggs\n\n.egg files are simply renamed zip files.\n\nOpen the egg with your zip program, or just rename the extension to .zip, and extract.\n"}
{"prompt": "I have a similar model\n\nClass Student(models.Model):\n\"\"\"A simple class which holds the basic info\nof a student.\"\"\"\n\nname = models.CharField(max_length=50)\nage = models.PositiveIntegerField()\nphoto = models.ImageField(upload_to='foobar', blank=True, null=True)\n\n\nAs we can see photo field is optional. I wanted all the students who have their images saved in the college DB. For that i did this\n\n>>> Student.objects.exclude(photo__name=None)\n\n\nBut i am getting this error :\n\nFieldError: Join on field 'photo' not permitted.\n\n\nSo, How can i extract all those students having their photos?\n\nAny sort of help regarding this would be appreciated.\nThanks in advance.\n", "completion": "It doesn't work because field lookups only work on other models. Here, name is an attribute on the return value of your photo field.\n\nTry this instead:\n\nStudent.objects.exclude(photo__isnull=True)\n\n\nIt is preferred to use isnull instead of comparing equality to None.\n\nEDIT:\n\nJeff Ober's suggestion:\n\nStudent.objects.exclude(photo='')\n\n\nHe says filtering is performed on the actual values stored in DB. In a file field's case the path to the file.\n"}
{"prompt": "How can I access Microsoft Access databases in Python? With SQL?\n\nI'd prefere a solution that works with Linux, but I could also settle for Windows.\n\nI only require read access.\n", "completion": "On Linux, MDBTools is your only chance as of now. [disputed]\n\nOn Windows, you can deal with mdb files with pypyodbc.\n\nTo create an Access mdb file:\n\nimport pypyodbc\npypyodbc.win_create_mdb( \"D:\\\\Your_MDB_file_path.mdb\" )\n\n\nHere is an Hello World script that fully demostate pypyodbc's Access support functions.\n\nDisclaimer: I'm the developer of pypyodbc.\n"}
{"prompt": "What is the best way to find out the user that a python process is running under?\n\nI could do this:\n\nname = os.popen('whoami').read() \n\n\nBut that has to start a whole new process.\n\nos.environ[\"USER\"]\n\n\nworks sometimes, but sometimes that environment variable isn't set.\n", "completion": "import getpass\nprint getpass.getuser()\n\n\nSee the documentation of the getpass module.\n\n\n  getpass.getuser()\n  \n  Return the \u00e2\u0080\u009clogin name\u00e2\u0080\u009d of the user. Availability: Unix, Windows.\n  \n  This function checks the environment variables LOGNAME, USER,\n  LNAME and USERNAME, in order, and\n  returns the value of the first one\n  which is set to a non-empty string. If\n  none are set, the login name from the\n  password database is returned on\n  systems which support the pwd module,\n  otherwise, an exception is raised.\n\n"}
{"prompt": "I've got a python list of dictionaries:\n\nmylist = [\n{'id':0, 'weight':10, 'factor':1, 'meta':'ABC'},\n{'id':1, 'weight':5, 'factor':1, 'meta':'ABC'},\n{'id':2, 'weight':5, 'factor':2, 'meta':'ABC'},\n{'id':3, 'weight':1, 'factor':1, 'meta':'ABC'}\n]\n\n\nWhats the most efficient/cleanest way to order that list by weight then factor (numericaly). The resulting list should look like:\n\nmylist = [\n{'id':3, 'weight':1, 'factor':1, 'meta':'ABC'},\n{'id':1, 'weight':5, 'factor':1, 'meta':'ABC'},\n{'id':2, 'weight':5, 'factor':2, 'meta':'ABC'},\n{'id':0, 'weight':10, 'factor':1, 'meta':'ABC'},\n]\n\n", "completion": "mylist.sort(key=lambda d: (d['weight'], d['factor']))\n\n\nor\n\nimport operator\nmylist.sort(key=operator.itemgetter('weight', 'factor'))\n\n"}
{"prompt": "I'm trying to get the elements in an HTML doc that contain the following pattern of text: #\\S{11}\n\n<h2> this is cool #12345678901 </h2>\n\n\nSo, the previous would match by using:\n\nsoup('h2',text=re.compile(r' #\\S{11}'))\n\n\nAnd the results would be something like:\n\n[u'blahblah #223409823523', u'thisisinteresting #293845023984']\n\n\nI'm able to get all the text that matches (see line above). But I want the parent element of the text to match, so I can use that as a starting point for traversing the document tree. In this case, I'd want all the h2 elements to return, not the text matches.\n\nIdeas?\n", "completion": "from BeautifulSoup import BeautifulSoup\nimport re\n\nhtml_text = \"\"\"\n<h2>this is cool #12345678901</h2>\n<h2>this is nothing</h2>\n<h1>foo #126666678901</h1>\n<h2>this is interesting #126666678901</h2>\n<h2>this is blah #124445678901</h2>\n\"\"\"\n\nsoup = BeautifulSoup(html_text)\n\n\nfor elem in soup(text=re.compile(r' #\\S{11}')):\n    print elem.parent\n\n\nPrints:\n\n<h2>this is cool #12345678901</h2>\n<h2>this is interesting #126666678901</h2>\n<h2>this is blah #124445678901</h2>\n\n"}
{"prompt": "I'm writing a web app using Python and the web.py framework, and I need to use memcached throughout.\n\nI've been searching the internet trying to find some good documentation on the python-memcached module, but all I could find was this example on the MySQL website, and the documentation on its methods isn't great.\n", "completion": "It's fairly simple. You write values using keys and expiry times. You get values using keys. You can expire keys from the system.\n\nMost clients follow the same rules. You can read the generic instructions and best practices on the memcached homepage.\n\nIf you really want to dig into it, I'd look at the source. Here's the header comment:\n\n\"\"\"\nclient module for memcached (memory cache daemon)\n\nOverview\n========\n\nSee U{the MemCached homepage<http://www.danga.com/memcached>} for more about memcached.\n\nUsage summary\n=============\n\nThis should give you a feel for how this module operates::\n\n    import memcache\n    mc = memcache.Client(['127.0.0.1:11211'], debug=0)\n\n    mc.set(\"some_key\", \"Some value\")\n    value = mc.get(\"some_key\")\n\n    mc.set(\"another_key\", 3)\n    mc.delete(\"another_key\")\n\n    mc.set(\"key\", \"1\")   # note that the key used for incr/decr must be a string.\n    mc.incr(\"key\")\n    mc.decr(\"key\")\n\nThe standard way to use memcache with a database is like this::\n\n    key = derive_key(obj)\n    obj = mc.get(key)\n    if not obj:\n        obj = backend_api.get(...)\n        mc.set(key, obj)\n\n    # we now have obj, and future passes through this code\n    # will use the object from the cache.\n\nDetailed Documentation\n======================\n\nMore detailed documentation is available in the L{Client} class.\n\"\"\"\n\n"}
{"prompt": "Imagine a string, like 'Agh#$%#%2341-   -!zdrkfd' and I only wish to perform some operating on it such that only the lowercase letters are returned (as an example), which in this case would bring 'ghzdrkfd'.\n\nHow do you do this in Python?  The obvious way would be to create a list, of characters, 'a' through 'z', then iterate over the characters in my string and build a new string, character by character, of those in my list only.  This seems primitive.\n\nI was wondering if regular expressions are appropriate.  Replacing unwanted characters seems problematic and I tend to prefer whitelisting over blacklisting.  The .match function does not seem appropriate.  I have looked over the appropriate page on the Python site, but have not found a method which seems to fit.\n\nIf regular expressions are not appropriate and the correct approach is looping, is there a simple function which \"explodes\" a string into a list?  Or am I just hitting another for loop there?\n", "completion": "If you are looking for efficiency. Using the translate function is the fastest you can get.\n\nIt can be used to quickly replace characters and/or delete them.\n\nimport string\ndelete_table  = string.maketrans(\n    string.ascii_lowercase, ' ' * len(string.ascii_lowercase)\n)\ntable = string.maketrans('', '')\n\n\"Agh#$%#%2341- -!zdrkfd\".translate(table, delete_table)\n\n\nIn python 2.6: you don't need the second table anymore\n\nimport string\ndelete_table  = string.maketrans(\n    string.ascii_lowercase, ' ' * len(string.ascii_lowercase)\n)\n\"Agh#$%#%2341- -!zdrkfd\".translate(None, delete_table)\n\n\nThis is method is way faster than any other. Of course you need to store the delete_table somewhere and use it. But even if you don't store it and build it every time, it is still going to be faster than other suggested methods so far.\n\nTo confirm my claims here are the results:\n\nfor i in xrange(10000):\n    ''.join(c for c in s if c.islower())\n\nreal    0m0.189s\nuser    0m0.176s\nsys 0m0.012s\n\n\nWhile running the regular expression solution:\n\nfor i in xrange(10000):\n    re.sub(r'[^a-z]', '', s)\n\nreal    0m0.172s\nuser    0m0.164s\nsys 0m0.004s\n\n\n[Upon request] If you pre-compile the regular expression:\n\nr = re.compile(r'[^a-z]')\nfor i in xrange(10000):\n    r.sub('', s)\n\nreal    0m0.166s\nuser    0m0.144s\nsys 0m0.008s\n\n\nRunning the translate method the same number of times took:\n\nreal    0m0.075s\nuser    0m0.064s\nsys 0m0.012s\n\n"}
{"prompt": "I have a file containing roughly all the words in English (~60k words, ~500k characters). I want to test whether a certain word I receive as input is \"in English\" (i.e. if this exact word is in the list).\n\nWhat would be the most efficient way to do this in Python?\n\nThe trivial solution is to load the file into a list and check whether the word is in that list. The list can be sorted, which I believe will shrink the complexity to O(logn). However I'm not sure about how Python implements searching through lists, and whether there's a performance penalty if such a large list is in memory. Can I \"abuse\" the fact I can put a cap on the length of words? (e.g. say the longest one is 15 characters long).\n\nPlease note I run the application on a machine with lots of memory, so I care less for memory consumption than for speed and CPU utilization.\n\nThanks\n", "completion": "The python Set is what you should try.\n\n\n  A set object is an unordered collection of distinct hashable objects. Common uses include membership testing, removing duplicates from a sequence, and computing mathematical operations such as intersection, union, difference, and symmetric difference. \n\n"}
{"prompt": "Are there regular expression equivalents for searching and modifying tree structures? Concise mini-languages (like perl regex) are what I am looking for.\n\nHere is an example that might clarify what I am looking for.\n\n<root>\n  <node name=\"1\">\n    subtrees ....\n  </node>\n  <node name=\"2\">\n    <node name=\"2.1\">\n     data\n    </node>\n    other subtrees...\n  </node>\n</root>\n\n\nAn operation that would be possible on the above tree is \"move subtree at node 2.1 into\nthe subtree at node 1.\" The result of the operation might look something like..\n\n<root>\n  <node name=\"1\">\n    subtrees ....\n    <node name=\"2.1\">\n     data\n    </node>\n  </node>\n  <node name=\"2\">\n    other subtrees...\n  </node>\n</root>\n\n\nSearch and replace operations like find all nodes with atleast 2 children, find all nodes whose data starts with \"a\" and replace it with \"b\" if the subtrees have atleast 2 other siblings, etc. should be supported. \n\nFor strings, where the only dimension is across the length of the string, we can do many of above operations (or their 1D equivalents) using regular expressions. I wonder if there are equivalents for trees. (instead of a single regex, you might need to write a set of transformation rules, but that is ok).\n\nI would like to know if there is some simple mini language (not regex per.se, but something that is as accessible as regex via libraries, etc..). to perform these operations? Preferably, as a python library.\n", "completion": "TSurgeon and Tregex from Stanford is capable of doing that. You can download the library from http://nlp.stanford.edu/software/tregex.shtml\n"}
{"prompt": "With python properties, I can make it such that \n\nobj.y \n\n\ncalls a function rather than just returning a value.\n\nIs there a way to do this with modules? I have a case where I want\n\nmodule.y \n\n\nto call a function, rather than just returning the value stored there.\n", "completion": "Only instances of new-style classes can have properties.  You can make Python believe such an instance is a module by stashing it in sys.modules[thename] = theinstance.  So, for example, your m.py module file could be:\n\nimport sys\nclass _M(object):\n  def __init__(self):\n    self.c = 0\n  def afunction(self):\n    self.c += 1\n    return self.c\n  y = property(afunction)\nsys.modules[__name__] = _M()\n\n\nEdited: removed an implicit dependency on globals (had nothing to do with the point of the example but did confuse things by making the original code fail!).\n"}
{"prompt": "Brand new to web design, using python.  Got Apache up and running, test python script working in cgi-bin directory.  Get valid results when I type in the URL explicitly:  \".../cgi-bin/showenv.py\"\n\nBut I don't want the URL to look that way.  Here at stackoverflow, for example, the URLs that display in my address bar never have the messy details showing the script that was used to run them.  They're clean of cgi-bin, .py, etc. extensions.  How do I do that?\n\nEDIT: Thanks for responses, every single one helpful, lots to learn.  I'm going with URL Rewriting for now; example in the docs looks extremely close to what I actually want to do.  But I'm committed to python, so will have to look at WSGI down the road.\n", "completion": "The python way of writing web applications is not cgi-bin. It is by using WSGI.\n\nWSGI is a standard interface between web servers and Python web applications or frameworks. The PEP 0333 defines it.\n\nThere are no disadvantages in using it instead of CGI. And you'll gain a lot. Beautiful URLs is just one of the neat things you can do easily.\n\nAlso, writing a WSGI application means you can deploy on any web server that supports the WSGI interface. Apache does so by using mod_wsgi.\n\nYou can configure it in apache like that:\n\nWSGIScriptAlias /myapp /usr/local/www/wsgi-scripts/myapp.py\n\n\nThen all requests on http://myserver.domain/myapp will go to myapp.py's application callable, including http://myserver.domain/myapp/something/here. \n\nexample myapp.py:\n\ndef application(environ, start_response):\n    start_response('200 OK', [('Content-type', 'text/plain')])\n    return ['Hello World!']\n\n"}
{"prompt": "I am trying to run a simple multiple processes application in Python. The main thread spawns 1 to N processes and waits until they all done processing. The processes each run an infinite loop, so they can potentially run forever without some user interruption, so I put in some code to handle a KeyboardInterrupt:\n\n#!/usr/bin/env python\nimport sys\nimport time\nfrom multiprocessing import Process\n\ndef main():\n    # Set up inputs..\n\n    # Spawn processes\n    Proc( 1).start()\n    Proc( 2).start()\n\nclass Proc ( Process ):\n    def __init__ ( self, procNum):\n        self.id = procNum\n        Process.__init__(self)\n\n    def run ( self ):\n        doneWork = False\n\n        while True:\n\n            try:\n                # Do work...\n                time.sleep(1)\n                sys.stdout.write('.')\n\n                if doneWork:\n                    print \"PROC#\" + str(self.id) + \" Done.\"\n                    break\n\n            except KeyboardInterrupt:\n                print \"User aborted.\"\n                sys.exit()\n\n# Main Entry\nif __name__==\"__main__\":\n    main()\n\n\nThe problem is that when using CTRL-C to exit, I get an additional error even though the processes seem to exit immediately:\n\n......User aborted.\nError in atexit._run_exitfuncs:\nTraceback (most recent call last):\n  File \"C:\\Python26\\lib\\atexit.py\", line 24, in _run_exitfuncs\n    func(*targs, **kargs)\n  File \"C:\\Python26\\lib\\multiprocessing\\util.py\", line 281, in _exit_function\n    p.join()\n  File \"C:\\Python26\\lib\\multiprocessing\\process.py\", line 119, in join\n    res = self._popen.wait(timeout)\n  File \"C:\\Python26\\lib\\multiprocessing\\forking.py\", line 259, in wait\n    res = _subprocess.WaitForSingleObject(int(self._handle), msecs)\nKeyboardInterrupt\nError in sys.exitfunc:\nTraceback (most recent call last):\n  File \"C:\\Python26\\lib\\atexit.py\", line 24, in _run_exitfuncs\n    func(*targs, **kargs)\n  File \"C:\\Python26\\lib\\multiprocessing\\util.py\", line 281, in _exit_function\n    p.join()\n  File \"C:\\Python26\\lib\\multiprocessing\\process.py\", line 119, in join\n    res = self._popen.wait(timeout)\n  File \"C:\\Python26\\lib\\multiprocessing\\forking.py\", line 259, in wait\n    res = _subprocess.WaitForSingleObject(int(self._handle), msecs)\nKeyboardInterrupt\n\n\nI am running Python 2.6 on Windows. If there is a better way to handle multiprocessing in Python, please let me know.\n", "completion": "Rather then just forcing sys.exit(), you want to send a signal to your threads to tell them to stop.  Look into using signal handlers and threads in Python.\n\nYou could potentially do this by changing your while True: loop to be while keep_processing:  where keep_processing is some sort of global variable that gets set on the KeyboardInterrupt exception.  I don't think this is a good practice though.\n"}
{"prompt": "I have a Python script that I want to use as a controller to another Python script. I have a server with 64 processors, so want to spawn up to 64 child processes of this second Python script. The child script is called:\n\n$ python create_graphs.py --name=NAME\n\n\nwhere NAME is something like XYZ, ABC, NYU etc.\n\nIn my parent controller script I retrieve the name variable from a list:\n\nmy_list = [ 'XYZ', 'ABC', 'NYU' ]\n\n\nSo my question is, what is the best way to spawn off these processes as children? I want to limit the number of children to 64 at a time, so need to track the status (if the child process has finished or not) so I can efficiently keep the whole generation running.\n\nI looked into using the subprocess package, but rejected it because it only spawns one child at a time. I finally found the multiprocessor package, but I admit to being overwhelmed by the whole threads vs. subprocesses documentation.\n\nRight now, my script uses subprocess.call to only spawn one child at a time and looks like this:\n\n#!/path/to/python\nimport subprocess, multiprocessing, Queue\nfrom multiprocessing import Process\n\nmy_list = [ 'XYZ', 'ABC', 'NYU' ]\n\nif __name__ == '__main__':\n    processors = multiprocessing.cpu_count()\n\n    for i in range(len(my_list)):\n        if( i < processors ):\n             cmd = [\"python\", \"/path/to/create_graphs.py\", \"--name=\"+ my_list[i]]\n             child = subprocess.call( cmd, shell=False )\n\n\nI really want it to spawn up 64 children at a time. In other stackoverflow questions I saw people using Queue, but it seems like that creates a performance hit?\n", "completion": "What you are looking for is the process pool class in multiprocessing.\n\nimport multiprocessing\nimport subprocess\n\ndef work(cmd):\n    return subprocess.call(cmd, shell=False)\n\nif __name__ == '__main__':\n    count = multiprocessing.cpu_count()\n    pool = multiprocessing.Pool(processes=count)\n    print pool.map(work, ['ls'] * count)\n\n\nAnd here is a calculation example to make it easier to understand. The following will divide 10000 tasks on N processes where N is the cpu count. Note that I'm passing None as the number of processes. This will cause the Pool class to use cpu_count for the number of processes (reference)\n\nimport multiprocessing\nimport subprocess\n\ndef calculate(value):\n    return value * 10\n\nif __name__ == '__main__':\n    pool = multiprocessing.Pool(None)\n    tasks = range(10000)\n    results = []\n    r = pool.map_async(calculate, tasks, callback=results.append)\n    r.wait() # Wait on the results\n    print results\n\n"}
{"prompt": "Why python compile the source to bytecode before interpreting?\n\nWhy not interpret from the source directly?\n", "completion": "Nearly no interpreter really interprets code directly, line by line \u00e2\u0080\u0093 it's simply too inefficient. Almost all interpreters use some intermediate representation which can be executed easily. Also, small optimizations can be performed on this intermediate code.\n\nPython furthermore stores this code which has a huge advantage for the next time this code gets executed: Python doesn't have to parse the code anymore; parsing is the slowest part in the compile process. Thus, a bytecode representation reduces execution overhead quite substantially.\n"}
{"prompt": "I'm programming in python on windows and would like to accurately measure the time it takes for a function to run.  I have written a function \"time_it\" that takes another function, runs it, and returns the time it took to run.\n\ndef time_it(f, *args):\n    start = time.clock()\n    f(*args)\n    return (time.clock() - start)*1000\n\n\ni call this 1000 times and average the result.  (the 1000 constant at the end is to give the answer in milliseconds.)\n\nThis function seems to work but i have this nagging feeling that I'm doing something wrong, and that by doing it this way I'm using more time than the function actually uses when its running.\n\nIs there a more standard or accepted way to do this?\n\nWhen i changed my test function to call a print so that it takes longer, my time_it function returns an average of 2.5 ms while the cProfile.run('f()') returns and average of 7.0 ms.  I figured my function would overestimate the time if anything, what is going on here?\n\nOne additional note, it is the relative time of functions compared to each other that i care about, not the absolute time as this will obviously vary depending on hardware and other factors.\n", "completion": "Use the timeit module from the Python standard library.\n\nBasic usage:\n\nfrom timeit import Timer\n\n# first argument is the code to be run, the second \"setup\" argument is only run once,\n# and it not included in the execution time.\nt = Timer(\"\"\"x.index(123)\"\"\", setup=\"\"\"x = range(1000)\"\"\")\n\nprint t.timeit() # prints float, for example 5.8254\n# ..or..\nprint t.timeit(1000) # repeat 1000 times instead of the default 1million\n\n"}
{"prompt": "I'm working on code to parse a configuration file written in XML, where the XML tags are mixed case and the case is significant.  Beautiful Soup appears to convert XML tags to lowercase by default, and I would like to change this behavior.\n\nI'm not the first to ask a question on this subject [see here].  However, I did not understand the answer given to that question and in BeautifulSoup-3.1.0.1 BeautifulSoup.py does not appear to contain any instances of \"encodedName\" or \"Tag.__str__\"\n", "completion": "import html5lib\nfrom html5lib import treebuilders\n\nf = open(\"mydocument.html\")\nparser = html5lib.XMLParser(tree=treebuilders.getTreeBuilder(\"beautifulsoup\"))\ndocument = parser.parse(f)\n\n\n'document' is now a BeautifulSoup-like tree, but retains the cases of tags. See html5lib for documentation and installation.\n"}
{"prompt": "I have been doing some work in python, but that was all for stand alone applications. I'm curious to know whether any offshoot of python supports web development?\n\nWould some one also suggest a good tutorial or a website from where I can pick up some of the basics of web development using python?\n", "completion": "Now that everyone has said Django, I can add my two cents: I would argue that you might learn more by looking at the different components first, before using Django. For web development with Python, you often want 3 components:\n\n\nSomething that takes care\nof the HTTP stuff (e.g.\nCherryPy)\nA templating language\nto create your web pages.\nMako\nis very pythonic and works with Cherrpy.\nIf you get your data from a\ndatabase, an ORM comes in handy.\nSQLAlchemy\nwould be an example.\n\n\nAll the links above have good tutorials. For many real-world use-cases, Django will be a better solution than such a stack as it seamlessly integrates this functionality (and more). And if you need a CMS, Django is your best bet short of Zope. Nevertheless, to get a good grasp of what's going on, a stack of loosely coupled programs might be better. Django hides a lot of the details.\n"}
{"prompt": "I've tried all manner of Python modules and they either escape too much or in the wrong way.\nWhat's the best way you've found to escape quotes (\", ') in Python?\n", "completion": "If it's part of a Database query you should be able to use a Parameterized SQL Statement.\n\nAs well as escaping your quotes, this will deal with all special characters and will protect you from SQL injection attacks.\n"}
{"prompt": "Does python provide an elegant way to check for \"equality\" of sequences of different types? The following work, but they seem rather ugly and verbose for python code:\n\ndef comp1(a, b):\n    if len(a) != len(b):\n        return False\n    for i, v in enumerate(a):\n        if v != b[i]:\n            return False\n    return True\n\n\nThe following is a bit shorter, but also less efficient since a third sequence is created:\n\ndef comp2(a, b):\n    for l, r in map(None, a, b):\n        if l != r:\n            return False\n    return True\n\n\nShoehorning one of those examples into a list comprehension isn't really what I'm looking for either.\n\nEdit: Ideally I am looking for a solution that doesn't create another sequence during the comparison.\n", "completion": "Convert both sequences to lists, and use builtin list comparison. It should be sufficient, unless your sequences are really large.\n\nlist(a) == list(b)\n\n\nEdit:\n\nTesting done by schickb shows that using tuples is slightly faster:\n\ntuple(a) == tuple(b)\n\n"}
{"prompt": "I have 20+ tables similar to table 1. Where all letters represent actual values.\n\nTable 1:\n$ / cars |<1 | 2 | 3 | 4+\n<10,000  | a | b | c | d\n20,000   | e | f | g | h\n30,000   | i | j | k | l\n40,000+  | m | n | o | p\n\n\nA user input could be for example, (2.4, 24594) which is a value between f, g, j, and k.\nMy Python function definition and pseudo-code to calculate this bilinear interpolation is as follows.\n\ndef bilinear_interpolation( x_in, y_in, x_high, x_low, y_low, y_high ):\n   # interpolate with respect to x\n   # interpolate with respect to y\n   # return result\n\n\nHow should I store the data from table 1 (a file, a dict, tuple of tuples, or dict of lists), so I can perform the bilinear interpolation most efficiently and correctly? \n", "completion": "If you want the most computationally efficient solution I can think of and are not restricted to the standard library, then I would recommend scipy/numpy.  First, store the a..p array as a 2D numpy array and then both the $4k-10k and 1-4 arrays as 1D numpy arrays.  Use scipy's interpolate.interp1d if both 1D arrays are monotonically increasing, or interpolate.bsplrep (bivariate spline representation) if not and your example arrays are as small as your example.  Or simply write your own and not bother with scipy.  Here are some examples:\n\n# this follows your pseudocode most closely, but it is *not*\n# the most efficient since it creates the interpolation \n# functions on each call to bilinterp\nfrom scipy import interpolate\nimport numpy\ndata = numpy.arange(0., 16.).reshape((4,4))  #2D array\nprices = numpy.arange(10000., 50000., 10000.)\ncars = numpy.arange(1., 5.)\ndef bilinterp(price,car):\n    return interpolate.interp1d(cars, interpolate.interp1d(prices, a)(price))(car)\nprint bilinterp(22000,2)\n\n\nThe last time I checked (a version of scipy from 2007-ish) it only worked for monotonically increasing arrays of x and y)\n\nfor small arrays like this 4x4 array, I think you want to use this:\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.bisplrep.html#scipy.interpolate.bisplrep\nwhich will handle more interestingly shaped surfaces and the function only needs to be created once.  For larger arrays, I think you want this (not sure if this has the same restrictions as interp1d):\nhttp://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp2d.html#scipy.interpolate.interp2d\nbut they both require a different and more verbose data structure than the three arrays in the example above.\n"}
{"prompt": "if hasattr(obj, 'attribute'):\n    # do somthing\n\n\nvs\n\ntry:\n    # access obj.attribute\nexcept AttributeError, e:\n    # deal with AttributeError\n\n\nWhich should be preferred and why?\n", "completion": "Any benches that illustrate difference in performance?\n\ntimeit it's your friend\n\n$ python -mtimeit -s 'class C(object): a = 4\nc = C()' 'hasattr(c, \"nonexistent\")'\n1000000 loops, best of 3: 1.87 usec per loop\n$ python -mtimeit -s 'class C(object): a = 4\nc = C()' 'hasattr(c, \"a\")'\n1000000 loops, best of 3: 0.446 usec per loop\n$ python -mtimeit -s 'class C(object): a = 4\nc = C()' 'try:\n c.a\nexcept:\n pass'\n1000000 loops, best of 3: 0.247 usec per loop\n$ python -mtimeit -s 'class C(object): a = 4\nc = C()' 'try:\n c.nonexistent\nexcept:\n pass'\n100000 loops, best of 3: 3.13 usec per loop\n$\n\n       |positive|negative\nhasattr|  0.446 |  1.87 \ntry    |  0.247 |  3.13\n\n"}
{"prompt": "I updated my python interpreter, but I think the old one is still called. When I check for the version I get:\n\n$ python -V\nPython 3.0.1\n\n\nBut I believe the old interpreter is still being called. When I run the command:\n\npython myProg.py\n\n\nThe script runs properly. But when I invoke it with the command\n\n./myProg.py\n\n\nI get the error message:\n\nAttributeError: 'str' object has no attribute 'format'\n\n\nWhich apparently is due to the old interpreter being called. How can I fix this? I run Mac OS X 10.5. Has it something to do with the first line:\n\n#!/usr/bin/python\n\n\nI just started out with python and am not very familiar with interpreted languages, so I am not too sure what is going on.\n\nEdit: Wow, that was quick. Thanks a lot!\n", "completion": "According to the first line of the script, #!/usr/bin/python, you are calling the Python interpreter at /usr/bin/python (which is most likely the one that ships with Mac OS X). You have to change that path to the path where you installed your Python 3 interpreter (likely /usr/local/bin/python or /opt/local/bin/python); or you can just change that line to read #!/usr/bin/env python, which will call the python listed first in your PATH variable (which seems to be the newer version you installed).\n"}
{"prompt": "I'm trying to learn a little about python scripting in GVim, but I'm having trouble with starting. Elementary things, really.\n\nAre there any resources / tutorials concerting python scripting in Vim out there ? Simple examples which show the basic principles, would be welcomed also.\n", "completion": "Here is all I know of in terms of such docs:\n\n\nA nice presentation by Sean Reifschneider\nThe \"official\" VIM Python Interface reference\nsome shorter notes\nExtending Vim With Python\n\n"}
{"prompt": "I don't think this has been asked before-I have a folder that has lots of different .py files.  The script I've made only uses some-but some call others & I don't know all the ones being used.  Is there a program that will get everything needed to make that script run into one folder?\n\nCheers!\n", "completion": "# zipmod.py - make a zip archive consisting of Python modules and their dependencies as reported by modulefinder\n# To use: cd to the directory containing your Python module tree and type\n# $ python zipmod.py archive.zip mod1.py mod2.py ...\n# Only modules in the current working directory and its subdirectories will be included.\n# Written and tested on Mac OS X, but it should work on other platforms with minimal modifications.\n\nimport modulefinder\nimport os\nimport sys\nimport zipfile\n\ndef main(output, *mnames):\n    mf = modulefinder.ModuleFinder()\n    for mname in mnames:\n        mf.run_script(mname)\n    cwd = os.getcwd()\n    zf = zipfile.ZipFile(output, 'w')\n    for mod in mf.modules.itervalues():\n        if not mod.__file__:\n            continue\n        modfile = os.path.abspath(mod.__file__)\n        if os.path.commonprefix([cwd, modfile]) == cwd:\n            zf.write(modfile, os.path.relpath(modfile))\n    zf.close()\n\nif __name__ == '__main__':\n    main(*sys.argv[1:])\n\n"}
{"prompt": "Modifying Abstract Syntax Trees\n\nI would like to be able to build and modify an ast and then optionally write it out as python byte code for execution later without overhead.\n\nI have been hacking around with the ast docs for python3.0 and python2.6, but I can't seem to find any good sources on best practices for this type of code.\n\nQuestion\n\nWhat are some best practices and guidelines for modifying abstract syntax trees in python?\n\n[edit]\n\nUnknown states that byteplay is a good example of such a library.\n\nAlso, benford cites GeniuSQL which uses abstract syntax trees to transform python code to SQL.\n", "completion": "Other than the manual and the source code, you are on your own. This subject and python bytecode are very undocumented.\n\nAlternatively you could try using this python bytecode library which I have heard good thing about but haven't tried it yet:\n\nhttp://code.google.com/p/byteplay/\n"}
{"prompt": "Is there a simple way to run a Python script on Windows/Linux/OS X?\n\nOn the latter two, subprocess.Popen(\"/the/script.py\") works, but on Windows I get the following error:\n\nTraceback (most recent call last):\n  File \"test_functional.py\", line 91, in test_functional\n    log = tvnamerifiy(tmp)\n  File \"test_functional.py\", line 49, in tvnamerifiy\n    stdout = PIPE\n  File \"C:\\Python26\\lib\\subprocess.py\", line 595, in __init__\n    errread, errwrite)\n  File \"C:\\Python26\\lib\\subprocess.py\", line 804, in _execute_child\n    startupinfo)\nWindowsError: [Error 193] %1 is not a valid Win32 application\n\n\n\n\n\n  monkut's comment: The use case isn't clear. Why use subprocess to run a python script? Is there something preventing you from importing the script and calling the necessary function?\n\n\nI was writing a quick script to test the overall functionality of a Python-command-line tool (to test it on various platforms). Basically it had to create a bunch of files in a temp folder, run the script on this and check the files were renamed correctly.\n\nI could have imported the script and called the function, but since it relies on sys.argv and uses sys.exit(), I would have needed to do something like..\n\nimport sys\nimport tvnamer\nsys.argv.append(\"-b\", \"/the/folder\")\ntry:\n    tvnamer.main()\nexcept BaseException, errormsg:\n    print type(errormsg)\n\n\nAlso, I wanted to capture the stdout and stderr for debugging incase something went wrong.\n\nOf course a better way would be to write the script in more unit-testable way, but the script is basically \"done\" and I'm doing a final batch of testing before doing a \"1.0\" release (after which I'm going to do a rewrite/restructure, which will be far tidier and more testable)\n\nBasically, it was much easier to simply run the script as a process, after finding the sys.executable variable. I would have written it as a shell-script, but that wouldn't have been cross-platform. The final script can be found here\n", "completion": "Just found sys.executable - the full path to the current Python executable, which can be used to run the script (instead of relying on the shbang, which obviously doesn't work on Windows)\n\nimport sys\nimport subprocess\n\ntheproc = subprocess.Popen([sys.executable, \"myscript.py\"])\ntheproc.communicate()\n\n"}
{"prompt": "I'm learning Django by building a simple recipes app. I have a 1 table model using the 'choices' field option for recipe categories rather than using a 2nd 'categories' table and a foreign key relationship. So i created db table via syncdb and then loaded table with test data. When i go to admin and click on the 'Recipes' link in an attempt to view recipes i get the following error:\n\nTemplate error\n\nIn template /var/lib/python-support/python2.6/django/contrib/admin/templates/admin/change_list.html, error at line 34\nCaught an exception while rendering: too many values to unpack\n\n\nIf anyone can shed light on this cryptic error that would be great. Db is Sqlite. Django version is 1.0. The model is listed below:\n\nfrom django.db import models\n\nclass Recipe(models.Model):\n    CATEGORY_CHOICES = (\n        (1, u'Appetizer'),\n        (2, u'Bread'),\n        (3, u'Dessert'),\n        (4, u'Drinks'),\n        (5, u'Main Course'),\n        (6, u'Salad'),\n        (7, u'Side Dish'),\n        (8, u'Soup'),\n        (9, u'Sauce/Marinade'),\n        (10, u'Other'),        \n    )\n    name = models.CharField(max_length=255)\n    submitter = models.CharField(max_length=40)\n    date = models.DateTimeField()\n    category = models.SmallIntegerField(choices=CATEGORY_CHOICES)\n    ingredients = models.TextField()\n    directions = models.TextField()\n    comments = models.TextField(null=True, blank=True)\n\n", "completion": "Edit: Updated in light of kibibu's correction.\n\nI have encountered what I believe is this same error, producing the message:\n\nCaught ValueError while rendering: too many values to unpack\n\n\nMy form class was as follows:\n\nclass CalcForm(forms.Form):\n    item = forms.ChoiceField(choices=(('17815', '17816')))\n\n\nNote that my choices type here a tuple. Django official documentation reads as follows for the choices arg:\n\n\n  An iterable (e.g., a list or tuple) of 2-tuples to use as choices for\n  this field. This argument accepts the same formats as the choices\n  argument to a model field.\n\n\nsrc: https://docs.djangoproject.com/en/1.3/ref/forms/fields/#django.forms.ChoiceField.choices\n\nThis problem was solved by my observing the documentation and using a list of tuples:\n\nclass CalcForm(forms.Form):\n    item = forms.ChoiceField(choices=[('17815', '17816')])\n\n\nDo note that while the docs state any iterable of the correct form can be used, a tuple of 2-tuples did not work:\n\nitem = forms.ChoiceField(choices=(('17815', '17816'), ('123', '456')))\n\n\nThis produced the same error as before.\n\nLesson: bugs happen.\n"}
{"prompt": "I have an issue with the django.contrib.auth User model where the email max_length is 75.\n\nI am receiving email addresses that are longer than 75 characters from the facebook api, and I need to (would really like to) store them in the user for continuity among users that are from facebook connect and others.\n\nI am able to solve the problem of \"Data truncated for column 'email' at row 1\" by manually going editing the field in our mySql database, but is there a better way to solve this? preferably one that does not involve me manually editing the database every time I reset it for a schema change?\n\nI am ok with editing the database as long as I can add it to the reset script, or the initial_data.json file.\n", "completion": "EmailField 75 chars length is hardcoded in django. You can fix this like that:\n\nfrom django.db.models.fields import EmailField\ndef email_field_init(self, *args, **kwargs):\n  kwargs['max_length'] = kwargs.get('max_length', 200)\n  CharField.__init__(self, *args, **kwargs)\nEmailField.__init__ = email_field_init\n\n\nbut this will change ALL EmailField fields lengths, so you could also try:\n\nfrom django.contrib.auth.models import User\nfrom django.utils.translation import ugettext as _\nfrom django.db import models\nUser.email = models.EmailField(_('e-mail address'), blank=True, max_length=200)\n\n\nboth ways it'd be best to put this code in init of any module BEFORE django.contrib.auth in your INSTALLED_APPS\n"}
{"prompt": "What alternatives are there to pylab for plotting in Python? In particular, I'm looking for something that doesn't use the stateful model that pylab does.\n", "completion": "Plotly lets you make graphs using a Python API, matplotlib, and pandas. Their IPython gallery has some example scientific graphs with the Python scripts that generated them. \n\nHere's a sample: \n\n\n\nSome recent exciting open source offerings:\n\n\nggplot is based on R's ggplot2, with aesthetically pleasing defaults and a really concise api. wants to be a matplotlib killer\n\n\n\n\n\nbokeh makes interactive (html canvas) plots. emphasis on interativity + handling big data\n\n\n\n\n\nvega translates JSON \"plot descriptions\" into SVG or Canvas-based interactive plots, and vincent is a declarative interface for generating the JSON specifications.\n\n\n\n"}
{"prompt": "In a similar way to using varargs in C or C++:\n\nfn(a, b)\nfn(a, b, c, d, ...)\n\n", "completion": "Yes.\n\nThis is simple and works if you disregard keyword arguments:\n\ndef manyArgs(*arg):\n  print \"I was called with\", len(arg), \"arguments:\", arg\n\n>>> manyArgs(1)\nI was called with 1 arguments: (1,)\n>>> manyArgs(1, 2,3)\nI was called with 3 arguments: (1, 2, 3)\n\n\nAs you can see, Python will give you a single tuple with all the arguments.\n\nFor keyword arguments you need to accept those as a separate actual argument, as shown in Skurmedel's answer.\n"}
{"prompt": "I've been playing around with the email module in python but I want to be able to know how to embed images which are included in the html.\n\nSo for example if the body is something like\n\n<img src=\"../path/image.png\"></img>\n\n\nI would like to embed image.png into the email, and the src attribute should be replaced with content-id. Does anybody know how to do this?\n", "completion": "Here is an example I found.\n\n\n  Recipe 473810: Send an HTML email with embedded image and plain text alternate:  \n  \n  HTML is the method of choice for those\n  wishing to send emails with rich text,\n  layout and graphics. Often it is\n  desirable to embed the graphics within\n  the message so recipients can display\n  the message directly, without further\n  downloads.\n  \n  Some mail agents don't support HTML or\n  their users prefer to receive plain\n  text messages. Senders of HTML\n  messages should include a plain text\n  message as an alternate for these\n  users.\n  \n  This recipe sends a short HTML message\n  with a single embedded image and an\n  alternate plain text message.\n\n\n# Send an HTML email with an embedded image and a plain text message for\n# email clients that don't want to display the HTML.\n\nfrom email.MIMEMultipart import MIMEMultipart\nfrom email.MIMEText import MIMEText\nfrom email.MIMEImage import MIMEImage\n\n# Define these once; use them twice!\nstrFrom = 'from@example.com'\nstrTo = 'to@example.com'\n\n# Create the root message and fill in the from, to, and subject headers\nmsgRoot = MIMEMultipart('related')\nmsgRoot['Subject'] = 'test message'\nmsgRoot['From'] = strFrom\nmsgRoot['To'] = strTo\nmsgRoot.preamble = 'This is a multi-part message in MIME format.'\n\n# Encapsulate the plain and HTML versions of the message body in an\n# 'alternative' part, so message agents can decide which they want to display.\nmsgAlternative = MIMEMultipart('alternative')\nmsgRoot.attach(msgAlternative)\n\nmsgText = MIMEText('This is the alternative plain text message.')\nmsgAlternative.attach(msgText)\n\n# We reference the image in the IMG SRC attribute by the ID we give it below\nmsgText = MIMEText('<b>Some <i>HTML</i> text</b> and an image.<br><img src=\"cid:image1\"><br>Nifty!', 'html')\nmsgAlternative.attach(msgText)\n\n# This example assumes the image is in the current directory\nfp = open('test.jpg', 'rb')\nmsgImage = MIMEImage(fp.read())\nfp.close()\n\n# Define the image's ID as referenced above\nmsgImage.add_header('Content-ID', '<image1>')\nmsgRoot.attach(msgImage)\n\n# Send the email (this example assumes SMTP authentication is required)\nimport smtplib\nsmtp = smtplib.SMTP()\nsmtp.connect('smtp.example.com')\nsmtp.login('exampleuser', 'examplepass')\nsmtp.sendmail(strFrom, strTo, msgRoot.as_string())\nsmtp.quit()\n\n"}
{"prompt": "I've seen decorators that let you mark a function a deprecated so that a warning is given whenever that function is used. I'd like to do the same thing but for a global variable, but I can't think of a way to detect global variable accesses. I know about the globals() function, and I could check its contents, but that would just tell me if the global is defined (which it still will be if the function is deprecated and not all out removed) not if it's actually being used. The best alternative I can think of is something like this:\n\n# myglobal = 3\nmyglobal = DEPRECATED(3)\n\n\nBut besides the problem of how to get DEPRECATED to act exactly like a '3', I'm not sure what DEPRECATED could do that would let you detect every time it's accessed. I think the best it could do is iterate through all of the global's methods (since everything in Python is an object, so even '3' has methods, for converting to string and the like) and 'decorate' them to all be deprecated. But that's not ideal.\n\nAny ideas? Has anyone else tackled this problem?\n", "completion": "You can't do this directly, since theres no way of intercepting the module access.  However, you can replace that module with an object of your choosing that acts as a proxy, looking for accesses to certain properties:\n\nimport sys, warnings\n\ndef WrapMod(mod, deprecated):\n    \"\"\"Return a wrapped object that warns about deprecated accesses\"\"\"\n    deprecated = set(deprecated)\n    class Wrapper(object):\n        def __getattr__(self, attr):\n            if attr in deprecated:\n                warnings.warn(\"Property %s is deprecated\" % attr)\n\n            return getattr(mod, attr)\n\n        def __setattr__(self, attr, value):\n            if attr in deprecated:\n                warnings.warn(\"Property %s is deprecated\" % attr)\n            return setattr(mod, attr, value)\n    return Wrapper()\n\noldVal = 6*9\nnewVal = 42\n\nsys.modules[__name__] = WrapMod(sys.modules[__name__], \n                         deprecated = ['oldVal'])\n\n\nNow, you can use it as:\n\n>>> import mod1\n>>> mod1.newVal\n42\n>>> mod1.oldVal\nmod1.py:11: UserWarning: Property oldVal is deprecated\n  warnings.warn(\"Property %s is deprecated\" % attr)\n54\n\n\nThe downside is that you are now performing two lookups when you access the module, so there is a slight performance hit.\n"}
{"prompt": "Referring on this question, I have a similar -but not the same- problem..\n\nOn my way, I'll have some text file, structured like:\n\nvar_a: 'home'\nvar_b: 'car'\nvar_c: 15.5\n\n\nAnd I need that python read the file and then create a variable named var_a with value 'home', and so on.\n\nExample:\n\n#python stuff over here\ngetVarFromFile(filename) #this is the function that im looking for\nprint var_b\n#output: car, as string\nprint var_c\n#output 15.5, as number.\n\n\nIs this possible, I mean, even keep the var type?\n\nNotice that I have the full freedom to the text file structure, I can use the format I like if the one I proposed isn't the best.\n\nEDIT: the ConfigParser can be a solution, but I don't like it so much, because in my script I'll have then to refer to the variables in the file with\n\nconfig.get(\"set\", \"var_name\")\n\n\nBut what I'll love is to refer to the variable directly, as I declared it in the python script...\n\nThere is a way to import the file as a python dictionary?\n\nOh, last thing, keep in mind that I don't know exactly how many variables would I have in the text file.\n\nEdit 2: I'm very interested at stephan's JSON solution, because in that way the text file could be read simply with others languages (PHP, then via AJAX JavaScript, for example), but I fail in something while acting that solution:\n\n#for the example, i dont load the file but create a var with the supposed file content\nfile_content = \"'var_a': 4, 'var_b': 'a string'\"\nmydict = dict(file_content)\n#Error: ValueError: dictionary update sequence element #0 has length 1; 2 is required\nfile_content_2 = \"{'var_a': 4, 'var_b': 'a string'}\"\nmydict_2 = dict(json.dump(file_content_2, True))\n#Error:\n#Traceback (most recent call last):\n#File \"<pyshell#5>\", line 1, in <module>\n#mydict_2 = dict(json.dump(file_content_2, True))\n#File \"C:\\Python26\\lib\\json\\__init__.py\", line 181, in dump\n#fp.write(chunk)\n#AttributeError: 'bool' object has no attribute 'write'\n\n\nIn what kind of issues can I fall with the JSON format?\nAnd, how can I read a JSON array in a text file, and transform it in a python dict?\n\nP.S: I don't like the solution using .py files; I'll prefer .txt, .inc, .whatever is not restrictive to one language.\n", "completion": "\n  But what i'll love is to refer to the variable direclty, as i declared it in the python script..\n\n\nAssuming you're happy to change your syntax slightly, just use python and import the \"config\" module.\n\n# myconfig.py:\n\nvar_a = 'home'\nvar_b = 'car'\nvar_c = 15.5\n\n\nThen do \n\nfrom myconfig import *\n\n\nAnd you can reference them by name in your current context.\n"}
{"prompt": "This sample code works (I can write something in the file):\n\nfrom multiprocessing import Process, Queue\n\nqueue = Queue()\ndef _printer(self, queue):\n    queue.put(\"hello world!!\")\n\ndef _cmdDisp(self, queue):\n    f = file(\"Cmd.log\", \"w\")\n    print >> f, queue.get()\n    f.close()\n\n\ninstead this other sample not: (errormsg: 'module' object is not callable)\n\nimport Queue\n\nqueue = Queue()\ndef _printer(self, queue):\n    queue.put(\"hello world!!\")\n\ndef _cmdDisp(self, queue):\n    f = file(\"Cmd.log\", \"w\")\n    print >> f, queue.get()\n    f.close()\n\n\nthis other sample not (I cannot write something in the file):\n\nimport Queue\n\nqueue = Queue.Queue()\ndef _printer(self, queue):\n    queue.put(\"hello world!!\")\n\ndef _cmdDisp(self, queue):\n    f = file(\"Cmd.log\", \"w\")\n    print >> f, queue.get()\n    f.close()\n\n\nCan someone explain the differences? and the right to do?\n", "completion": "For your second example, you already gave the explanation yourself---Queue is a module, which cannot be called.\n\nFor the third example: I assume that you use Queue.Queue together with multiprocessing.  A Queue.Queue will not be shared between processes.  If the Queue.Queue is declared before the processes then each process will receive a copy of it which is then independent of every other process.  Items placed in the Queue.Queue by the parent before starting the children will be available to each child.  Items placed in the Queue.Queue by the parent after starting the child will only be available to the parent. Queue.Queue is made for data interchange between different threads inside the same process (using the threading module). The multiprocessing queues are for data interchange between different Python processes. While the API looks similar (it's designed to be that way), the underlying mechanisms are fundamentally different.\n\n\nmultiprocessing queues exchange data by pickling (serializing) objects and sending them through pipes. \nQueue.Queue uses a data structure that is shared between threads and locks/mutexes for correct behaviour.\n\n"}
{"prompt": "What's the best way to parse messages received from an IRC server with Python according to the RFC? I simply want some kind of list/whatever, for example:\n\n:test!~test@test.com PRIVMSG #channel :Hi!\n\n\nbecomes this:\n\n{ \"sender\" : \"test!~test@test.com\", \"target\" : \"#channel\", \"message\" : \"Hi!\" }\n\n\nAnd so on?\n\n(Edit: I want to parse IRC messages in general, not just PRIVMSG's)\n", "completion": "Look at Twisted's implementation http://twistedmatrix.com/\n\nUnfortunately I'm out of time, maybe someone else can paste it here for you.\n\nEdit\n\nWell I'm back, and strangely no one has pasted it yet so here it is:\n\nhttp://twistedmatrix.com/trac/browser/trunk/twisted/words/protocols/irc.py#54\n\ndef parsemsg(s):\n    \"\"\"Breaks a message from an IRC server into its prefix, command, and arguments.\n    \"\"\"\n    prefix = ''\n    trailing = []\n    if not s:\n       raise IRCBadMessage(\"Empty line.\")\n    if s[0] == ':':\n        prefix, s = s[1:].split(' ', 1)\n    if s.find(' :') != -1:\n        s, trailing = s.split(' :', 1)\n        args = s.split()\n        args.append(trailing)\n    else:\n        args = s.split()\n    command = args.pop(0)\n    return prefix, command, args\n\nparsemsg(\":test!~test@test.com PRIVMSG #channel :Hi!\")\n# ('test!~test@test.com', 'PRIVMSG', ['#channel', 'Hi!'])\n\n\nThis function closely follows the EBNF described in the IRC RFC.\n"}
{"prompt": "I'm really confused with the codecs.open function. When I do:\n\nfile = codecs.open(\"temp\", \"w\", \"utf-8\")\nfile.write(codecs.BOM_UTF8)\nfile.close()\n\n\nIt gives me the error\n\n\n  UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position\n  0: ordinal not in range(128)\n\n\nIf I do:\n\nfile = open(\"temp\", \"w\")\nfile.write(codecs.BOM_UTF8)\nfile.close()\n\n\nIt works fine.\n\nQuestion is why does the first method fail? And how do I insert the bom?\n\nIf the second method is the correct way of doing it, what the point of using codecs.open(filename, \"w\", \"utf-8\")?\n", "completion": "Disclaimer: I'm not a Python programmer.\n\nI believe the problem is that codecs.BOM_UTF8 is a byte string, not a Unicode string. I suspect the file handler is trying to guess what you really mean based on \"I'm meant to be writing Unicode as UTF-8-encoded text, but you've given me a byte string!\"\n\nTry writing the Unicode string for the byte order mark (i.e. Unicode U+FEFF) directly, so that the file just encodes that as UTF-8:\n\nimport codecs\n\nfile = codecs.open(\"lol\", \"w\", \"utf-8\")\nfile.write(u'\\ufeff')\nfile.close()\n\n\n(That seems to give the right answer - a file with bytes EF BB BF.)\n\nEDIT: S. Lott's suggestion of using \"utf-8-sig\" as the encoding is a better one than explicitly writing the BOM yourself, but I'll leave this answer here as it explains what was going wrong before.\n"}
{"prompt": "So right now i need to create and implement an extension of the Python logging module that will be used to log to our database. Basically we have several python applications(that all run in the background) that currently log to a random mishmash of text files. Which makes it almost impossible to find out if a certain application failed or not. \n\nThe problem given to me is to move said logging to text files to an oracle DB. The tables have already been defined, and where things need to be logged to but right now, im looking at adding another logging handler that will log to the DB. \n\nI am using python 2.5.4 and cx_Oracle and the applications in general can be ether run as a service/daemon or a straight application.\n\nI'm just mainly curious about what would be the best possible way to go about this. Few questions:\n\n\nIf any errors occur with cx_Oracle, where should these errors be logged to? If its down would it be best to just go and have the logger retreat to the default text file?\nAwhile back we started enforcing that people use sys.stderr/stdout.write instead of print, so worst case scenario we wouldn't run into any issues with print becoming deprecated. Is there a way to seamlessly make all of the thousands of sys.std calls be piped directly into the logger, and have the logger pickup the slack?\nAfter every logged message, should the script automatically do a commit? (there's going to be several dozen a second.)\nWhat is the best way to implement a new handler for the logging system? Inheriting from the basic Handler class seems to be easiest. \n\n\nAny ideas / suggestions would be great. \n", "completion": "\nIf errors occur with cx_Oracle, it's probably best to log these to a text file.\nYou could try redirecting sys.stdout and sys.stderr to file-like objects which log whatever's written to them to a logger.\nI would guess you do want to commit after each event, unless you have strong reasons for not doing this. Alternatively, you can buffer several events and write them all in a single transaction every so often.\nBelow is an example which uses mx.ODBC, you can probably adapt this to cx_Oracle without too much trouble. It's meant to be Python DB-API 2.0 compliant, I think.\n\n\nThe standalone Python logging distribution (before logging was added to Python) is at http://www.red-dove.com/python_logging.html and although the logging package in Python is much more up to date, the standalone distribution contains a test directory which has a lot of useful examples of derived handler classes.\n\n#!/usr/bin/env python\n#\n# Copyright 2001-2009 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n#\n# This file is part of the standalone Python logging distribution. See\n# http://www.red-dove.com/python_logging.html\n#\n\"\"\"\nA test harness for the logging module. An example handler - DBHandler -\nwhich writes to an Python DB API 2.0 data source. You'll need to set this\nsource up before you run the test.\n\nCopyright (C) 2001-2009 Vinay Sajip. All Rights Reserved.\n\"\"\"\nimport sys, string, time, logging\n\nclass DBHandler(logging.Handler):\n    def __init__(self, dsn, uid='', pwd=''):\n        logging.Handler.__init__(self)\n        import mx.ODBC.Windows\n        self.dsn = dsn\n        self.uid = uid\n        self.pwd = pwd\n        self.conn = mx.ODBC.Windows.connect(self.dsn, self.uid, self.pwd)\n        self.SQL = \"\"\"INSERT INTO Events (\n                        Created,\n                        RelativeCreated,\n                        Name,\n                        LogLevel,\n                        LevelText,\n                        Message,\n                        Filename,\n                        Pathname,\n                        Lineno,\n                        Milliseconds,\n                        Exception,\n                        Thread\n                   )\n                   VALUES (\n                        %(dbtime)s,\n                        %(relativeCreated)d,\n                        '%(name)s',\n                        %(levelno)d,\n                        '%(levelname)s',\n                        '%(message)s',\n                        '%(filename)s',\n                        '%(pathname)s',\n                        %(lineno)d,\n                        %(msecs)d,\n                        '%(exc_text)s',\n                        '%(thread)s'\n                   );\n                   \"\"\"\n        self.cursor = self.conn.cursor()\n\n    def formatDBTime(self, record):\n        record.dbtime = time.strftime(\"#%m/%d/%Y#\", time.localtime(record.created))\n\n    def emit(self, record):\n        try:\n            #use default formatting\n            self.format(record)\n            #now set the database time up\n            self.formatDBTime(record)\n            if record.exc_info:\n                record.exc_text = logging._defaultFormatter.formatException(record.exc_info)\n            else:\n                record.exc_text = \"\"\n            sql = self.SQL % record.__dict__\n            self.cursor.execute(sql)\n            self.conn.commit()\n        except:\n            import traceback\n            ei = sys.exc_info()\n            traceback.print_exception(ei[0], ei[1], ei[2], None, sys.stderr)\n            del ei\n\n    def close(self):\n        self.cursor.close()\n        self.conn.close()\n        logging.Handler.close(self)\n\ndh = DBHandler('Logging')\nlogger = logging.getLogger(\"\")\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(dh)\nlogger.info(\"Jackdaws love my big %s of %s\", \"sphinx\", \"quartz\")\nlogger.debug(\"Pack my %s with five dozen %s\", \"box\", \"liquor jugs\")\ntry:\n    import math\n    math.exp(1000)\nexcept:\n    logger.exception(\"Problem with %s\", \"math.exp\")\n\n"}
{"prompt": "I'm fairly new to Python and recursive functions as a whole, so pardon my ignorance.\n\nI am trying to implement a binary search tree in Python and have the following insert method (taken out of a class):\n\ndef insert(self, key, root=None):\n    '''Inserts a node in the tree'''\n    if root == None:\n        root = self.root\n    if root.key == None:\n        self._update(root, key)\n        return 0\n    else:\n        tmp = root\n        if key > tmp.key: # we work with the right subtree\n            self.insert(key, root=tmp.right)\n        elif key < tmp.key: # we work with the left subtree\n            self.insert(key, root=tmp.left)\n        else: # key already exists\n            return 0\n\n\nI'm not sure if this is legible, but it traverses the tree until it gets to a None value and updates the node with the key to insert.\n\nNow, the method works nicely and correctly creates a BST from scratch. But there's a problem with the return statements, as it only returns 0 if there is no recursion performed.\n\n>>> bst.insert(10)\n0\n>>> bst.insert(15)\n>>> bst.root.right.key\n15\n>>>\n\n\n\"Inserting\" the root key again returns 0 (from line 15) the way it should.\n\n>>> bst.insert(10)\n0\n\n\nI can't figure out why this happens. If I put a print statement in line 6, it executes correctly, yet it just won't return anything past the first insertion. Why is this? (I'm pretty sure I'm missing some basic information regarding Python and recursion)\n\nThanks for your help,\n\nIvan\n\nP.S.: I've read that recursion is not the best way to implement a BST, so I'll look into other solutions, but I'd like to know the answer to this before moving on.\n", "completion": "On your recursive lines, you do not return anything.  If you want it to return 0, you should replace them with lines like:\n\nreturn self.insert(key, root=tmp.left)\n\n\ninstead of just\n\nself.insert(key, root=tmp.left)\n\n"}
{"prompt": "I'm guessing this is going to involve regexp or something, but I'll give it  a shot. At the minute, a user can break a website by typing something similar to \u00c2\u00a3$(*\u00c2\u00a3$(\u00c2\u00a3@$&\u00c2\u00a3($ in the title field, which is converted into a slug using Django slugify.\n\nBecause none of these characters can be converted, Django returns an error. My question is, what should I put in the form validation method to raise a forms.ValidationError when the user uses a title like this?\n\nThanks.\n", "completion": "SLUG_REGEX = re.compile('^[-\\w]+$')\n"}
{"prompt": "The Development version of Django has aggregate functions like Avg, Count, Max, Min, StdDev, Sum, and Variance (link text). Is there a reason Median is missing from the list? \n\nImplementing one seems like it would be easy. Am I missing something? How much are the aggregate functions doing behind the scenes?\n", "completion": "Because median isn't a SQL aggregate. See, for example, the list of PostgreSQL aggregate functions and the list of MySQL aggregate functions.\n"}
{"prompt": "float('nan') results in a thingy simply called nan. But how do I check for it? Should be very easy, but i cannot find it.\n", "completion": "math.isnan()\n\n\n  Checks if the float x is a NaN (not a number). NaNs are part of the IEEE 754 standards. Operation like but not limited to inf * 0, inf / inf or any operation involving a NaN, e.g. nan * 1, return a NaN.\n  \n  New in version 2.6.\n\n\n>>> import math\n>>> x=float('nan')\n>>> math.isnan(x)\nTrue\n>>>\n\n"}
{"prompt": "I managed to get a set of images loaded using Python. \n\nI'd like my script to take this series of images (in whatever format I need them), and create a video from them. The big limit in all this is that I am looking for something easy and simple to install. Ideally, using the standard OS X installation procedure: \n\n\ndownload .dmg\nclick\nmove into the application folder\n\n\nI do not want to expend a lot of effort to install the video editing program. Just something simple that works.\n\n\n\nQuestions\n\n\nWhat format should I aim for? I need my video to be playable on Linux, Mac, and Windows systems. The images are graphs, so we are speaking of discreet images, not photographs. It should be pretty easy to compress it. There will be about 1000 images, so this will be a short movie.\nWhat tools should I use to produce the actual video? I need to either do it directly from Python using a library designed for this purpose, or by scripting command-line tools called from Python. \n\n", "completion": "If you're not averse to using the command-line, there's the convert command from the ImageMagick package. It's available for Mac, Linux, Windows. See http://www.imagemagick.org/script/index.php.\n\nIt supports a huge number of image formats and you can output your movie as an mpeg file:\n\nconvert -quality 100 *.png outvideo.mpeg\n\n\nor as animated gifs for uploading to webpages:\n\nconvert -set delay 3 -loop 0 -scale 50% *.png animation.gif\n\n\nMore options for the convert command available here: ImageMagick v6 Examples -\nAnimation Basics\n"}
{"prompt": "Are there any equivalent Javascript functions for Python's urllib.quote() and urllib.unquote()?\n\nThe closest I've come across are escape(), encodeURI(), and encodeURIComponent() (and their corresponding un-encoding functions), but they don't encode/decode the same set of special characters as far as I can tell.\n\nThanks,\nCameron\n", "completion": "For the record:\n\nJavaScript               |  Python\n----------------------------------- \nencodeURI(str)           |  urllib.quote(str, safe='~@#$&()*!+=:;,.?/\\'');\n-----------------------------------\nencodeURIComponent(str)  |  urllib.quote(str, safe='~()*!.\\'')\n\n"}
{"prompt": "Python's list type has an index() method that takes one parameter and returns the index of the first item in the list matching the parameter.  For instance:\n\n>>> some_list = [\"apple\", \"pear\", \"banana\", \"grape\"]\n>>> some_list.index(\"pear\")\n1\n>>> some_list.index(\"grape\")\n3\n\n\nIs there a graceful (idiomatic) way to extend this to lists of complex objects, like tuples?  Ideally, I'd like to be able to do something like this:\n\n>>> tuple_list = [(\"pineapple\", 5), (\"cherry\", 7), (\"kumquat\", 3), (\"plum\", 11)]\n>>> some_list.getIndexOfTuple(1, 7)\n1\n>>> some_list.getIndexOfTuple(0, \"kumquat\")\n2\n\n\ngetIndexOfTuple() is just a hypothetical method that accepts a sub-index and a value, and then returns the index of the list item with the given value at that sub-index.  I hope\n\nIs there some way to achieve that general result, using list comprehensions or lambas or something \"in-line\" like that?  I think I could write my own class and method, but I don't want to reinvent the wheel if Python already has a way to do it.\n", "completion": "How about this?\n\n>>> tuple_list = [(\"pineapple\", 5), (\"cherry\", 7), (\"kumquat\", 3), (\"plum\", 11)]\n>>> [x for x, y in enumerate(tuple_list) if y[1] == 7]\n[1]\n>>> [x for x, y in enumerate(tuple_list) if y[0] == 'kumquat']\n[2]\n\n\nAs pointed out in the comments, this would get all matches. To just get the first one, you can do:\n\n>>> [y[0] for y in tuple_list].index('kumquat')\n2\n\n\nThere is a good discussion in the comments as to the speed difference between all the solutions posted. I may be a little biased but I would personally stick to a one-liner as the speed we're talking about is pretty insignificant versus creating functions and importing modules for this problem, but if you are planning on doing this to a very large amount of elements you might want to look at the other answers provided, as they are faster than what I provided.\n"}
{"prompt": "I find myself frequently using Python's interpreter to work with databases, files, etc -- basically a lot of manual formatting of semi-structured data.  I don't properly save and clean up the useful bits as often as I would like.  Is there a way to save my input into the shell (db connections, variable assignments, little for loops and bits of logic) -- some history of the interactive session?  If I use something like script I get too much stdout noise.  I don't really need to pickle all the objects -- though if there is a solution that does that, it would be OK.  Ideally I would just be left with a script that ran as the one I created interactively, and I could just delete the bits I didn't need.  Is there a package that does this, or a DIY approach?\n\nUPDATE:  I am really amazed at the quality and usefulness of these packages.  For those with a similar itch:\n\n\nIPython -- should have been using this for ages, kind of what I had in mind\nreinteract -- very impressive, I want to learn more about visualization and this seems like it will shine there.  Sort of a gtk/gnome desktop app that renders graphs inline.  Imagine a hybrid shell + graphing calculator + mini eclipse.  Source distribution here: http://www.reinteract.org/trac/wiki/GettingIt . Built fine on Ubuntu, integrates into gnome desktop, Windows and Mac installers too.\nbpython -- extremely cool, lots of nice features, autocomplete(!), rewind, one keystroke save to file, indentation, well done.  Python source distribution, pulled a couple of dependencies from sourceforge.\n\n\nI am converted, these really fill a need between interpreter and editor.\n", "completion": "IPython is extremely useful if you like using interactive sessions. For example for your usecase there is the %save magic command, you just input %save my_useful_session 10-20 23 to save input lines 10 to 20 and 23 to my_useful_session.py. (to help with this, every line is prefixed by its number)\n\nLook at the videos on the documentation page to get a quick overview of the features.\n"}
{"prompt": "In python it's possible to use '.' in order to access object's dictionary items. For example:\n\nclass test( object ) :\n  def __init__( self ) :\n    self.b = 1\n  def foo( self ) :\n    pass\nobj = test()\na = obj.foo\n\n\nFrom above example, having 'a' object, is it possible to get from it reference to 'obj' that is a parent namespace for 'foo' method assigned? For example, to change obj.b into 2?\n", "completion": "On bound methods, you can use three special read-only parameters:\n\n\nim_func which returns the (unbound) function object\nim_self which returns the object the function is bound to (class instance)\nim_class which returns the class of im_self\n\n\nTesting around:\n\nclass Test(object):\n    def foo(self):\n        pass\n\ninstance = Test()\ninstance.foo          # <bound method Test.foo of <__main__.Test object at 0x1>>\ninstance.foo.im_func  # <function foo at 0x2>\ninstance.foo.im_self  # <__main__.Test object at 0x1>\ninstance.foo.im_class # <__main__.Test class at 0x3>\n\n# A few remarks\ninstance.foo.im_self.__class__ == instance.foo.im_class # True\ninstance.foo.__name__ == instance.foo.im_func.__name__  # True\ninstance.foo.__doc__ == instance.foo.im_func.__doc__    # True\n\n# Now, note this:\nTest.foo.im_func != Test.foo # unbound method vs function\nTest.foo.im_self is None\n\n# Let's play with classmethods\nclass Extend(Test):\n    @classmethod\n    def bar(cls): \n        pass\n\nextended = Extend()\n\n# Be careful! Because it's a class method, the class is returned, not the instance\nextended.bar.im_self # <__main__.Extend class at ...>\n\n\nThere is an interesting thing to note here, that gives you a hint on how the methods are being called:\n\nclass Hint(object):\n    def foo(self, *args, **kwargs):\n        pass\n\n    @classmethod\n    def bar(cls, *args, **kwargs):\n        pass\n\ninstance = Hint()\n\n# this will work with both class methods and instance methods:\nfor name in ['foo', 'bar']:\n    method = instance.__getattribute__(name)\n    # call the method\n    method.im_func(method.im_self, 1, 2, 3, fruit='banana')\n\n\nBasically, im_self attribute of a bound method changes, to allow using it as the first parameter when calling im_func\n"}
{"prompt": "Is there anyway I can zip dynamically generated content, such as a freshly rendered html template, into a zip file using zipfile?\n\nThere seem to be some examples around for zipping static content, but none for zipping dynamic ones. Or, is it not possible at all?\n\nOne more question: Is it possible to create a zip file with a bunch of sub-folders inside it?\n\nThanks.\n", "completion": "The working code: (for app engine:)\n\noutput = StringIO.StringIO()\nz = zipfile.ZipFile(output,'w')\nmy_data = \"<html><body><p>Hello, world!</p></body></html>\"\nz.writestr(\"hello.html\", my_data)\nz.close()\n\nself.response.headers[\"Content-Type\"] = \"multipart/x-zip\"\nself.response.headers['Content-Disposition'] = \"attachment; filename=test.zip\"\nself.response.out.write(output.getvalue())\n\n\nThanks again to Schnouki and Ryan.\n"}
{"prompt": "I'm trying to read lines from a pipe and process them, but I'm doing something silly and I can't figure out what. The producer is going to keep producing lines indefinitely, like this:\n\nproducer.py\n\nimport time\n\nwhile True:\n    print 'Data'\n    time.sleep(1)\n\n\nThe consumer just needs to check for lines periodically:\n\nconsumer.py\n\nimport sys, time\nwhile True:\n    line = sys.stdin.readline()\n    if line:\n        print 'Got data:', line\n    else:\n        time.sleep(1)\n\n\nWhen I run this in the Windows shell as python producer.py | python consumer.py, it just sleeps forever (never seems to get data?) It seems that maybe the problem is that the producer never terminates, since if I send a finite amount of data then it works fine.\n\nHow can I get the data to be received and show up for the consumer? In the real application, the producer is a C++ program I have no control over.\n", "completion": "Some old versions of Windows simulated pipes through files (so they were prone to such problems), but that hasn't been a problem in 10+ years.  Try adding a\n\n  sys.stdout.flush()\n\n\nto the producer after the print, and also try to make the producer's stdout unbuffered (by using python -u).\n\nOf course this doesn't help if you have no control over the producer -- if it buffers too much of its output you're still going to wait a long time.\n\nUnfortunately - while there are many approaches to solve that problem on Unix-like operating systems, such as pyexpect, pexpect, exscript, and paramiko, I doubt any of them works on Windows; if that's indeed the case, I'd try Cygwin, which puts enough of a Linux-like veneer on Windows as to often enable the use of Linux-like approaches on a Windows box.\n"}
{"prompt": "I have been in love with zsh for a long time, and more recently I have been discovering the advantages of the ipython interactive interpreter over python itself. Being able to cd, to ls, to run or to ! is indeed very handy. But now it feels weird to have such a clumsy shell when in ipython, and I wonder how I could integrate my zsh and my ipython better.\n\nOf course, I could rewrite my .zshrc and all my scripts in python, and emulate most of my shell world from ipython, but it doesn't feel right. And I am obviously not ready to use ipython as a main shell anyway.\n\nSo, here comes my question: how do you work efficiently between your shell and your python command-loop ? Am I missing some obvious integration strategy ? Should I do all that in emacs ? \n", "completion": "I asked this question on the zsh list and this answer worked for me. YMMV.\n\nIn genutils.py after the line \n\n\n  if not debug:\n\n\nRemove the line:\n\n\n  stat = os.system(cmd)\n\n\nReplace it with:\n\n\n  stat =\n  subprocess.call(cmd,shell=True,executable='/bin/zsh')\n\n\nyou see, the problem is that that \"!\" call uses os.system to run it, which defaults to manky old /bin/sh .\n\nLike I said, it worked for me, although I'm not sure what got borked behind the scenes.\n"}
{"prompt": "I'm trying to figure out how to redirect output from some FORTRAN code for which I've generated a Python interface by using F2PY. I've tried:\n\nfrom fortran_code import fortran_function\nstdout_holder = sys.stdout\nstderr_holder = sys.stderr\nsys.stdout = file(\"/dev/null\",\"w\")\nfortran_function()\nsys.stdout.close()\nsys.stderr.close()\nsys.stdout = stdout_holder\nsys.stderr = stderr_holder\n\n\nThis is the de facto method of redirecting output in Python, but it doesn't seem to work in this case (i.e., the output is displayed anyway).\n\nI did find a mailing list post from 2002 saying that \"It is possible to read messages from pts devices, e.g. ttysnoop does this\". Information on ttysnoop seems to be pretty difficult to find online (I don't think it's been updated in quite a few years; for example, the first result on Google for \"ttysnoop\" has only dead links to tarballs, RPMs, and .deb's), and this request for a port to OS X received the response \"No luck, it requires some linux specific utmp functions which I can't create.\"\n\nI'm open to any suggestions on how to redirect the output (it doesn't have to use ttysnoop).\n\nThanks!\n", "completion": "The stdin and stdout fds are being inherited by the C shared library.\n\nfrom fortran_code import fortran_function\nimport os\n\nprint \"will run fortran function!\"\n\n# open 2 fds\nnull_fds = [os.open(os.devnull, os.O_RDWR) for x in xrange(2)]\n# save the current file descriptors to a tuple\nsave = os.dup(1), os.dup(2)\n# put /dev/null fds on 1 and 2\nos.dup2(null_fds[0], 1)\nos.dup2(null_fds[1], 2)\n\n# *** run the function ***\nfortran_function()\n\n# restore file descriptors so I can print the results\nos.dup2(save[0], 1)\nos.dup2(save[1], 2)\n# close the temporary fds\nos.close(null_fds[0])\nos.close(null_fds[1])\n\nprint \"done!\"\n\n"}
{"prompt": "Can Windows drivers be written in Python?\n", "completion": "Yes. You cannot create the \"classic\" kernel-mode drivers. However, starting with XP, Windows offers a User-Mode Driver Framework. They can't do everything, obviously - any driver used in booting the OS obviously has to be kernel-mode. But with UMDF, you only need to implement COM components. \n\nBesides boot-time drivers, you also can't write UMDF drivers that:\n\n\nHandle interrupts\nDirectly access hardware, such as direct memory access (DMA)\nhave strict timing loops\nUse nonpaged pool or other resources that are reserved for kernel mode\n\n"}
{"prompt": "I have a password string that must be passed to a method. Everything works fine but I don't feel comfortable storing the password in clear text. Is there a way to obfuscate the string or to truly encrypt it? I'm aware that obfuscation can be reverse engineered, but I think I should at least try to cover up the password a bit. At the very least it wont be visible to a indexing program, or a stray eye giving a quick look at my code.\n\nI am aware of pyobfuscate but I don't want the whole program obfuscated, just one string and possibly the whole line itself where the variable is defined.\n\nTarget platform is GNU Linux Generic (If that makes a difference)\n", "completion": "If you just want to prevent casually glancing at a password, you may want to consider encoding/decoding the password to/from base64. It's not secure in the least, but the password won't be casually human/robot readable.\n\nimport base64\n# Encode password\nencoded_pw = base64.b64encode(raw_pw)\n\n# Decode password\ndecoded_pw = base64.b64decode(encoded_pw)\n\n"}
{"prompt": "I want to count the number of times each character is repeated in a string. Is there any particular way to do it apart from comparing each character of the string from A-Z\nand incrementing a counter?\n\nUpdate (in reference to Anthony's answer): Whatever you have suggested till now I have to write 26 times. Is there an easier way?\n", "completion": "import collections\n\nd = collections.defaultdict(int)\nfor c in thestring:\n    d[c] += 1\n\n\nA collections.defaultdict is like a dict (subclasses it, actually), but when an entry is sought and not found, instead of reporting it doesn't have it, it makes it and inserts it by calling the supplied 0-argument callable. Most popular are defaultdict(int), for counting (or, equivalently, to make a multiset AKA bag data structure), and defaultdict(list), which does away forever with the need to use .setdefault(akey, []).append(avalue) and similar awkward idioms.\n\nSo once you've done this d is a dict-like container mapping every character to the number of times it appears, and you can emit it any way you like, of course. For example, most-popular character first:\n\nfor c in sorted(d, key=d.get, reverse=True):\n  print '%s %6d' % (c, d[c])\n\n"}
{"prompt": "This is hopefully a quick/easy one. I know a way to work around this via a custom template tag, but I was curious if there were other methods I was over looking. I've created a gallery function of sorts for my blog, and I have a gallery list page that paginates all my galleries. Now, I don't want to show all the photos of each gallery in that list, since if each gallery even has 20 images, then that's 100 images on a page if I paginate at 5 posts. That'd be wasteful, and the wrong way to go about things.\n\nThe question I have is, is there a way to just display 3 photos from the photo set? What I'd like to do, but I don't think is possible is something like (pseudocode):\n\n{% for photos in gallery.photo_set %}\n   {% if forloop.counter lt 3 %}\n     <img src=\"{{ photos.url }}\">\n   {% endif %}\n{% endfor %}\n\n\nJudging from the documentation, unless I'm completely missing it, that's not possible via the templating system. Hence, I can just write my own template tag of sorts to work around it. I could probably do something from the view aspect, but I haven't looked to far into that idea. The other option I have is giving the model a preview field, and allow the user to select the photos they want in the preview field. \n\nAnyways, a few different options, so I thought I'd poll the audience to see how you'd do it. Any opinion is appreciated. Personally, enjoying that there's numerous ways to skin this cat.\n", "completion": "Use:\n\n{% for photos in gallery.photo_set|slice:\":3\" %}\n\n"}
{"prompt": "On this data:\n\n<row Id=\"37501\" PostId=\"135577\" Text=\"...uses though.&#x10;\"/>\n\n\nI'm getting an error with the Python sax parser:\n\nxml.sax._exceptions.SAXParseException:\ncomments.xml:29776:332: reference to invalid character number\n\n\nI trimmed the example; 332 points to \"&#x10;\".\n\nIs the parser correct in rejecting this character?\n", "completion": "As others have stated, you probably meant &#10;. The reason why &#x10; (0x10 = 10h = 16) is invalid is that it's explicitly excluded by the XML 1.0 standard: (http://www.w3.org/TR/xml/#NT-Char)\n\nChar ::= #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]\n\n"}
{"prompt": "I have just started with Python. When I execute a python script file on Windows, the output window appears but instantaneously goes away. I need it to stay there so I can analyze my output. How can I keep it open?\n", "completion": "You have a few options:\n\n\nRun the program from an already-open terminal. Open a command prompt and type:\n\npython myscript.py\n\n\nFor that to work you need the python executable in your path. Just check on how to edit environment variables on Windows, and add C:\\PYTHON26 (or whatever directory you installed python to).\n\nWhen the program ends, it'll drop you back to the cmd prompt instead of closing the window.\nAdd code to wait at the end of your script. Adding ...\n\nraw_input()\n\n\n... at the end of the script makes it wait for the Enter key. That method is annoying because you have to modify the script, and have to remember removing it when you're done. Specially annoying when testing other people's scripts.\nUse an editor that pauses for you. Some editors prepared for python will automatically pause for you after execution. Other editors allow you to configure the command line it uses to run your program. I find it particularly useful to configure it as \"python -i myscript.py\" when running. That drops you to a python shell after the end of the program, with the program environment loaded, so you may further play with the variables and call functions and methods.\n\n"}
{"prompt": "I was writing a python function that looked something like this\n\ndef foo(some_list):\n   for i in range(0, len(some_list)):\n       bar(some_list[i], i)\n\n\nso that it was called with\n\nx = [0, 1, 2, 3, ... ]\nfoo(x)\n\n\nI had assumed that index access of lists was O(1), but was surprised to find that for large lists this was significantly slower than I expected.\n\nMy question, then, is how are python lists are implemented, and what is the runtime complexity of the following\n\n\nIndexing: list[x]\nPopping from the end: list.pop()\nPopping from the beginning: list.pop(0)\nExtending the list: list.append(x)\n\n\nFor extra credit, splicing or arbitrary pops.\n", "completion": "there is a very detailed table on python wiki which answers your question.\n\nHowever, in  your particular example you should use enumerate to get an index of an iterable within a loop. like so:\n\nfor i, item in enumerate(some_seq):\n    bar(item, i)\n\n"}
{"prompt": "I am originally a C programmer. I have seen numerous tricks and \"hacks\" to read many different arguments. \n\nWhat are some of the ways Python programmers can do this?\n\nRelated\n\n\nWhat\u00e2\u0080\u0099s the best way to grab/parse command line arguments passed to a Python script?\nImplementing a \u00e2\u0080\u009c[command] [action] [parameter]\u00e2\u0080\u009d style command-line interfaces?\nHow can I process command line arguments in Python?\nHow do I format positional argument help using Python\u00e2\u0080\u0099s optparse?\n\n", "completion": "import sys\n\nprint \"\\n\".join(sys.argv)\n\n\nsys.argv is a list that contains all the arguments passed to the script on the command line.\n\nBasically,\n\nimport sys\nprint sys.argv[1:]\n\n"}
{"prompt": "I am trying to write a \"login_required\" decorator for the views in a WSGI+Werkzeug application.\n\nIn order to do this, I need to get at the user's session, which is accessible via the Request object that is passed into the view methods.\n\nI can't figure out how to get at that instance of Request in the decorator, though. I looked at PEP318, specifically the fourth example, but I'm not quite getting it.\n\nHere's what I'm trying:\n\ndef login_required(*args, **kw):\n    def goto_login(**kw):\n        return redirect(url_for('login'))\n\n    def decorate(f):\n        # args[0] should be request\n        args[0].client_session['test'] = True\n        logged_in = 0\n        if logged_in:\n            return f\n        else:\n            return goto_login\n    return decorate\n\n\n@login_required()\n@expose('/hello/<string:name>')\ndef hello(request, name):\n    return render_template('say_hello.html', name=name)\n\n\nbut I get an index out of bounds error trying to call args[0].\n\nIs there any way I can get access to the request argument passed into the \"hello\" function in the \"login_required\" decorator?\n", "completion": "The decorator login_required is passed the function (hello in this case).\n\nSo what you want to do is:\n\ndef login_required(f):\n\n  # This function is what we \"replace\" hello with\n  def wrapper(*args, **kw):\n    args[0].client_session['test'] = True\n    logged_in = 0\n    if logged_in:\n      return f(*args, **kw)  # Call hello\n    else:\n      return redirect(url_for('login'))\n\n  return wrapper\n\n"}
{"prompt": "I have a text file which a lot of random occurrences of the string @STRING_A, and I would be interested in writing a short script which removes only some of them. Particularly one that scans the file and once it finds a line which starts with this string like\n\n@STRING_A\n\n\nthen checks if 3 lines backwards there is another occurrence of a line starting with the same string, like\n\n@STRING_A\n\n\n@STRING_A\n\n\nand if it happens, to delete the occurrence 3 lines backward. I was thinking about bash, but I do not know how to \"go backwards\" with it. So I am sure that this is not possible with bash. I also thought about python, but then I should store all information in memory in order to go backwards and then, for long files it would be unfeasible. \n\nWhat do you think? Is it possible to do it in bash or python?\n\nThanks\n", "completion": "Funny that after all these hours nobody's yet given a solution to the problem as actually phrased (as @John Machin points out in a comment) -- remove just the leading marker (if followed by another such marker 3 lines down), not the whole line containing it. It's not hard, of course -- here's a tiny mod as needed of @truppo's fun solution, for example:\n\nfrom itertools import izip, chain\nf = \"foo.txt\"\nfor third, line in izip(chain(\"   \", open(f)), open(f)):\n    if third.startswith(\"@STRING_A\") and line.startswith(\"@STRING_A\"):\n        line = line[len(\"@STRING_A\"):]\n    print line,\n\n\nOf course, in real life, one would use an iterator.tee instead of reading the file twice, have this code in a function, not repeat the marker constant endlessly, &c;-).\n"}
{"prompt": "I'm curious as to why it's so much faster to multiply than to take powers in python (though from what I've read this may well be true in many other languages too). For example it's much faster to do\n\nx*x\n\n\nthan\n\nx**2\n\n\nI suppose the ** operator is more general and can also deal with fractional powers. But if that's why it's so much slower, why doesn't it perform a check for an int exponent and then just do the multiplication?\n\n\nEdit: Here's some example code I tried...\n\ndef pow1(r, n):\n  for i in range(r):\n    p = i**n\n\ndef pow2(r, n):\n  for i in range(r):\n    p = 1\n    for j in range(n):\n      p *= i\n\n\nNow, pow2 is just a quick example and is clearly not optimised!\nBut even so I find that using n = 2 and r = 1,000,000, then pow1 takes ~ 2500ms and pow2 takes ~ 1700ms.\n I admit that for large values of n, then pow1 does get much quicker than pow2. But that's not too surprising.\n", "completion": "Basically naive multiplication is O(n) with a very low constant factor. Taking the power is O(log n) with a higher constant factor (There are special cases that need to be tested... fractional exponents, negative exponents, etc) . Edit: just to be clear, that's O(n) where n is the exponent.\n\nOf course the naive approach will be faster for small n, you're only really implementing a small subset of exponential math so your constant factor is negligible. \n"}
{"prompt": "I'm making a program in Python to be distributed to windows users via an installer.\n\nThe program needs to be able to download a file every day encrypted with the user's public key and then decrypt it.\n\nSo I need to find a Python library that will let me generate public and private PGP keys, and also decrypt files encrypted with the public key.\n\nIs this something pyCrypto will do (documentation is nebulous)?  Are there other pure Python libraries?  How about a standalone command line tool in any language?\n\nAll I saw so far was GNUPG but installing that on Windows does stuff to the registry and throws dll's everywhere, and then I have to worry about whether the user already has this installed, how to backup their existing keyrings, etc.  I'd rather just have a python library or command line tool and mange the keys myself.\n\nUpdate: pyME might work but it doesn't seem to be compatible with Python 2.4 which I have to use.\n", "completion": "You don't need PyCrypto or PyMe, fine though those packages may be - you will have all kinds of problems building under Windows. Instead, why not avoid the rabbit-holes and do what I did? Use gnupg 1.4.9. You don't need to do a full installation on end-user machines - just gpg.exe and iconv.dll from the distribution are sufficient, and you just need to have them somewhere in the path or accessed from your Python code using a full pathname. No changes to the registry are needed, and everything (executables and data files) can be confined to a single folder if you want.\n\nThere's a module GPG.py which was originally written by Andrew Kuchling, improved by Richard Jones and improved further by Steve Traugott. It's available here, but as-is it's not suitable for Windows because it uses os.fork(). Although originally part of PyCrypto, it is completely independent of the other parts of PyCrypto and needs only gpg.exe/iconv.dll in order to work.\n\nI have a version (gnupg.py) derived from Traugott's GPG.py, which uses the subprocess module. It works fine under Windows, at least for my purposes - I use it to do the following:\n\n\nKey management - generation, listing, export etc.\nImport keys from an external source (e.g. public keys received from a partner company)\nEncrypt and decrypt data\nSign and verify signatures\n\n\nThe module I've got is not ideal to show right now, because it includes some other stuff which shouldn't be there - which means I can't release it as-is at the moment. At some point, perhaps in the next couple of weeks, I hope to be able to tidy it up, add some more unit tests (I don't have any unit tests for sign/verify, for example) and release it (either under the original PyCrypto licence or a similar commercial-friendly license). If you can't wait, go with Traugott's module and modify it yourself - it wasn't too much work to make it work with the subprocess module.\n\nThis approach was a lot less painful than the others (e.g. SWIG-based solutions, or solutions which require building with MinGW/MSYS), which I considered and experimented with. I've used the same (gpg.exe/iconv.dll) approach with systems written in other languages, e.g. C#, with equally painless results.\n\nP.S. It works with Python 2.4 as well as Python 2.5 and later. Not tested with other versions, though I don't foresee any problems.\n"}
{"prompt": "I need to write a Python script that can run another command line program and interact with it's stdin and stdout streams. Essentially, the Python script will read from the target command line program, intelligently respond by writing to its stdin, and then read the results from the program again. (It would do this repeatedly.)\n\nI've looked through the subprocess module, and I can't seem to get it to do this read/write/read/write thing that I'm looking for. Is there something else I should be trying?\n", "completion": "To perform such detailed interaction (when, outside of your control, the other program may be buffering its output unless it thinks it's talking to a terminal) needs something like pexpect -- which in turns requires pty, a Python standard library module that (on operating systems that allow it, such as Linux and Mac OS x) implements \"pseudo-terminals\".\n\nLife is harder on Windows, but maybe this zipfile can help -- it's supposed to be a port of pexpect to Windows (sorry, I have no Windows machine to check it on). The project in question, called wexpect, lives here.\n"}
{"prompt": "I'm trying to build my first facebook app, and it seems that the python facebook (pyfacebook) wrapper is really out of date, and the most relevant functions, like stream functions, are not implemented.\n\nAre there any mature python frontends for facebook? If not, what's the best language for facebook development?\n", "completion": "The updated location of pyfacebook is on github. Plus, as arstechnica well explains:\n\n\n  PyFacebook is also very easy to extend\n  when new Facebook API methods are\n  introduced. Each Facebook API method\n  is described in the PyFacebook library\n  using a simple data structure that\n  specifies the method's name and\n  parameter types.\n\n\nso, even should you be using a pyfacebook version that doesn't yet implement some brand-new thing you need, it's easy to add said thing, as Ryan Paul shows here regarding some of the stream functions (back in April right after they were launched).\n"}
{"prompt": "Here is a simple example of a django view with a potential race condition:\n\n# myapp/views.py\nfrom django.contrib.auth.models import User\nfrom my_libs import calculate_points\n\ndef add_points(request):\n    user = request.user\n    user.points += calculate_points(user)\n    user.save()\n\n\nThe race condition should be fairly obvious: A user can make this request twice, and the application could potentially execute user = request.user simultaneously, causing one of the requests to override the other.\n\nSuppose the function calculate_points is relatively complicated, and makes calculations based on all kinds of weird stuff that cannot be placed in a single update and would be difficult to put in a stored procedure.\n\nSo here is my question: What kind of locking mechanisms are available to django, to deal with situations similar to this?\n", "completion": "Django 1.4+ supports select_for_update, in earlier versions you may execute raw SQL queries e.g. select ... for update which depending on underlying DB will lock the row from any updates, you can do whatever you want with that row until the end of transaction. e.g.\n\nfrom django.db import transaction\n\n@transaction.commit_manually()\ndef add_points(request):\n    user = User.objects.select_for_update().get(id=request.user.id)\n    # you can go back at this point if something is not right \n    if user.points > 1000:\n        # too many points\n        return\n    user.points += calculate_points(user)\n    user.save()\n    transaction.commit()\n\n"}
{"prompt": "In Python, how do I read in a binary file and loop over each byte of that file?\n", "completion": "f = open(\"myfile\", \"rb\")\ntry:\n    byte = f.read(1)\n    while byte != \"\":\n        # Do stuff with byte.\n        byte = f.read(1)\nfinally:\n    f.close()\n\n\nBy suggestion of chrispy:\n\nwith open(\"myfile\", \"rb\") as f:\n    byte = f.read(1)\n    while byte != \"\":\n        # Do stuff with byte.\n        byte = f.read(1)\n\n\nNote that the with statement is not available in versions of Python below 2.5. To use it in v 2.5 you'll need to import it:\n\nfrom __future__ import with_statement\n\n\nIn 2.6 this is not needed.\n\nIn Python 3, it's a bit different. We will no longer get raw characters from the stream in byte mode but byte objects, thus we need to alter the condition:\n\nwith open(\"myfile\", \"rb\") as f:\n    byte = f.read(1)\n    while byte != b\"\":\n        # Do stuff with byte.\n        byte = f.read(1)\n\n\nOr as benhoyt says, skip the not equal and take advantage of the fact that b\"\" evaluates to false. This makes the code compatible between 2.6 and 3.x without any changes. It would also save you from changing the condition if you go from byte mode to text or the reverse.\n\nwith open(\"myfile\", \"rb\") as f:\n    byte = f.read(1)\n    while byte:\n        # Do stuff with byte.\n        byte = f.read(1)\n\n"}
{"prompt": "My scenario is as follows: I have a table of data (handful of fields, less than a hundred rows) that I use extensively in my program. I also need this data to be persistent, so I save it as a CSV and load it on start-up. I choose not to use a database because every option (even SQLite) is an overkill for my humble requirement (also - I would like to be able to edit the values offline in a simple way, and nothing is simpler than notepad).\n\nAssume my data looks as follows (in the file it's comma separated without titles, this is just an illustration):\n\n Row  | Name     | Year   | Priority\n------------------------------------\n 1    | Cat      | 1998   | 1\n 2    | Fish     | 1998   | 2\n 3    | Dog      | 1999   | 1 \n 4    | Aardvark | 2000   | 1\n 5    | Wallaby  | 2000   | 1\n 6    | Zebra    | 2001   | 3\n\n\nNotes:\n\n\nRow may be a \"real\" value written to the file or just an auto-generated value that represents the row number. Either way it exists in memory.\nNames are unique.\n\n\nThings I do with the data:\n\n\nLook-up a row based on either ID (iteration) or name (direct access).\nDisplay the table in different orders based on multiple field: I need to sort it e.g. by Priority and then Year, or Year and then Priority, etc.\nI need to count instances based on sets of parameters, e.g. how many rows have their year between 1997 and 2002, or how many rows are in 1998 and priority > 2, etc.\n\n\nI know this \"cries\" for SQL...\n\nI'm trying to figure out what's the best choice for data structure. Following are several choices I see:\n\nList of row lists:\n\na = []\na.append( [1, \"Cat\", 1998, 1] )\na.append( [2, \"Fish\", 1998, 2] )\na.append( [3, \"Dog\", 1999, 1] )\n...\n\n\nList of column lists (there will obviously be an API for add_row etc):\n\na = []\na.append( [1, 2, 3, 4, 5, 6] )\na.append( [\"Cat\", \"Fish\", \"Dog\", \"Aardvark\", \"Wallaby\", \"Zebra\"] )\na.append( [1998, 1998, 1999, 2000, 2000, 2001] )\na.append( [1, 2, 1, 1, 1, 3] )\n\n\nDictionary of columns lists (constants can be created to replace the string keys):\n\na = {}\na['ID'] = [1, 2, 3, 4, 5, 6]\na['Name'] = [\"Cat\", \"Fish\", \"Dog\", \"Aardvark\", \"Wallaby\", \"Zebra\"] \na['Year'] = [1998, 1998, 1999, 2000, 2000, 2001] \na['Priority'] = [1, 2, 1, 1, 1, 3]\n\n\nDictionary with keys being tuples of (Row, Field):\n\nCreate constants to avoid string searching\nNAME=1\nYEAR=2\nPRIORITY=3\n\na={}\na[(1, NAME)] = \"Cat\"\na[(1, YEAR)] = 1998\na[(1, PRIORITY)] = 1\na[(2, NAME)] = \"Fish\"\na[(2, YEAR)] = 1998\na[(2, PRIORITY)] = 2\n...\n\n\nAnd I'm sure there are other ways... However each way has disadvantages when it comes to my requirements (complex ordering and counting). \n\nWhat's the recommended approach?\n\nEDIT:\n\nTo clarify, performance is not a major issue for me. Because the table is so small, I believe almost every operation will be in the range of milliseconds, which is not a concern for my application.\n", "completion": "Having a \"table\" in memory that needs lookups, sorting, and arbitrary aggregation really does call out for SQL.  You said you tried SQLite, but did you realize that SQLite can use an in-memory-only database?\n\nconnection = sqlite3.connect(':memory:')\n\n\nThen you can create/drop/query/update tables in memory with all the functionality of SQLite and no files left over when you're done.  And as of Python 2.5, sqlite3 is in the standard library, so it's not really \"overkill\" IMO.\n\nHere is a sample of how one might create and populate the database:\n\nimport csv\nimport sqlite3\n\ndb = sqlite3.connect(':memory:')\n\ndef init_db(cur):\n    cur.execute('''CREATE TABLE foo (\n        Row INTEGER,\n        Name TEXT,\n        Year INTEGER,\n        Priority INTEGER)''')\n\ndef populate_db(cur, csv_fp):\n    rdr = csv.reader(csv_fp)\n    cur.executemany('''\n        INSERT INTO foo (Row, Name, Year, Priority)\n        VALUES (?,?,?,?)''', rdr)\n\ncur = db.cursor()\ninit_db(cur)\npopulate_db(cur, open('my_csv_input_file.csv'))\ndb.commit()\n\n\nIf you'd really prefer not to use SQL, you should probably use a list of dictionaries:\n\nlod = [ ] # \"list of dicts\"\n\ndef populate_lod(lod, csv_fp):\n    rdr = csv.DictReader(csv_fp, ['Row', 'Name', 'Year', 'Priority'])\n    lod.extend(rdr)\n\ndef query_lod(lod, filter=None, sort_keys=None):\n    if filter is not None:\n        lod = (r for r in lod if filter(r))\n    if sort_keys is not None:\n        lod = sorted(lod, key=lambda r:[r[k] for k in sort_keys])\n    else:\n        lod = list(lod)\n    return lod\n\ndef lookup_lod(lod, **kw):\n    for row in lod:\n        for k,v in kw.iteritems():\n            if row[k] != str(v): break\n        else:\n            return row\n    return None\n\n\nTesting then yields:\n\n>>> lod = []\n>>> populate_lod(lod, csv_fp)\n>>> \n>>> pprint(lookup_lod(lod, Row=1))\n{'Name': 'Cat', 'Priority': '1', 'Row': '1', 'Year': '1998'}\n>>> pprint(lookup_lod(lod, Name='Aardvark'))\n{'Name': 'Aardvark', 'Priority': '1', 'Row': '4', 'Year': '2000'}\n>>> pprint(query_lod(lod, sort_keys=('Priority', 'Year')))\n[{'Name': 'Cat', 'Priority': '1', 'Row': '1', 'Year': '1998'},\n {'Name': 'Dog', 'Priority': '1', 'Row': '3', 'Year': '1999'},\n {'Name': 'Aardvark', 'Priority': '1', 'Row': '4', 'Year': '2000'},\n {'Name': 'Wallaby', 'Priority': '1', 'Row': '5', 'Year': '2000'},\n {'Name': 'Fish', 'Priority': '2', 'Row': '2', 'Year': '1998'},\n {'Name': 'Zebra', 'Priority': '3', 'Row': '6', 'Year': '2001'}]\n>>> pprint(query_lod(lod, sort_keys=('Year', 'Priority')))\n[{'Name': 'Cat', 'Priority': '1', 'Row': '1', 'Year': '1998'},\n {'Name': 'Fish', 'Priority': '2', 'Row': '2', 'Year': '1998'},\n {'Name': 'Dog', 'Priority': '1', 'Row': '3', 'Year': '1999'},\n {'Name': 'Aardvark', 'Priority': '1', 'Row': '4', 'Year': '2000'},\n {'Name': 'Wallaby', 'Priority': '1', 'Row': '5', 'Year': '2000'},\n {'Name': 'Zebra', 'Priority': '3', 'Row': '6', 'Year': '2001'}]\n>>> print len(query_lod(lod, lambda r:1997 <= int(r['Year']) <= 2002))\n6\n>>> print len(query_lod(lod, lambda r:int(r['Year'])==1998 and int(r['Priority']) > 2))\n0\n\n\nPersonally I like the SQLite version better since it preserves your types better (without extra conversion code in Python) and easily grows to accommodate future requirements.  But then again, I'm quite comfortable with SQL, so YMMV.\n"}
{"prompt": "I have some some data that I would like to visualize. Each byte of the source data roughly corresponds to a pixel value of the image.\n\nWhat is the easiest way to generate an image file (bitmap?) using Python?\n", "completion": "You can create images with a list of pixel values using Pillow:\n\nfrom PIL import Image\n\nimg = Image.new('RGB', (width, height))\nimg.putdata(my_list)\nimg.save('image.png')\n\n"}
{"prompt": "Suppose I have the following Event model:\n\nfrom django.db import models\nimport datetime\n\nclass Event(models.Model):\n    date_start = models.DateField()\n    date_end = models.DateField()\n\n    def is_over(self):\n        return datetime.date.today() > self.date_end\n\n\nI want to test Event.is_over() by creating an Event that ends in the future (today + 1 or something), and stubbing the date and time so the system thinks we've reached that future date.\n\nI'd like to be able to stub ALL system time objects as far as python is concerned. This includes datetime.date.today(), datetime.datetime.now(), and any other standard date/time objects.\n\nWhat's the standard way to do this?\n", "completion": "EDIT: Since my answer is the accepted answer here I'm updating it to let everyone know a better way has been created in the meantime, the freezegun library: https://pypi.python.org/pypi/freezegun. I use this in all my projects when I want to influence time in tests. Have a look at it.\n\nOriginal answer:\n\nReplacing internal stuff like this is always dangerous because it can have nasty side effects. So what you indeed want, is to have the monkey patching be as local as possible.\n\nWe use Michael Foord's excellent mock library: http://www.voidspace.org.uk/python/mock/ that has a @patch decorator which patches certain functionality, but the monkey patch only lives in the scope of the testing function, and everything is automatically restored after the function runs out of its scope.\n\nThe only problem is that the internal datetime module is implemented in C, so by default you won't be able to monkey patch it. We fixed this by making our own simple implementation which can be mocked.\n\nThe total solution is something like this (the example is a validator function used within a Django project to validate that a date is in the future). Mind you I took this from a project but took out the non-important stuff, so things may not actually work when copy-pasting this, but you get the idea, I hope :)\n\nFirst we define our own very simple implementation of datetime.date.today in a file called utils/date.py:\n\nimport datetime\n\ndef today():\n    return datetime.date.today()\n\n\nThen we create the unittest for this validator in tests.py:\n\nimport datetime\nimport mock\nfrom unittest2 import TestCase\n\nfrom django.core.exceptions import ValidationError\n\nfrom .. import validators\n\nclass ValidationTests(TestCase):\n    @mock.patch('utils.date.today')\n    def test_validate_future_date(self, today_mock):\n        # Pin python's today to returning the same date\n        # always so we can actually keep on unit testing in the future :)\n        today_mock.return_value = datetime.date(2010, 1, 1)\n\n        # A future date should work\n        validators.validate_future_date(datetime.date(2010, 1, 2))\n\n        # The mocked today's date should fail\n        with self.assertRaises(ValidationError) as e:\n            validators.validate_future_date(datetime.date(2010, 1, 1))\n        self.assertEquals([u'Date should be in the future.'], e.exception.messages)\n\n        # Date in the past should also fail\n        with self.assertRaises(ValidationError) as e:\n            validators.validate_future_date(datetime.date(2009, 12, 31))\n        self.assertEquals([u'Date should be in the future.'], e.exception.messages)\n\n\nThe final implementation looks like this:\n\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.core.exceptions import ValidationError\n\nfrom utils import date\n\ndef validate_future_date(value):\n    if value <= date.today():\n        raise ValidationError(_('Date should be in the future.'))\n\n\nHope this helps\n"}
{"prompt": "Is there a library for using MS Access database in python? The win32 module is not as easy as the MySQL library. Is there a simpler way to use MS Access with Python?\n", "completion": "Depending on what you want to do, pyodbc might be what you are looking for.\n\n\nimport pyodbc\ndb_file = r'''C:\\x.mdb'''\nuser = 'admin'\npassword = ''\n\nodbc_conn_str = 'DRIVER={Microsoft Access Driver (*.mdb)};DBQ=%s;UID=%s;PWD=%s' %\\\n                (db_file, user, password)\n# Or, for newer versions of the Access drivers:\nodbc_conn_str = 'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=%s;UID=%s;PWD=%s' %\\\n                (db_file, user, password)\n\nconn = pyodbc.connect(odbc_conn_str)\n\n"}
{"prompt": "I have a function that take an argument which can be either a single item or a double item:\n\ndef iterable(arg)\n    if #arg is an iterable:\n        print \"yes\"\n    else:\n        print \"no\"\n\n\nso that:\n\n\n>>> iterable( (\"f\",\"f\") )\nyes\n\n>>> iterable( [\"f\",\"f\"] )\nyes\n\n>>> iterable(\"ff\")\nno\n\n\nThe problem is that string is technically iterable, so I can't just catch the ValueError when trying arg[1]. I don't want to use isinstance(), because that's not good practice (or so I'm told).\n", "completion": "Use isinstance (I don't see why it's bad practice)\n\nimport types\nif not isinstance(arg, types.StringTypes):\n\n\nNote the use of StringTypes.  It ensures that we don't forget about some obscure type of string.\n\nOn the upside, this also works for derived string classes.\n\nclass MyString(str):\n    pass\n\nisinstance(MyString(\"  \"), types.StringTypes) # true\n\n\nAlso, you might want to have a look at this previous question.\n\nCheers.\n"}
{"prompt": "I was just wondering if anyone knew of a way to change variable names based off of a for loop for something like this:\n\nfor i in range(3)\n     group+i=self.getGroup(selected, header+i)\n\n\nso that the names of the variables change to accomodate the data.  Thanks!\n\n~Sam\n", "completion": "You probably want a dict instead of separate variables.  For example\n\nd = {}\nfor i in range(3):\n    d[\"group\" + str(i)] = self.getGroup(selected, header+i)\n\n\nIf you insist on actually modifying local variables, you could use the locals function:\n\nfor i in range(3):\n    locals()[\"group\"+str(i)] = self.getGroup(selected, header+i)\n\n\nOn the other hand, if what you actually want is to modify instance variables of the class you're in, then you can use the setattr function\n\nfor i in group(3):\n    setattr(self, \"group\"+str(i), self.getGroup(selected, header+i)\n\n\nAnd of course, I'm assuming with all of these examples that you don't just want a list:\n\ngroups = [self.getGroup(i,header+i) for i in range(3)]\n\n"}
{"prompt": "I found this Perl script while migrating my SQLite database to mysql\n\nI was wondering (since I don't know Perl) how could one rewrite this in Python?\n\nBonus points for the shortest (code) answer :)\n\nedit: sorry I meant shortest code, not strictly shortest answer\n\n#! /usr/bin/perl\n\nwhile ($line = <>){\n    if (($line !~  /BEGIN TRANSACTION/) && ($line !~ /COMMIT/) && ($line !~ /sqlite_sequence/) && ($line !~ /CREATE UNIQUE INDEX/)){\n\n        if ($line =~ /CREATE TABLE \\\"([a-z_]*)\\\"(.*)/){\n                $name = $1;\n                $sub = $2;\n                $sub =~ s/\\\"//g; #\"\n                $line = \"DROP TABLE IF EXISTS $name;\\nCREATE TABLE IF NOT EXISTS $name$sub\\n\";\n        }\n        elsif ($line =~ /INSERT INTO \\\"([a-z_]*)\\\"(.*)/){\n                $line = \"INSERT INTO $1$2\\n\";\n                $line =~ s/\\\"/\\\\\\\"/g; #\"\n                $line =~ s/\\\"/\\'/g; #\"\n        }else{\n                $line =~ s/\\'\\'/\\\\\\'/g; #'\n        }\n        $line =~ s/([^\\\\'])\\'t\\'(.)/$1THIS_IS_TRUE$2/g; #'\n        $line =~ s/THIS_IS_TRUE/1/g;\n        $line =~ s/([^\\\\'])\\'f\\'(.)/$1THIS_IS_FALSE$2/g; #'\n        $line =~ s/THIS_IS_FALSE/0/g;\n        $line =~ s/AUTOINCREMENT/AUTO_INCREMENT/g;\n        print $line;\n    }\n}\n\n\nSome additional code was necessary to successfully migrate the sqlite database (handles one line Create table statements, foreign keys, fixes a bug in the original program that converted empty fields '' to \\'. \n\nI posted the code on the migrating my SQLite database to mysql Question\n", "completion": "Here's a pretty literal translation with just the minimum of obvious style changes (putting all code into a function, using string rather than re operations where possible).\n\nimport re, fileinput\n\ndef main():\n  for line in fileinput.input():\n    process = False\n    for nope in ('BEGIN TRANSACTION','COMMIT',\n                 'sqlite_sequence','CREATE UNIQUE INDEX'):\n      if nope in line: break\n    else:\n      process = True\n    if not process: continue\n    m = re.search('CREATE TABLE \"([a-z_]*)\"(.*)', line)\n    if m:\n      name, sub = m.groups()\n      line = '''DROP TABLE IF EXISTS %(name)s;\nCREATE TABLE IF NOT EXISTS %(name)s%(sub)s\n'''\n      line = line % dict(name=name, sub=sub)\n    else:\n      m = re.search('INSERT INTO \"([a-z_]*)\"(.*)', line)\n      if m:\n        line = 'INSERT INTO %s%s\\n' % m.groups()\n        line = line.replace('\"', r'\\\"')\n        line = line.replace('\"', \"'\")\n    line = re.sub(r\"([^'])'t'(.)\", r\"\\1THIS_IS_TRUE\\2\", line)\n    line = line.replace('THIS_IS_TRUE', '1')\n    line = re.sub(r\"([^'])'f'(.)\", r\"\\1THIS_IS_FALSE\\2\", line)\n    line = line.replace('THIS_IS_FALSE', '0')\n    line = line.replace('AUTOINCREMENT', 'AUTO_INCREMENT')\n    print line,\n\nmain()\n\n"}
{"prompt": "The source code behind EveryBlock.com, a major Django-powered website founded by Adrian Holovaty, one of the co-Benevolent Dictators For Life of the Django framework, was recently open-sourced. The source is available as tarballs and on github.\n\nThis large body of code from an originator of Django should have some interesting features, patterns, tricks, or techniques. What is your favorite?\n", "completion": "Some of the things that I noticed:\n\n\nThe publishing system ebpub uses custom django Authentication and user system, hence cannot use django-admin.\nAltho' it uses Relational Database PostgreSQL, the data items for various data entries are stored in a single table, with types of fields defined in another table, for scalability. (An alternative to key-value pair storing systems, CouchDB)\nThe system uses custom database backend, so that such a modified form database can accessed with convenience in views.\nThe blog application is very small and sweet; Just 1 Entry model and no views, Only generic views.\nSome of the bots present could be used for multiple purposes, with little tweaking.\n\n"}
{"prompt": "I have been doing active development in C# for several years now.  I primarily build enterprise application and in house frameworks on the .NET stack.\n\nI've never had the need to use any other mainstream high level languages besides C# for my tasks, since .NET is the standard platform we use.\n\nThere are some legacy Python applications that I have been asked to support going forward, I have no exposure to python and dynamic languages in general(although I've done a fair bit of JavaScript).\n\nI was hoping to get some guidance/advise to aid in how to go about learning a language like python for the statically typed mind.\n\nEDIT: Using IronPython is not an option!\n", "completion": "Foord and Muirhead's IronPython in Action is an amazingly good book, perfectly suitable for teaching Python to .NET folks as well as teaching .NET to Python folks. I may be biased, as I was a tech reviewer and Foord is a friend, but I've had other cases in the past where a friend wrote a book and I tech reviewed it -- and ended up deciding the book was just wrong and publicly saying so (way to lose friends, but, I just can't tell a lie, not where Python is concerned at least!-)\n\nEdit: If you're forbidden from moving to IronPython (which would probably support your legacy apps just fine, btw), there are better answers: Mark Pilgrim's Dive into Python is often considered the best Python intro for the experienced developer, and my own Python in a Nutshell has been praised as the fastest way onboard for superstar developers. I am of course biased in favor of these -- Mark is a colleague, and my wife was a key tech editor for his book (and my own as well), and obviously I'm biased in favor of my own book too;-). But then, I do tend to be biased towards a lot of the best Python books, as I've either had a hand in their editing, or am friends with their authors, or both;-).\n"}
{"prompt": "Which are the user friendly frameworks for building personal sites? Specially if that comes with little programming knowledge. And integrated jquery will be great. python or php based framework will do better.\n\nI tried wordpress and joomla! But those are far more complex for a simple personal site with personal blogging, live commenting, twitting, keeping personal projects and resume etc.\n\nPlease suggest me. Thanks in advance.\n", "completion": "\"a simple personal site with personal blogging, live commenting, twitting, keeping personal projects and resume etc.\"\n\nIn my opinion, a personal site means a single author.  You don't have a lot of really \"dynamic\" content.  How many times a day will  you update a person site?  Once?  Twice?\n\nA blog, comment, twitter things change relatively slowly -- once or twice a day.\n\nPersonal projects, resume, etc. change even more slowly.\n\nNone of this requires dynamic content creation.  A database is often more trouble than help.  Most of it is simply unstructured text.  Consequently, consider using a toolset to build static HTML and simply FTP that to a server.  \n\nConsider using Sphinx to build static content.  You can generate a mountain of content, maintain it, and upload it periodically.  You don't need to know HTML because you write in RST.  It's easy to generate hundreds of pages of content and adjust the look and feel.\n\nBest of all, it's very, very lightweight.  You can easily get by with zero code.  Or, if you want to add directives or interpreted text roles, you can do a little coding.\n"}
{"prompt": "The following snippet of code doesn't seem to affect the system clipboard at all:\n\nclipboard = QtGui.QApplication.clipboard()\nclipboard.setText(text)\n\n\nAccording to the Qt documentation, this is how you copy text to clipboard, \n\nWhy isn't it working?\n\nGoogling turned this up.\n\nIt suggests adding this after the above code:\n\nevent = QtCore.QEvent(QtCore.QEvent.Clipboard)\napp.sendEvent(clipboard, event)\n\n\nBut this one behaves odd: it only copies the text to the clipboard after the program exits. Plus, some people in that link reported that this doesn't work with linux.\n\nUPDATE:\n\nNevermind, I was doing something wrong else where, instead of binding the copy slot to the copy button, I connected it to the \"quit\" button.\n", "completion": "I know you are not using Windows, but maybe this will give you some ideas... I used this in a PyQt program to copy URLs to the clipboard:\n\nimport win32clipboard\n\ns = 'copy this to the clipboard'\ntry:\n    win32clipboard.OpenClipboard()\n    win32clipboard.EmptyClipboard()\n    win32clipboard.SetClipboardText(s)\n    win32clipboard.CloseClipboard()\nexcept:\n    print 'Could not copy clipboard data.'\n\n"}
{"prompt": "How do I serve a dynamically generated image in Django? \n\nI have an html tag\n\n<html>\n...\n    <img src=\"images/dynamic_chart.png\" />\n...\n</html>\n\n\nlinked up to this request handler, which creates an in-memory image\n\ndef chart(request):\n    img = Image.new(\"RGB\", (300,300), \"#FFFFFF\")\n    data = [(i,randint(100,200)) for i in range(0,300,10)]\n    draw = ImageDraw.Draw(img)\n    draw.polygon(data, fill=\"#000000\")\n    # now what?\n    return HttpResponse(output)\n\n\nI also plan to change the requests to AJAX, and add some sort of caching mechanism, but my understanding is that wouldn't affect this part of the solution.\n", "completion": "I assume you're using PIL (Python Imaging Library). You need to replace your last line with (for example, if you want to serve a PNG image):\n\nresponse = HttpResponse(mimetype=\"image/png\")\nimg.save(response, \"PNG\")\nreturn response\n\n\nSee here for more information.\n"}
{"prompt": "Is there any programming language (or type system) in which you could express the following Python-functions in a statically typed and type-safe way (without having to use casts, runtime-checks etc)?\n\n#1:\n\n# My function - What would its type be? \ndef Apply(x):\n    return x(x)\n\n# Example usage\nprint Apply(lambda _: 42)\n\n\n#2:\n\nwhite = None\nblack = None\n\ndef White():\n    for x in xrange(1, 10):\n        print (\"White move #%s\" % x)\n        yield black\n\ndef Black():\n    for x in xrange(1, 10):\n        print (\"Black move #%s\" % x)\n        yield white\n\nwhite = White()\nblack = Black()\n\n# What would the type of the iterator objects be?\nfor it in white:\n    it = it.next()\n\n", "completion": "1#\nThis is not typeable with a finite type. This means that very few (if any) programming languages will be able to type this.\n\nHowever, as you have demonstrated, there is a specific type for x that allows the function to be typed:\n\nx :: t -> B\n\n\nWhere B is some concrete type. This results in apply being typed as:\n\napply :: (t -> B) -> B\n\n\nNote that Hindley-Milner will not derive this type.\n\n2#\nThis is easy to represent in Haskell (left as an exercise to the reader...)\n"}
{"prompt": "I want to do something after I have rendered the view using \n\nreturn render_to_response()\n\n\nAre signals the only way to do this? Do I need to write a custom signal or does request_finished give me enough information? Basically I need to know what page was rendered, and then do an action in response to that.\n\nThanks.\n\nUPDATE FROM COMMENTS: I don't want to hold up the rendering of the page, so I want to render the page first and then do the action.\n", "completion": "You spawn a separate thread and have it do the action.\n\nt = threading.Thread(target=do_my_action, args=[my_argument])\n# We want the program to wait on this thread before shutting down.\nt.setDaemon(False)\nt.start()\n\n\nThis will cause 'do_my_action(my_argument)' to be executed in a second thread which will keep working even after you send your Django response and terminate the initial thread. For example it could send an email without delaying the response.\n"}
{"prompt": "Best practices aside, is there a compelling reason not to do this?\n\nI'm writing a post-commit hook for use with a Google Code project, which provides commit data via a JSON object.  GC provides an HMAC authentication token along with the request (outside the JSON data), so by validating that token I gain high confidence that the JSON data is both benign (as there's little point in distrusting Google) and valid.\n\nMy own (brief) investigations suggest that JSON happens to be completely valid Python, with the exception of the \"\\/\" escape sequence \u00e2\u0080\u0094 which GC doesn't appear to generate.\n\nSo, as I'm working with Python 2.4 (i.e. no json module), eval() is looking really tempting.\n\nEdit: For the record, I am very much not asking if this is a good idea.  I'm quite aware that it isn't, and I very much doubt I'll ever use this technique for any future projects even if I end up using it for this one.  I just wanted to make sure that I know what kind of trouble I'll run into if I do.  :-)\n", "completion": "If you're comfortable with your script working fine for a while, and then randomly failing on some obscure edge case, I would go with eval.\n\nIf it's important that your code be robust, I would take the time to add simplejson. You don't need the C portion for speedups, so it really shouldn't be hard to dump a few .py files into a directory somewhere.\n\nAs an example of something that might bite you, JSON uses Unicode and simplejson returns Unicode, whereas eval returns str:\n\n>>> simplejson.loads('{\"a\":1, \"b\":2}')\n{u'a': 1, u'b': 2}\n>>> eval('{\"a\":1, \"b\":2}')\n{'a': 1, 'b': 2}\n\n\nEdit: a better example of where eval() behaves differently:\n\n>>> simplejson.loads('{\"X\": \"\\uabcd\"}')\n{u'X': u'\\uabcd'}\n>>> eval('{\"X\": \"\\uabcd\"}')\n{'X': '\\\\uabcd'}\n>>> simplejson.loads('{\"X\": \"\\uabcd\"}') == eval('{\"X\": \"\\uabcd\"}')\nFalse\n\n\nEdit 2: saw yet another problem today pointed out by SilentGhost: eval doesn't handle true -> True, false -> False, null -> None correctly.\n\n>>> simplejson.loads('[false, true, null]')\n[False, True, None]\n>>> eval('[false, true, null]')\nTraceback (most recent call last):\n  File \"<interactive input>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\nNameError: name 'false' is not defined\n>>>\n\n"}
{"prompt": "What are all the special (magic) methods in Python? The __xxx__ methods, that is.\n\nI'm often looking for a way to override something which I know is possible to do through one of these methods, but I'm having a hard time to find how since as far as I can tell there is no definitive list of these methods, PLUS their names are not really Google friendly. So I think having a list of those here on SO would be a good idea.\n", "completion": "At the python level, most of them are documented in the language reference. At the C level, you can find it under the object protocol section (strictly speaking, you only have a subset here, though).\n"}
{"prompt": "Is there a FFT-based 2D cross-correlation or convolution function built into scipy (or another popular library)?\n\nThere are functions like these:\n\n\nscipy.signal.correlate2d - \"the direct method implemented by convolveND will be\nslow for large data\"\nscipy.ndimage.correlate - \"The array is correlated with the given kernel using\nexact calculation (i.e. not FFT).\"\nscipy.fftpack.convolve.convolve, which I don't really understand, but seems wrong\n\n\nnumarray had a correlate2d() function with an fft=True switch, but I guess numarray was folded\ninto numpy, and I can't find if this function was included.\n", "completion": "I found scipy.signal.fftconvolve, as also pointed out by magnus, but didn't realize at the time that it's n-dimensional.  Since it's built-in and produces the right values, it seems like the ideal solution.\n\nFrom Example of 2D Convolution:\n\nIn [1]: a = asarray([[ 1, 2, 3],\n   ...:              [ 4, 5, 6],\n   ...:              [ 7, 8, 9]])\n\nIn [2]: b = asarray([[-1,-2,-1],\n   ...:              [ 0, 0, 0],\n   ...:              [ 1, 2, 1]])\n\nIn [3]: scipy.signal.fftconvolve(a, b, mode = 'same')\nOut[3]: \narray([[-13., -20., -17.],\n       [-18., -24., -18.],\n       [ 13.,  20.,  17.]])\n\n\nCorrect!  The STSCI version, on the other hand, requires some extra work to make the boundaries correct?\n\nIn [4]: stsci.convolve2d(a, b, fft = True)\nOut[4]: \narray([[-12., -12., -12.],\n       [-24., -24., -24.],\n       [-12., -12., -12.]])\n\n\n(The STSCI method also requires compiling, which I was unsuccessful with (I just commented out the non-python parts), has some bugs like this and modifying the inputs ([1, 2] becomes [[1, 2]]), etc.  So I changed my accepted answer to the built-in fftconvolve() function.)\n\nCorrelation, of course, is the same thing as convolution, but with one input reversed:\n\nIn [5]: a\nOut[5]: \narray([[3, 0, 0],\n       [2, 0, 0],\n       [1, 0, 0]])\n\nIn [6]: b\nOut[6]: \narray([[3, 2, 1],\n       [0, 0, 0],\n       [0, 0, 0]])\n\nIn [7]: scipy.signal.fftconvolve(a, b[::-1, ::-1])\nOut[7]: \narray([[ 0., -0.,  0.,  0.,  0.],\n       [ 0., -0.,  0.,  0.,  0.],\n       [ 3.,  6.,  9.,  0.,  0.],\n       [ 2.,  4.,  6.,  0.,  0.],\n       [ 1.,  2.,  3.,  0.,  0.]])\n\nIn [8]: scipy.signal.correlate2d(a, b)\nOut[8]: \narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0],\n       [3, 6, 9, 0, 0],\n       [2, 4, 6, 0, 0],\n       [1, 2, 3, 0, 0]])\n\n\nand the latest revision has been sped up by using power-of-two sizes internally (and then I sped it up more by using real FFT for real input and using 5-smooth lengths instead of powers of 2  :D ).\n"}
{"prompt": "I want to build a bot that basically does the following:\n\n\nListens to the room and interacts with users and encourages them to PM the bot.\nOnce a user has PMed the bot engage with the client using various AI techniques. \n\n\nShould I just use the IRC library or Sockets in python or do I need more of a bot framework.\n\nWhat would you do?\n\nThanks!\n\nHere is the code I'm currently using, however, I haven't gotten it to work.\n\n#!/usr/bin/python \nimport socket\nnetwork = 'holmes.freenet.net'\nport = 6667\nirc = socket.socket ( socket.AF_INET, socket.SOCK_STREAM )\nirc.connect ( ( network, port ) )\nirc.send ( 'NICK PyIRC\\r\\n' )\nirc.send ( 'USER PyIRC PyIRC PyIRC :Python IRC\\r\\n' )\nirc.send ( 'JOIN #pyirc\\r\\n' )\nirc.send ( 'PRIVMSG #pyirc :Can you hear me?\\r\\n' )\nirc.send ( 'PART #pyirc\\r\\n' )\nirc.send ( 'QUIT\\r\\n' )\nirc.close()\n\n", "completion": "Use Twisted or Asynchat if you want to have a sane design. It is possible to just do it with sockets but why bother doing it from scratch?\n"}
{"prompt": "I have seen a couple of other posts on similar error message but couldn't find a solution which would fix it in my case.\n\nI dabbled a bit with TkInter and created a very simple UI. The code follows-\n\nfrom string import *\nfrom Tkinter import *\nimport tkMessageBox\n\nroot=Tk()\nvid = IntVar()\n\ndef grabText(event):\n    if entryBox.get().strip()==\"\":\n        tkMessageBox.showerror(\"Error\", \"Please enter text\")\n    else:\n        print entryBox.get().strip()    \n\nroot.title(\"My Sample\")\nroot.maxsize(width=550, height=200)\nroot.minsize(width=550, height=200)\nroot.resizable(width=NO, height=NO)    \n\nlabel=Label(root, text = \"Enter text:\").grid(row=2,column=0,sticky=W)\nentryBox=Entry(root,width=60).grid(row=2, column=1,sticky=W)\ngrabBtn=Button(root, text=\"Grab\")\ngrabBtn.grid(row=8, column=1)\ngrabBtn.bind('<Button-1>', grabText)\n\nroot.mainloop()\n\n\nI get the UI up and running. When I click on the Grab button, I get the following error on the console:\n\nC:\\Python25>python.exe myFiles\\testBed.py\nException in Tkinter callback\nTraceback (most recent call last):\n  File \"C:\\Python25\\lib\\lib-tk\\Tkinter.py\", line 1403, in __call__\n    return self.func(*args)\n  File \"myFiles\\testBed.py\", line 10, in grabText\n    if entryBox.get().strip()==\"\":\nAttributeError: 'NoneType' object has no attribute 'get'\n\n\nThe error traces back to Tkinter.py.\n\nI'm sure some one might have dealt with this before. Any help is appreciated.\n", "completion": "The grid (and pack, and place) function of the Entry object (and of all other widgets) returns None. In python when you do a().b(), the result of the expression is whatever b() returns, therefore Entry(...).grid(...) will return None. \n\nYou should split that onto two lines, like this:\n\nentryBox = Entry(root, width=60)\nentryBox.grid(row=2, column=1, sticky=W)\n\n\nThat way, you get your Entry reference stored in entryBox, and it's laid out like you expect. This has a bonus side effect of making your layout easier to understand and maintain, if you collect all of your grid and/or pack statements in blocks. \n"}
{"prompt": "Ok I know you can use the dir() method to list everything in a module, but is there any way to see only the functions that are defined in that module?  For example, assume my module looks like this:\n\nfrom datetime import date, datetime\n\ndef test():\n    return \"This is a real method\"\n\n\nEven if i use inspect() to filter out the builtins, I'm still left with anything that was imported.  E.g I'll see:\n\n['date', 'datetime', 'test']\n\nIs there any way to exclude imports?  Or another way to find out what's defined in a module?\n", "completion": "Are you looking for something like this?\n\nimport sys, inspect\n\ndef is_mod_function(mod, func):\n    return inspect.isfunction(func) and inspect.getmodule(func) == mod\n\ndef list_functions(mod):\n    return [func.__name__ for func in mod.__dict__.itervalues() \n            if is_mod_function(mod, func)]\n\n\nprint 'functions in current module:\\n', list_functions(sys.modules[__name__])\nprint 'functions in inspect module:\\n', list_functions(inspect)\n\n\nEDIT: Changed variable names from 'meth' to 'func' to avoid confusion (we're dealing with functions, not methods, here).\n"}
{"prompt": "What's going on here?  I'm trying to create a list of functions:\n\ndef f(a,b):\n    return a*b\n\nfuncs = []\n\nfor i in range(0,10):\n    funcs.append(lambda x:f(i,x))\n\n\nThis isn't doing what I expect.  I would expect the list to act like this:\n\nfuncs[3](3) = 9\nfuncs[0](5) = 0\n\n\nBut all the functions in the list seem to be identical, and be setting the fixed value to be 9:\n\nfuncs[3](3) = 27\nfuncs[3](1) = 9\n\nfuncs[2](6) = 54\n\n\nAny ideas?\n", "completion": "lambdas in python are closures.... the arguments you give it aren't going to be evaluated until the lambda is evaluated.  At that time, i=9 regardless, because your iteration is finished.\n\nThe behavior you're looking for can be achieved with functools.partial\n\nimport functools\n\ndef f(a,b):\n    return a*b\n\nfuncs = []\n\nfor i in range(0,10):\n    funcs.append(functools.partial(f,i))\n\n"}
{"prompt": "I'm wondering if Python has anything like the C# anonymous classes feature. To clarify, here's a sample C# snippet:\n\nvar foo = new { x = 1, y = 2 };\nvar bar = new { y = 2, x = 1 };\nfoo.Equals(bar); // \"true\"\n\n\nIn Python, I would imagine something like this:\n\nfoo = record(x = 1, y = 2)\nbar = record(y = 2, x = 1)\nfoo == bar  # true\n\n\nThe specific requirement is being able to create an object with specified fields in expression context (e.g. usable in lambdas and other places where statements aren't allowed), with no additional external declarations, and ability to access individual components by name via the normal member access syntax foo.bar. The created object should also implement structural comparison by component names  (not by position, as tuples do).\n\nIn particular: tuples isn't it because their components are not named; classes isn't it because they require a declaration; dicts isn't it because they have undesired foo[\"bar\"] syntax to access components.\n\nnamedtuple isn't it, because it still requires a name even if you define the type inline, and the comparison is position-based, not name-based. In particular:\n\n def foo(): return namedtuple(\"Foo\", \"x y\")(x = 1, y = 2)\n def bar(): return namedtuple(\"Foo\", \"y x\")(x = 1, y = 2)\n foo() == bar()   # False because fields are compared in order, and not by name\n                  # True would be desired instead\n\n\nI know how to write such a thing in Python if needed. But I would like to know if there's anything like that in the Python standard library, or any popular third-party libraries.\n\n[EDIT]\n\nJust for the sake of it, here's a single-expression solution that combines two very informative answers by Ken and alanlcode, yielding structural equality without any extra outside declarations:\n\ntype(\"\", (), { \\\n    \"__init__\": (lambda self, **kwargs: self.__dict__.update(kwargs)), \\\n    \"__eq__\": (lambda self, other: self.__dict__ == other.__dict__) } \\\n)(x = 1, y = 2)\n\n\nTechnically, it satisfies all the requirements of the question, but I sincerely hope that no-one ever uses it (I definitely won't).\n", "completion": "The pythonic way would be to use a dict:\n\n>>> foo = dict(x=1, y=2)\n>>> bar = dict(y=2, x=1)\n>>> foo == bar\nTrue\n\n\nMeets all your requirements except that you still have to do foo['x'] instead of foo.x. \n\nIf that's a problem, you could easily define a class such as:\n\nclass Bunch(object):\n    def __init__(self, **kwds):\n        self.__dict__.update(kwds)\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n\nOr, a nice and short one\n\nclass Bunch(dict):\n    __getattr__, __setattr__ = dict.get, dict.__setitem__\n\n\n(but note that this second one has problems as Alex points out in his comment!)\n"}
{"prompt": "I can't find a good clean way to lock a critical section in Django.  I could use a lock or semaphore but the python implementation is for threads only, so if the production server forks then those will not be respected.  Does anyone know of a way (I am thinking posix semaphores right now) to guarantee a lock across processes, or barring that a way to stop a Django server from forking.\n", "completion": "If you use RDBMS, you can use its \"LOCK\" mechanism.\nFor example, while one \"SELECT FOR UPDATE\" transaction locks a row, the other \"SELECT FOR UPDATE\" transactions with the row must wait.\n\n# You can use any Python DB API.\n[SQL] BEGIN;\n\n[SQL] SELECT col_name FROM table_name where id = 1 FOR UPDATE;\n\n[Process some python code]\n\n[SQL] COMMIT;\n\n"}
{"prompt": "I'm looking for a documentation generator for Python. I'm familiar with javadoc, and I  tried Doxygen, but it seems quite unfit and counter-intuitive for Python.\n\nAny ideas?\n\nEDIT: Apart from the excellent answers below, you can also consult wikipedia's exhaustive Comparison of documentation generators.\n", "completion": "The classic tool for API doc is epydoc. It handles javadoc, docstrings, etc... But I find API docs tools to be quite poor. I much prefer tool which focus around the documentation itself, and enables to inject additional documentation extracted from the code. Sphinx is perfect for this job. It can generates html and pdf, you can include automatically extracted docstring from code, it does syntax highlighting, etc... A strong point of sphinx is that it is done by someone who knows something about web design, and does not look like a**. matplotlib website and doc is generated entirely from sphinx, with default values. It looks much nicer than anything you will get with epydoc/doxygen. And there is an integrated search engine in javascript\n"}
{"prompt": "For my debugging needs, pdb is pretty good. However, it would be much cooler (and helpful) if I could go into ipython. Is this thing possible?\n", "completion": "There is an ipdb project which embeds iPython into the standard pdb, so you can just do:\n\nimport ipdb; ipdb.set_trace()\n\n\nIt's installable via the usual easy_install ipdb.\n\nipdb is pretty short, so instead of easy_installing you can also create a file ipdb.py somewhere on your Python path and paste the following into the file:\n\nimport sys\nfrom IPython.Debugger import Pdb\nfrom IPython.Shell import IPShell\nfrom IPython import ipapi\n\nshell = IPShell(argv=[''])\n\ndef set_trace():\n    ip = ipapi.get()\n    def_colors = ip.options.colors\n    Pdb(def_colors).set_trace(sys._getframe().f_back)\n\n"}
{"prompt": "When packaging a Python package with a setup.py that uses the setuptools:\n\nfrom setuptools import setup\n...\n\n\nthe source distribution created by:\n\npython setup.py sdist\n\n\nnot only includes, as usual, the files specified in MANIFEST.in, but it also, gratuitously, includes all of the files that Subversion lists as being version controlled beneath the package directory. This is vastly annoying. Not only does it make it difficult to exercise any sort of explicit control over what files get distributed with my package, but it means that when I build my package following an \"svn export\" instead of an \"svn checkout\", the contents of my package might be quite different, since without the .svn metadata setuptools will make different choices about what to include.\n\nMy question: how can I turn off this terrible behavior, so that \"setuptools\" treats my project the same way whether I'm using Subversion, or version control it's never heard of, or a bare tree created with \"svn export\" that I've created at the end of my project to make sure it builds cleanly somewhere besides my working directory?\n\nThe best I have managed so far is an ugly monkey-patch:\n\nfrom setuptools.command import sdist\ndel sdist.finders[:]\n\n\nBut this is Python, not the jungle, so of course I want a better solution that involves no monkeys at all. How can I tame setuptools, turn off its magic, and have it behave sensibly by looking at the visible, predictable rules in my MANIFEST.py instead?\n", "completion": "I know you know much of this, Brandon, but I'll try to give as a complete answer as I can (although I'm no setuptools gury) for the benefit of others.\n\nThe problem here is that setuptools itself involves quite a lot of black magick, including using an entry point called setuptools.file_finders where you can add plugins to find files to include. I am, however, at a complete loss as to how REMOVE plugins from it...\n\n\nQuick workaround: svn export your package to a temporary directory and run the setup.py from there. That means you have no svn, so the svn finder finds no files to include. :)\nLonger workaround: Do you really need setuptools? Setuptools have a lot of features, so the answer is likely yes, but mainly those features are depdenencies (so your dependencies get installed by easy_install), namespace packages (foo.bar), and entry points. Namespace packages can actually be created without setuptools as well. But if you use none of these you might actually get away with just using distutils.\nUgly workaround: The monkeypatch you gave to sdist in your question, which simply makes the plugin not have any finders, and exit quickly.\n\n\nSo as you see, this answer, although as complete as I can make it, is still embarrassingly incomplete. I can't actually answer your question, though I think the answer is \"You can't\". \n"}
{"prompt": "I have used hashlib (which replaces md5 in Python 2.6/3.0) and it worked fine if I opened a file and put its content in hashlib.md5() function.\n\nThe problem is with very big files that their sizes could exceed RAM size.\n\nHow to get the MD5 hash of a file without loading the whole file to memory?\n", "completion": "You need to read the file in chunks of suitable size:\n\ndef md5_for_file(f, block_size=2**20):\n    md5 = hashlib.md5()\n    while True:\n        data = f.read(block_size)\n        if not data:\n            break\n        md5.update(data)\n    return md5.digest()\n\n\nNOTE: Make sure you open your file with the 'rb' to the open - otherwise you will get the wrong result.\n\nSo to do the whole lot in one method - use something like: \n\ndef generate_file_md5(rootdir, filename, blocksize=2**20):\n    m = hashlib.md5()\n    with open( os.path.join(rootdir, filename) , \"rb\" ) as f:\n        while True:\n            buf = f.read(blocksize)\n            if not buf:\n                break\n            m.update( buf )\n    return m.hexdigest()\n\n\nThe update above was based on the comments provided by Frerich Raabe - and I tested this and found it to be correct on my Python 2.7.2 windows installation\n\nI cross-checked the results using the 'jacksum' tool.\n\njacksum -a md5 <filename>\n\n\nhttp://www.jonelo.de/java/jacksum/\n"}
{"prompt": "I have a multithreaded program where I create a generator function and then pass it to new threads.  I want it to be shared/global in nature so each thread can get the next value from the generator.\n\nIs it safe to use a generator like this, or will I run into problems/conditions accessing the shared generator from multiple threads?  \n\nIf not, is there a better way to approach the problem?  I need something that will cycle through a list and produce the next value for whichever thread calls it.\n", "completion": "It's not thread-safe; simultaneous calls may interleave, and mess with the local variables.\n\nThe common approach is to use the master-slave pattern (now called farmer-worker pattern in PC). Make a third thread which generates data, and add a Queue between the master and the slaves, where slaves will read from the queue, and the master will write to it. The standard queue module provides the necessary thread safety and arranges to block the master until the slaves are ready to read more data.\n"}
{"prompt": "I have a problem with serialization of Django inherited models. For example\n\nclass Animal(models.Model):\n    color = models.CharField(max_length=50)\n\nclass Dog(Animal):\n    name = models.CharField(max_length=50)\n\n...\n# now I want to serialize Dog model with Animal inherited fields obviously included\nprint serializers.serialize('xml', Dog.objects.all())\n\n\nand only Dog model has been serialized.\n\nI can do smth like \n\nall_objects = list(Animal.objects.all()) + list(Dog.objects.all())\nprint serializers.serialize('xml', all_objects)\n\n\nBut it looks ugly and because my models are very big so I have to use SAX parser and with such output it's difficult to parse.\n\nAny idea how to serialize django models with parent class?\n\n**EDIT: ** It use to work ok before this patch has been applied. And the explanation why the patch exist \"Model saving was too aggressive about creating new parent class instances during deserialization. Raw save on a model now skips saving of the parent class. \" I think there should be an option to be able to serialize \"local fields only\" by default and second option - \"all\" - to serialize all inherited fields.\n", "completion": "You found your answer in the documentation of the patch.\n\nall_objects = list(Animal.objects.all()) + list(Dog.objects.all())\nprint serializers.serialize('xml', all_objects)\n\n\nHowever, if you change Animal to be an abstract base class it will work:\n\nclass Animal(models.Model):\n    color = models.CharField(max_length=50)\n\n    class Meta:\n        abstract = True\n\nclass Dog(Animal):\n    name = models.CharField(max_length=50)\n\n\nThis works as of Django 1.0.  See http://docs.djangoproject.com/en/dev/topics/db/models/.\n"}
{"prompt": "I am parsing some XML with the elementtree.parse() function.  It works, except for some utf-8 characters(single byte character above 128).  I see that the default parser is XMLTreeBuilder which is based on expat.\n\nIs there an alternative parser that I can use that may be less strict and allow utf-8 characters?\n\nThis is the error I'm getting with the default parser:\n\nExpatError: not well-formed (invalid token): line 311, column 190\n\n\nThe character causing this is a single byte x92 (in hex).  I'm not certain this is even a valid utf-8 character.  But it would be nice to handle it because most text editors display this as: \u00c3\u00ad\n\nEDIT: The context of the character is: can\u00c3\u00adt , where I assume it is supposed to be a fancy apostraphe, but in the hex editor, that same sequence is: 63 61 6E 92 74 \n", "completion": "I'll start from the question: \"Is there an alternative parser that I can use that may be less strict and allow utf-8 characters?\"\n\nAll XML parsers will accept data encoded in UTF-8. In fact, UTF-8 is the default encoding.\n\nAn XML document may start with a declaration like this:\n\n`<?xml version=\"1.0\" encoding=\"UTF-8\"?>`\n\n\nor like this:\n    <?xml version=\"1.0\"?>\nor not have a declaration at all ... in each case the parser will decode the document using UTF-8.\n\nHowever your data is NOT encoded in UTF-8 ... it's probably Windows-1252 aka cp1252.\n\nIf the encoding is not UTF-8, then either the creator should include a declaration (or the recipient can prepend one) or the recipient can transcode the data to UTF-8. The following showcases what works and what doesn't:\n\n>>> import xml.etree.ElementTree as ET\n>>> from StringIO import StringIO as sio\n\n>>> raw_text = '<root>can\\x92t</root>' # text encoded in cp1252, no XML declaration\n\n>>> t = ET.parse(sio(raw_text))\n[tracebacks omitted]\nxml.parsers.expat.ExpatError: not well-formed (invalid token): line 1, column 9\n# parser is expecting UTF-8\n\n>>> t = ET.parse(sio('<?xml version=\"1.0\" encoding=\"UTF-8\"?>' + raw_text))\nxml.parsers.expat.ExpatError: not well-formed (invalid token): line 1, column 47\n# parser is expecting UTF-8 again\n\n>>> t = ET.parse(sio('<?xml version=\"1.0\" encoding=\"cp1252\"?>' + raw_text))\n>>> t.getroot().text\nu'can\\u2019t'\n# parser was told to expect cp1252; it works\n\n>>> import unicodedata\n>>> unicodedata.name(u'\\u2019')\n'RIGHT SINGLE QUOTATION MARK'\n# not quite an apostrophe, but better than an exception\n\n>>> fixed_text = raw_text.decode('cp1252').encode('utf8')\n# alternative: we transcode the data to UTF-8\n\n>>> t = ET.parse(sio(fixed_text))\n>>> t.getroot().text\nu'can\\u2019t'\n# UTF-8 is the default; no declaration needed\n\n"}
{"prompt": "Is there a way to create an NTFS junction point in Python? I know I can call the junction utility, but it would be better not to rely on external tools.\n", "completion": "you can use python win32 API modules e.g.\n\nimport win32file\n\nwin32file.CreateSymbolicLink(srcDir, targetDir, 1)\n\n\nsee http://docs.activestate.com/activepython/2.5/pywin32/win32file__CreateSymbolicLink_meth.html for more details\n\nif you do not want to rely on that too, you can always use ctypes and directly call CreateSymbolicLinl win32 API, which is anyway a simple call\n\nhere is example call using ctypes\n\nimport ctypes\n\nkdll = ctypes.windll.LoadLibrary(\"kernel32.dll\")\n\nkdll.CreateSymbolicLinkA(\"d:\\testdir\", \"d:\\testdir_link\", 1)\n\n\nMSDN says Minimum supported client Windows Vista\n"}
{"prompt": "I am using Sphinx for documenting my python project. I have the autodoc extension enabled and have the following in my docs.\n\n.. autoclass:: ClassName\n   :members:\n\n\nThe problem is, it only documents the non-private methods in the class. How do I include the private methods too?\n", "completion": "if you are using sphinx 1.1 or above, from the sphinx documentation site at http://sphinx.pocoo.org/ext/autodoc.html,\n\n:special-members:\n:private-members:\n\n"}
{"prompt": "How do I get the DNS records for a zone in python?  I'm looking for data similar to the output of dig.\n", "completion": "Try the dnspython library:\n\n\nhttp://www.dnspython.org/\n\n\nYou can see some examples here:\n\n\nhttp://www.dnspython.org/examples.html\n\n"}
{"prompt": "\n  Possible Duplicate:\n  Java Python Integration  \n\n\nI have a large existing codebase written in 100% Java, but I would like to use Python for some new sections of it. I need to do some text and language processing, and I'd much rather use Python and a library like NLTK to do this. \n\nI'm aware of the Jython project, but it looks like this represents a way to use Java and its libraries from within Python, rather than the other way round - am I wrong about this?\n\nIf not, what would be the best method to interface between Java and Python, such that (ideally) I can call a method in Python and have the result returned to Java? \n\nThank you.\n", "completion": "\n  I'm aware of the Jython project, but\n  it looks like this represents a way to\n  use Java and its libraries from within\n  Python, rather than the other way\n  round - am I wrong about this?\n\n\nYes, you are wrong. You can either use jythonc to compile python code to java .class files or call the python interpreter directly from Java.\n"}
{"prompt": "I'm new to Django (and Python) and I have been trying to work out a few things myself, before jumping into using other people's apps. I'm having trouble understanding where things 'fit' in the Django (or Python's) way of doing things. What I'm trying to work out is how to resize an image, once it's been uploaded. I have my model setup nicely and plugged into the admin, and the image uploads fine to the directory:\n\nfrom django.db import models\n\n# This is to list all the countries\n# For starters though, this will be just United Kingdom (GB)\nclass Country(models.Model):\n    name = models.CharField(max_length=120, help_text=\"Full name of country\")\n    code = models.CharField(max_length=2, help_text=\"This is the ISO 3166 2-letter country code (see: http://www.theodora.com/country_digraphs.html)\")\n    flag = models.ImageField(upload_to=\"images/uploaded/country/\", max_length=150, help_text=\"The flag image of the country.\", blank=True)\n\n    class Meta:\n        verbose_name_plural = \"Countries\"\n\n    def __unicode__(self):\n        return self.name\n\n\nThe thing I'm now having trouble with is taking that file and making a new file into a thumbnail. Like I say, I'd like to know how to do it without using others' apps (for now). I have got this code from DjangoSnippets:\n\nfrom PIL import Image\nimport os.path\nimport StringIO\n\ndef thumbnail(filename, size=(50, 50), output_filename=None):\n    image = Image.open(filename)\n    if image.mode not in ('L', 'RGB'):\n        image = image.convert('RGB')\n    image = image.resize(size, Image.ANTIALIAS)\n\n    # get the thumbnail data in memory.\n    if not output_filename:\n        output_filename = get_default_thumbnail_filename(filename)\n    image.save(output_filename, image.format) \n    return output_filename\n\ndef thumbnail_string(buf, size=(50, 50)):\n    f = StringIO.StringIO(buf)\n    image = Image.open(f)\n    if image.mode not in ('L', 'RGB'):\n        image = image.convert('RGB')\n    image = image.resize(size, Image.ANTIALIAS)\n    o = StringIO.StringIO()\n    image.save(o, \"JPEG\")\n    return o.getvalue()\n\ndef get_default_thumbnail_filename(filename):\n    path, ext = os.path.splitext(filename)\n    return path + '.thumb.jpg'\n\n\n...but this has ultimately confused me... As I don't know how this 'fits in' to my Django app? And really, is it the best solution for simply making a thumbnail of an image that has been successfully uploaded? Can anyone possibly show me a good, solid, decent way that a beginner like me can learn to do this properly? As in, knowing where to put that sort of code (models.py? forms.py? ...) and how it would work in context? ... I just need a bit of help understanding and working this problem out.\n\nThank you!\n", "completion": "If it's OK for you, there is a Django application ready, doing exactly what you want:\nhttps://github.com/sorl/sorl-thumbnail\n"}
{"prompt": "I can't remember which application I was using, but I do recall it having really neat date parsing/interpretation.\n\nFor example, you could type in 'two days ago' or 'tomorrow' and it would understand.\n\nAny libraries to suggest? Bonus points if usable from Python.\n", "completion": "Perhaps you are thinking of PHP's strtotime() function, the Swiss Army Knife of date parsing:\n\n\n  Man, what did I do before strtotime().  Oh, I know, I had a 482 line function to parse date formats and return timestamps.  And I still could not do really cool stuff.  Like tonight I needed to figure out when Thanksgiving was in the US.  I knew it was the 4th Thursday in November.  So, I started with some math stuff and checking what day of the week Nov. 1 would fall on.  All that was making my head hurt.  So, I just tried this for fun.\n\nstrtotime(\"thursday, november \".date(\"Y\").\" + 3 weeks\")\n\n  \n  That gives me Thanksgiving.  Awesome.\n\n\nWould you believe it even has its own dedicated web site, www.strtotime.net?\n\nSadly, there does not appear to be a Python equivalent. The closest thing I could find is the dateutil.parser module.\n"}
{"prompt": "I have been playing around with sorl-thumbnail for Django. And trying to understand how it works better. \n\nI've read the guide for it, installed it in my site-packages, made sure PIL is installed correctly, put sorl.thumbnail in the INSTALLED APPS in my settings.py, put from sorl.thumbnail.fields import ImageWithThumbnailsField at the top in my models.py, added image = ImageWithThumbnailsField(upload to=\"images/\", thumbnail={'size':(80, 80)}) as one of my model fields, passed the model through my view to the template, and in the template added {% load thumbnail %} at the top and put in the variable {{ mymodel.image.thumbnail_tag }} in there too.\n\nBut from what I understood is that when I upload an image through the admin, it would create the thumbnail straight away, but it only actually creates in when I see my template in the browser? Is this correct? The thumbnail shows fine, it looks great in fact, but I thought that adding the model field part of it would create the thumbnail instantly once the image has uploaded? ...Why not just use the models.ImageField in my model instead?\n\n...or have I done this all OK and I've just got the way it works wrong?\n", "completion": "I'm one of the sorl-thumbnail developers.\n\nFirstly, you don't need to {% load thumbnail %} unless you're just using the thumbnail tag rather than a thumbnail field.\n\nCurrently, a thumbnail is only ever created the first time it is used - even if you use the field [I'll get around to changing that one day if no-one else does first]. The advantage of the field is that you can specify the sizing rather than giving the freedom to the designer in the template level [and making it easier for an admin thumbnail].\n\nBoth ways work, you get to decide which works best for you.\n"}
{"prompt": "When using (pseudo) random numbers in Jython, would it be more efficient to use the Python random module or Java's random class?\n", "completion": "Python's version is much faster in a simple test on my Mac:\n\njython -m timeit -s \"import random\" \"random.random()\"\n\n\n1000000 loops, best of 3: 0.266 usec per loop\n\nvs\n\n jython -m timeit -s \"import java.util.Random; random=java.util.Random()\" \"random.nextDouble()\"\n\n\n1000000 loops, best of 3: 1.65 usec per loop\n\nJython version 2.5b3 and Java version 1.5.0_19.\n"}
{"prompt": "Currently, django.contrib.comments sends the user to the preview page if there is any error on the form. \n\nI am using comments in the context of a blog and I would much rather that the user stayed on the page they were on if something went wrong with the submission. As far as I can tell though, this is hard-coded in django.contrib.comments.views.comments.post_comment:\n\n# If there are errors or if we requested a preview show the comment\nif form.errors or preview:\n    template_list = [\n        \"comments/%s_%s_preview.html\" % tuple(str(model._meta).split(\".\")),\n        \"comments/%s_preview.html\" % model._meta.app_label,\n        \"comments/preview.html\",\n    ]\n    return render_to_response(\n        template_list, {\n            \"comment\" : form.data.get(\"comment\", \"\"),\n            \"form\" : form,\n            \"next\": next,\n        },\n        RequestContext(request, {})\n    )\n\n\nIs there any way that I can change this behavior without changing the source code to django.contrib.comments?\n\nAny pointer would be appreciated...\n\nThanks!\n", "completion": "Looks like you have two real options:\n\n\nWrite your own view.  Possibly copy that view's code to get started.\nPatch that view to take an extra parameter, such as 'preview_on_errors' which defaults to True but can be overridden.  Contribute the patch back to Django so other people can benefit from it.\n\n"}
{"prompt": "I have a service that runs that takes a list of about 1,000,000 dictionaries and does the following\n\nmyHashTable = {}\nmyLists = { 'hits':{}, 'misses':{}, 'total':{} }\nsorted = { 'hits':[], 'misses':[], 'total':[] }\nfor item in myList:\n  id = item.pop('id')\n  myHashTable[id] = item\n  for k, v in item.iteritems():\n    myLists[k][id] = v\n\n\nSo, if I had the following list of dictionaries:\n\n[ {'id':'id1', 'hits':200, 'misses':300, 'total':400},\n  {'id':'id2', 'hits':300, 'misses':100, 'total':500},\n  {'id':'id3', 'hits':100, 'misses':400, 'total':600}\n]\n\n\nI end up with\n\nmyHashTable =\n{ \n  'id1': {'hits':200, 'misses':300, 'total':400},\n  'id2': {'hits':300, 'misses':100, 'total':500},\n  'id3': {'hits':100, 'misses':400, 'total':600}\n}\n\n\nand\n\nmyLists = \n\n    {\n      'hits': {'id1':200, 'id2':300, 'id3':100},\n      'misses': {'id1':300, 'id2':100, 'id3':400},\n      'total': {'id1':400, 'id2':500, 'id3':600}\n    }\n\n\nI then need to sort all of the data in each of the myLists dictionaries.\n\nWhat I doing currently is something like the following:\n\ndef doSort(key):\n  sorted[key] = sorted(myLists[key].items(), key=operator.itemgetter(1), reverse=True)\n\nwhich would yield, in the case of misses:\n[('id3', 400), ('id1', 300), ('id2', 200)]\n\n\nThis works great when I have up to 100,000 records or so, but with 1,000,000 it is taking at least 5 - 10 minutes to sort each with a total of 16 (my original list of dictionaries actually has 17 fields including id which is popped)\n\n\n  * EDIT * This service is a ThreadingTCPServer which has a method\n  allowing a client to connect and add\n  new data.  The new data may include\n  new records (meaning dictionaries with\n  unique 'id's to what is already in\n  memory) or modified records (meaning\n  the same 'id' with different data for\n  the other key value pairs\n  \n  So, once this is running I would pass\n  in \n\n[\n  {'id':'id1', 'hits':205, 'misses':305, 'total':480},\n  {'id':'id4', 'hits':30, 'misses':40, 'total':60},\n  {'id':'id5', 'hits':50, 'misses':90, 'total':20\n]\n\n  \n  I have been using dictionaries to\n  store the data so that I don't end up\n  with duplicates.   After the\n  dictionaries are updated with the\n  new/modified data I resort each of\n  them.\n  \n  * END EDIT *\n\n\nSo, what is the best way for me to sort these?  Is there a better method?\n", "completion": "You may find this related answer from Guido:  Sorting a million 32-bit integers in 2MB of RAM using Python\n"}
{"prompt": "I am a .Net / SQL Server developer via my daytime job, and on the side I do some objective C development for the iPhone.  I would like to develop a web service and since dreamhost supports mySql, python, ruby on rails and PHP5, I would like to create it using one of those languages.  If you had no experience in either python, Ruby on Rails or PHP, which would you go with and why?  The service basically just takes a request and talks to a MySql database.\n\nNote:  Was planning on using the SOAP protocol.. though I am open to suggestions since I have a clean slate with these languages.\n", "completion": "Ruby-on-rails, Python and PHP would all be excellent choices for developing a web service in.  All the languages are capable (with of course Ruby being the language that Ruby on Rails is written in), have strong frameworks if that is your fancy (Django being a good python example, and something like Drupal or CakePHP being good PHP examples) and can play nicely with MySql.\n\nI'd say that it would depend mostly on your past experience and what you'd be the most comfortable with.  Assuming that you're developing C# on .NET and have experience with Objective-C PHP may be a good choice because it is most certainly from the C family of languages.  So the syntax might be more familiar and a bit easier to deal with.\n\nI'm a PHP developer so I'll give you that slant and let more knowledgeable developers with the others give theirs as well.  PHP is tightly integrated with Apache, which can make some of the more mundane tasks that you'd have to handle with the others a bit more trivial (though when working with a framework those are usually removed). The PHP documentation is second to none and is a great resource for getting up and going easily.  It has decent speed and there are good caching mechanisms out there to get more performance out of it.  I know that getting up and running with PHP on Dreamhost is trivial.  I haven't done it in the other instances although it wouldn't surprise me if those were just as easy as well.\n\nI'd suggest digging a bit more into the documentation and frameworks for each language to find out what suits you best. \n"}
{"prompt": "Consider:\n\npipe_read, pipe_write = os.pipe()\n\n\nNow, I would like to know two things:\n\n(1) I have two threads. If I guarantee that only one is reading os.read(pipe_read,n) and the other is only writing os.write(pipe_write), will I have any problem, even if the two threads do it simultaneously? Will I get all data that was written in the correct order? What happens if they do it simultaneously? Is it possible that a single write is read in pieces, like?:\n\nThread 1: os.write(pipe_write, '1234567')\nThread 2: os.read(pipe_read,big_number) --> '123'\nThread 2: os.read(pipe_read,big_number) --> '4567'\n\n\nOr -- again, consider simultaneity -- will a single os.write(some_string) always return entirely by a single os.read(pipe_read, very_big_number)?\n\n(2) Consider more than one thread writing to the pipe_write end of the pipe using logging.handlers.FileHandler() -- I've read that the logging module is threadsafe. Does this mean that I can do this without losing data? I think I won't be able to control the order of the data in the pipe; but this is not a requirement.\nRequirements:\n\n\nall data written by some threads on the write end must come out at the read end \na string written by a single logger.info(), logger.error(), ... has to stay in one piece.\n\n\nAre these reqs fulfilled?\n\nThank you in advance,\n\nJan-Philip Gehrcke\n", "completion": "os.read and os.write on the two fds returned from os.pipe is threadsafe, but you appear to demand more than that. Sub (1), yes, there is no \"atomicity\" guarantee for sinle reads or writes -- the scenario you depict (a single short write ends up producing two reads) is entirely possible. (In general, os.whatever is a thin wrapper on operating system functionality, and it's up to the OS to ensure, or fail to ensure, the kind of functionality you require; in this case, the Posix standard doesn't require the OS to ensure this kind of \"atomicity\"). You're guaranteed to get all data that was written, and in the correct order, but that's it. A single write of a large piece of data might stall once it's filled the OS-supplied buffer and only proceed once some other thread has read some of the initial data (beware deadlocks, of course!), etc, etc.\n\nSub (2), yes, the logging module is threadsafe AND \"atomic\" in that data produced by a single call to logging.info, logging.warn, logging.error, etc, \"stays in one piece\" in terms of calls to the underlying handler (however if that handler in turn uses non-atomic means such as os.write, it may still e.g. stall in the kernel until the underlying buffer gets unclogged, etc, etc, as above).\n"}
{"prompt": "I would like to know how to I exit from Python without having an traceback dump on the output. \n\nI still want want to be able to return an error code but I do not want to display the traceback log.\n\nI want to be able to exit using exit(number) without trace but in case of an Exception (not an exit) I want the trace.\n", "completion": "You are presumably encountering an exception and the program is exiting because of this (with a traceback).  The first thing to do therefore is to catch that exception, before exiting cleanly (maybe with a message, example given).\n\nTry something like this in your main routine:\n\nimport sys, traceback\n\ndef main():\n    try:\n        do main program stuff here\n        ....\n    except KeyboardInterrupt:\n        print \"Shutdown requested...exiting\"\n    except Exception:\n        traceback.print_exc(file=sys.stdout)\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n\n"}
{"prompt": "Suppose I have a relatively long module, but need an external module or method only once.\n\nIs it considered OK to import that method or module in the middle of the module?\n\nOr should imports only be in the first part of the module.\n\nExample:\n\nimport string, pythis, pythat\n...\n...\n...\n...\ndef func():\n     blah\n     blah \n     blah\n     from pysomething import foo\n     foo()\n     etc\n     etc \n     etc\n...\n...\n...\n\n\nPlease justify your answer and add links to PEPs or relevant sources\n", "completion": "PEP 8 authoritatively states:\n\n\n  Imports are always put at the top of\n  the file, just after any module\n        comments and docstrings, and before module globals and constants.\n\n\nPEP 8 should be the basis of any \"in-house\" style guide, since it summarizes what the core Python team has found to be the most effective style, overall (and with individual dissent of course, as on any other language, but consensus and the BDFL agree on PEP 8).\n"}
{"prompt": "How to make tabulation look different than whitespace in vim (highlighted for example).\n\nThat would be useful for code in Python.\n", "completion": "I use something like this:\n\nset list listchars=tab:\u00c2\u00bb\u00c2\u00b7,trail:\u00c2\u00b7,precedes:\u00e2\u0080\u00a6,extends:\u00e2\u0080\u00a6,nbsp:\u00e2\u0080\u0097\n\n\nRequires Vim7 and I'm not sure how well this is going to show up in a browser, because it uses some funky Unicode characters.  It's good to use some oddball characters so that you can distinguish a tab from something you may have typed yourself.\n\nIn addition to showing tabs, showing spaces at the end of lines is useful so you know to remove them (they are annoying).\n"}
{"prompt": "Is there a standard pythonic way to treat physical units / quantities in python? I saw different module-specific solutions from different fields like physics or neuroscience. But I would rather like to use a standard method than \"island\"-solutions as others should be able to easily read my code.\n", "completion": "The best solution is the Unum package. a de-facto standard, imho.\n"}
{"prompt": "I'm trying to reduce the number of scripts included in our website and we use buildout to handle deployments. Has anybody successfully implemented a method of combining and compressing scripts with buildout?\n", "completion": "Here's a Python script I made that I use with all my heavy JavaScript projects. I'm using YUICompressor, but you can change the code to use another compressor.\n\nimport os, os.path, shutil\n\nYUI_COMPRESSOR = 'yuicompressor-2.4.2.jar'\n\ndef compress(in_files, out_file, in_type='js', verbose=False,\n             temp_file='.temp'):\n    temp = open(temp_file, 'w')\n    for f in in_files:\n        fh = open(f)\n        data = fh.read() + '\\n'\n        fh.close()\n\n        temp.write(data)\n\n        print ' + %s' % f\n    temp.close()\n\n    options = ['-o \"%s\"' % out_file,\n               '--type %s' % in_type]\n\n    if verbose:\n        options.append('-v')\n\n    os.system('java -jar \"%s\" %s \"%s\"' % (YUI_COMPRESSOR,\n                                          ' '.join(options),\n                                          temp_file))\n\n    org_size = os.path.getsize(temp_file)\n    new_size = os.path.getsize(out_file)\n\n    print '=> %s' % out_file\n    print 'Original: %.2f kB' % (org_size / 1024.0)\n    print 'Compressed: %.2f kB' % (new_size / 1024.0)\n    print 'Reduction: %.1f%%' % (float(org_size - new_size) / org_size * 100)\n    print ''\n\n    #os.remove(temp_file)\n\n\nI use it like this (the below is just a code snippet, and assumes that the compress function exists in the current namespace):\n\nSCRIPTS = [\n    'app/js/libs/EventSource.js',\n    'app/js/libs/Hash.js',\n    'app/js/libs/JSON.js',\n    'app/js/libs/ServiceClient.js',\n    'app/js/libs/jquery.hash.js',\n    'app/js/libs/Application.js',\n    'app/js/intro.js',\n    'app/js/jquery-extras.js',\n    'app/js/settings.js',\n    'app/js/api.js',\n    'app/js/game.js',\n    'app/js/user.js',\n    'app/js/pages.intro.js',\n    'app/js/pages.home.js',\n    'app/js/pages.log-in.js',\n    'app/js/pages.log-out.js',\n    'app/js/pages.new-command.js',\n    'app/js/pages.new-frame.js',\n    'app/js/pages.not-found.js',\n    'app/js/pages.register.js',\n    'app/js/pages.outro.js',\n    'app/js/outro.js',\n    ]\nSCRIPTS_OUT_DEBUG = 'app/js/multifarce.js'\nSCRIPTS_OUT = 'app/js/multifarce.min.js'\n\nSTYLESHEETS = [\n    'app/media/style.css',\n    ]\nSTYLESHEETS_OUT = 'app/media/style.min.css'\n\ndef main():\n    print 'Compressing JavaScript...'\n    compress(SCRIPTS, SCRIPTS_OUT, 'js', False, SCRIPTS_OUT_DEBUG)\n\n    print 'Compressing CSS...'\n    compress(STYLESHEETS, STYLESHEETS_OUT, 'css')\n\nif __name__ == '__main__':\n    main()\n\n"}
{"prompt": "What's the right way to control timeouts, from the client, when running against a MySQL database, using SQLAlchemy?  The connect_timeout URL parameter seems to be insufficient.\n\nI'm more interested in what happens when the machine that the database is running on, e.g., disappears from the network unexpectedly.  I'm not worried about the queries themselves taking too long.  \n\nThe following script does what you'd expect (i.e., time out after approximately one second) if somehost is unavailable before the while loop is ever reached.  But if somehost goes down during the while loop (e.g., try yanking out its network cable after the loop has started), then the timeout seems to take at least 18 seconds.  Is there some additional setting or parameter I'm missing?\n\nIt's not surprising that the wait_timeout session variable doesn't work, as I think that's a server-side variable.  But I threw it in there just to make sure.\n\nfrom sqlalchemy import *\nfrom sqlalchemy.exc import *\nimport time\nimport sys\n\nengine = create_engine(\"mysql://user:password@somehost/test?connect_timeout=1\")\ntry:\n    engine.execute(\"set session wait_timeout = 1;\")\n    while True:\n        t = time.time()\n        print t\n        engine.execute(\"show tables;\")\nexcept DBAPIError:\n    pass\nfinally:\n    print time.time() - t, \"seconds to time out\"\n\n", "completion": "this isn't possible due to the way TCP works. if the other computer drops off the network, it will simply stop responding to incoming packets. the \"18 seconds\" you're seeing is something on your TCP stack timing out due to no response.\n\nthe only way you can get your desired behavior is to have the computer generate a \"i'm dying\" message immediately before it dies. which, if the death is unexpected, is completely impossible.\n\nhave you ever heard of hearbeats? these are packets that high-availability systems send to each other every second or less to let the other one know they still exist. if you want your application to know \"immediately\" that the server is gone, you first have to decide how long \"immediate\" is (1 second, 200 ms, etc.) and then designed a system (such as heartbeats) to detect when the other system is no longer there.\n"}
{"prompt": "I'm starting to learn python and loving it. I work on a Mac mainly as well as Linux. I'm finding that on Linux (Ubuntu 9.04 mostly) when I install a python module using apt-get it works fine. I can import it with no trouble.\n\nOn the Mac, I'm used to using Macports to install all the Unixy stuff. However, I'm finding that most of the python modules I install with it are not being seen by python. I've spent some time playing around with PATH settings and using python_select . Nothing has really worked and at this point I'm not really understanding, instead I'm just poking around.\n\nI get the impression that Macports isn't universally loved for managing python modules. I'd like to start fresh using a more \"accepted\" (if that's the right word) approach. \n\nSo, I was wondering, what is the method that Mac python developers use to manage their modules?\n\nBonus questions: \n\nDo you use Apple's python, or some other version?\nDo you compile everything from source or is there a package manger that works well (Fink?).\n\nAny tips or suggestions here are greatly appreciated. Thanks for your time. :)\n", "completion": "The most popular way to manage python packages (if you're not using your system package manager) is to use setuptools and easy_install. It is probably already installed on your system. Use it like this:\n\neasy_install django\n\n\neasy_install uses the Python Package Index  which is an amazing resource for python developers. Have a look around to see what packages are available.\n\nA better option is pip, which is gaining traction, as it attempts to fix a lot of the problems associated with easy_install. Pip uses the same package repository as easy_install, it just works better. Really the only time use need to use easy_install is for this command:\n\neasy_install pip\n\n\nAfter that, use:\n\npip install django\n\n\nAt some point you will probably want to learn a bit about virtualenv. If you do a lot of python development on projects with conflicting package requirements, virtualenv is a godsend. It will allow you to have completely different versions of various packages, and switch between them easily depending your needs.\n\nRegarding which python to use, sticking with Apple's python will give you the least headaches, but If you need a newer version (Leopard is 2.5.1 I believe), I would go with the macports python 2.6.\n"}
{"prompt": "Can someone please explain to me what is going on with python in ubuntu 9.04?\n\nI'm trying to spin up virtualenv, and the --no-site-packages flag seems to do nothing with ubuntu. I installed virtualenv 1.3.3 with easy_install (which I've upgraded to setuptools 0.6c9) and everything seems to be installed to /usr/local/lib/python2.6/dist-packages\n\nI assume that when installing a package using apt-get, it's placed in /usr/lib/python2.6/dist-packages/ ?\n\nThe issue is, there is a /usr/local/lib/python2.6/site-packages as well that just sits there being empty. It would seem (by looking at the path in a virtualenv) that this is the folder virtualenv uses as backup. Thus even thought I omit --no-site-packages, I cant access my local systems packages from any of my virtualenv's.\n\nSo my questions are:\n\n\nHow do I get virtualenv to point to one of the dist-packages?\nWhich dist-packages should I point it to? /usr/lib/python2.6/dist-packages or /usr/local/lib/python2.6/dist-packages/\nWhat is the point of /usr/lib/python2.6/site-packages? There's nothing in there!\nIs it first come first serve on the path? If I have a newer version of package XYZ installed in /usr/local/lib/python2.6/dist-packages/ and and older one (from ubuntu repos/apt-get) in /usr/lib/python2.6/dist-packages, which one gets imported when I import xyz? I'm assuming this is based on the path list, yes?\nWhy the hell is this so confusing? Is there something I'm missing here?\nWhere is it defined that easy_install should install to /usr/local/lib/python2.6/dist-packages?\nWill this affect pip as well?\n\n\nThanks to anyone who can clear this up!\n", "completion": "I believe Mike Orr's answer from the virtual-env mailing list seems to be the best. Note the OP published this question in both places.\n\nOriginal content of mail:\n\nYears ago Debian created /usr/local/lib/pythonVERSION/site-packages,\nand compiled the Python binary to include it in the default search\npath.  Ubuntu followed Debian's lead as it normally does.  The Python\ndevelopers did not like this because you'd get interference with a\nlocally-installed /usr/local/bin/python using the same site-packages\ndirectory. Ubuntu finally decided to abandon site-packages and use\ndist-packages instead, a name which they invented so it wouldn't\ninterfere with anything.  The loing story is out there somewhere if\nyou google it, somewhere in the Python bug tracker or distutils SIG or\nsuch.\n\nThe system works, at least if you use the Ubuntu virtualenv package.\nSome people have had problems using a locally-installed virtualenv on\nUbuntu because the magic sys.path entries weren't being added or\nsomething.  I'm not sure about --no-site-packages because I never use\nthat option: I run PIL and mysqldb from the Ubuntu packages because it\ncan be hard to compile their C dependencies sometimes.  (Need the\nright header files, Python is ignoring the header files, etc.)\n\nSo Ubuntu Python packages go into\n/usr/lib/pythonVERSION/dist-packages.  Or that python-support\ndirectory for some reason.  Locally-installed Python packages go into\n/usr/local/lib/pythonVERSION/dist-packages by default.  Whenever I\ninstall an Ubuntu 9.04 system I run:\n\n$ sudo apt-get install python-setuptools   (6.0c9)\n$ sudo apt-get install python-virtualenv   (1.3.3)\n$ sudo easy_install pip\n$ sudo pip install virtualenvwrapper\n\nThe virtualenvs work fine this way, although I haven't tried --no-site-packages.\n\n\n  I'm trying to spin up virtualenv, and the --no-site-packages flag\n  seems to do nothing with ubuntu. I installed virtualenv 1.3.3 with\n  easy_install (which I've upgraded to setuptools 0.6c9)\n\n\nThese versions are both in Ubuntu 9.04, so you're making it harder on\nyourself by installing them locally.\n\n\n  and everything\n  seems to be installed to /usr/local/lib/python2.6/dist-packages\n\n\nYes\n\n\n  I assume that when installing a package using apt-get, it's placed in /\n  usr/lib/python2.6/dist-packages/ ?\n\n\nYes\n\n\n  \n  Is it first come first serve on the path? If I have a newer\n  version of package XYZ installed in /usr/local/lib/python2.6/dist-\n  packages/ and and older one (from ubuntu repos/apt-get) in /usr/lib/\n  python2.6/dist-packages, which one gets imported when I import xyz?\n  I'm assuming this is based on the path list, yes?\n  \n\n\nsys.path is scanned in order.  The only funny thing is that .pth eggs\nget put earlier or later in the path than some people expect.  But if\nyou're using pip for everything it can do (i.e. except to install pip\nitself, precompiled eggs, and a snapshot of a local directory that's a\ncopy rather than an egg link), you won't have many .pth eggs anyway.\n\n\n  \n  Why the hell is this so confusing? Is there something I'm\n  missing here?\n  \n\n\nIt's not well documented.  I figured it out by scanning the web.\n\n\n  \n  Will this affect pip as well?\n  \n\n\nYes, pip will automatically install to\n/usr/local/lib/pythonVERSION/site-packages.  Use \"pip install -E\n$VIRTUAL_ENV packagename\" to install into a virtualenv.\n"}
{"prompt": "The app runs fine using django internal server however when I use apache + mod_python I get the below error \n\n\n\n  File \"/usr/local/lib/python2.6/dist-packages/django/conf/__init__.py\", line 75, in __init__\n    raise ImportError, \"Could not import settings '%s' (Is it on sys.path? Does it have syntax errors?): %s\" % (self.SETTINGS_MODULE, e)\n\nImportError: Could not import settings 'settings' (Is it on sys.path? Does it have syntax errors?): No module named settings\n\n\n\n\nHere is the needed information \n\n1) Project directory: /root/djangoprojects/mysite\n\n2) directory listing of /root/djangoprojects/mysite\n\nls -ltr\ntotal 28\n-rw-r--r-- 1 root root  546 Aug  1 08:34 manage.py\n-rw-r--r-- 1 root root    0 Aug  1 08:34 __init__.py\n-rw-r--r-- 1 root root  136 Aug  1 08:35 __init__.pyc\n-rw-r--r-- 1 root root 2773 Aug  1 08:39 settings.py\n-rw-r--r-- 1 root root 1660 Aug  1 08:53 settings.pyc\ndrwxr-xr-x 2 root root 4096 Aug  1 09:04 polls\n-rw-r--r-- 1 root root  581 Aug  1 10:06 urls.py\n-rw-r--r-- 1 root root  314 Aug  1 10:07 urls.pyc\n\n\n3) App directory : /root/djangoprojects/mysite/polls\n\n4) directory listing of /root/djangoprojects/mysite/polls \n\nls -ltr\ntotal 20\n-rw-r--r-- 1 root root 514 Aug  1 08:53 tests.py\n-rw-r--r-- 1 root root  57 Aug  1 08:53 models.py\n-rw-r--r-- 1 root root   0 Aug  1 08:53 __init__.py\n-rw-r--r-- 1 root root 128 Aug  1 09:02 views.py\n-rw-r--r-- 1 root root 375 Aug  1 09:04 views.pyc\n-rw-r--r-- 1 root root 132 Aug  1 09:04 __init__.pyc\n\n\n5) Anywhere in the filesystem running import django in python interpreter works fine\n\n6) content of httpd.conf\n\n<Location \"/mysite\">\n    SetHandler python-program\n    PythonHandler django.core.handlers.modpython\n    SetEnv DJANGO_SETTINGS_MODULE settings\n    PythonOption django.root /mysite\n    PythonPath \"['/root/djangoprojects/', '/root/djangoprojects/mysite','/root/djangoprojects/mysite/polls', '/var/www'] + sys.path\"\n    PythonDebug On\n</Location>\n\n\n7) PYTHONPATH variable is set to \n\necho $PYTHONPATH\n/root/djangoprojects/mysite\n\n\n8) DJANGO_SETTINGS_MODULE is set to\n\necho $DJANGO_SETTINGS_MODULE\nmysite.settings\n\n\n9) content of sys.path is \n\nimport sys\n>>> sys.path\n['', '/root/djangoprojects/mysite', '/usr/lib/python2.6', '/usr/lib/python2.6/plat-linux2', '/usr/lib/python2.6/lib-tk', '/usr/lib/python2.6/lib-old', '/usr/lib/python2.6/lib-dynload', '/usr/lib/python2.6/dist-packages', '/usr/local/lib/python2.6/dist-packages']\n\n\nHow do I add settings location to sys.path such that it persistent across sessions ? \n\nI have read umpteen no of post with people having the same issue it and I have tried a lot completely beats me as to what I need to do.\n\nLooking for some help.\n\nThanks in advance\nAnkur Gupta\n", "completion": "Your apache configuration should look like this:\n\n<Location \"/mysite\">\n    SetHandler python-program\n    PythonHandler django.core.handlers.modpython\n    SetEnv DJANGO_SETTINGS_MODULE mysite.settings\n    PythonOption django.root /mysite\n    PythonPath \"['/root/djangoprojects/', '/root/djangoprojects/mysite','/root/djangoprojects/mysite/polls', '/var/www'] + sys.path\"\n    PythonDebug On\n</Location>\n\n\nNote that the sole difference is the \"mysite.settings\". Don't forget to restart apache once the config has changed (apache2ctl restart). See http://docs.djangoproject.com/en/dev/howto/deployment/modpython/ for more info.\n"}
{"prompt": "Is there a simple and quick way to use sum() with non-integer values?\n\nSo I can use it like this:\n\nclass Foo(object):\n    def __init__(self,bar)\n        self.bar=bar\n\nmylist=[Foo(3),Foo(34),Foo(63),200]\nresult=sum(mylist) # result should be 300\n\n\nI tried overriding __add__ and __int__ etc, but I don't have found a solution yet\n\nEDIT:\n\nThe solution is to implement:\n\n def __radd__(self, other):\n    return other + self.bar\n\n\nas Will suggested in his post. But as always, all roads lead to Rome, but I think this is the best solution since I don't need __add__ in my class\n", "completion": "Its a bit tricky - the sum() function takes the start and adds it to the next and so on\n\nYou need to implement the __radd__ method:\n\nclass T:\n    def __init__(self,x):\n    \tself.x = x\n    def __radd__(self, other):\n    \treturn other + self.x\n\ntest = (T(1),T(2),T(3),200)\nprint sum(test)\n\n"}
{"prompt": "I am trying to modify the controls of a Panel, have it update, then continue on with code execution.  The problem seems to be that the Panel is waiting for Idle before it will refresh itself.  I've tried refresh of course as well as GetSizer().Layout() and even sent a resize event to the frame using the SendSizeEvent() method, but to no avail.  I'm at a loss here, I find it difficult to believe there is no way to force a redrawing of this panel.  Here is the code that changes the controls:\n\ndef HideButtons(self):\n        self.newButton.Show(False)\n        self.openButton.Show(False)\n        self.exitButton.Show(False)\n        self.buttonSizer.Detach(self.newButton)\n        self.buttonSizer.Detach(self.openButton)\n        self.buttonSizer.Detach(self.exitButton)\n        loadingLabel = wx.StaticText(self.splashImage, wx.ID_ANY, \"Loading...\", style=wx.ALIGN_LEFT)\n        loadingLabel.SetBackgroundColour(wx.WHITE)\n        self.buttonSizer.Add(loadingLabel)\n        self.GetSizer().Layout()\n        self.splashImage.Refresh()\n\n\nHas anybody else encountered anything like this? And how did you resolve it if so?\n", "completion": "You need to call the Update method.\n"}
{"prompt": "I am looking for a syntax definition, example, sample code, wiki, etc. for \nexecuting a LOAD DATA LOCAL INFILE command from python.\n\nI believe I can use mysqlimport as well if that is available, so any feedback (and code snippet) on which is the better route, is welcome.  A Google search is not turning up much in the way of current info\n\nThe goal in either case is the same:  Automate loading hundreds of files with a known naming convention & date structure, into a single MySQL table.\n\nDavid\n", "completion": "Well, using python's MySQLdb, I use this:\n\nconnection = MySQLdb.Connect(host='**', user='**', passwd='**', db='**')\ncursor = connection.cursor()\nquery = \"LOAD DATA INFILE '/path/to/my/file' INTO TABLE sometable FIELDS TERMINATED BY ';' ENCLOSED BY '\\\"' ESCAPED BY '\\\\\\\\'\"\ncursor.execute( query )\nconnection.commit()\n\n\nreplacing the host/user/passwd/db as appropriate for your needs.  This is based on the MySQL docs here,  The exact LOAD DATA INFILE statement would depend on your specific requirements etc (note the FIELDS TERMINATED BY, ENCLOSED BY, and ESCAPED BY statements will be specific to the type of file you are trying to read in). \n"}
{"prompt": "Python rather stupidly has a pragma directive in its include files that forces a link against python26_d.lib when the DEBUG preprocessor variable is defined. This is a problem because the python installer doesn't come with python26_d.lib! So I can't build applications in msvc in debug mode. If i temporarily #undef DEBUG for just one file I get many complaints about inconsistent DLL linkage. If I change the pragma in pythons include file I get undefined references to various debug functions.\n\nI have tried compiling my own version of python but its somehow different enough from the python that gets distributed that I can't use my modules with apps built with the vanilla version of python\n\nCan anyone give me any advice on how to get round this?\n\nThanks\n", "completion": "From python list\n\n\n  As a workaround to the situation, try\n  to copy the file python26.dll to\n  python26_d.dll.  (I'm not sure this\n  will work; you say you are building a\n  SWIG library in debug mode, and it's\n  possible that SWIG will try to use\n  features of the Python debugging\n  version.  If that's the case, you'll\n  have no choice but to use the\n  debugging version of Python.)\n\n\nEdit: From comments:\n\n\n  You should also edit pyconfig.h and\n  comment out the line \"#define\n  Py_DEBUG\" (line 374)\n\n"}
{"prompt": "x=True\ndef stupid():\n    x=False\nstupid()\nprint x\n\n", "completion": "You don't need to declare a function-local variable in Python.  The \"x=False\" is referring to an x local to stupid().  If you really want to modify the global x inside stupid:\n\ndef stupid():\n    global x\n    x=False\n\n"}
{"prompt": "I'm becoming acquainted with python and am creating problems in order to help myself learn the ins and outs of the language.  My next problem comes as follows:\n\nI have copied and pasted a huge slew of text from the internet, but the copy and paste added several new lines to break up the huge string.  I wish to programatically remove all of these and return the string into a giant blob of characters.  This is obviously a job for regex (I think), and parsing through the file and removing all instances of the newline character sounds like it would work, but it doesn't seem to be going over all that well for me.  \n\nIs there an easy way to go about this?  It seems rather simple.\n", "completion": "The two main alternatives: read everything in as a single string and remove newlines:\n\nclean = open('thefile.txt').read().replace('\\n', '')\n\n\nor, read line by line, removing the newline that ends each line, and join it up again:\n\nclean = ''.join(l[:-1] for l in open('thefile.txt'))\n\n\nThe former alternative is probably faster, but, as always, I strongly recommend you MEASURE speed (e.g., use python -mtimeit) in cases of your specific interest, rather than just assuming you know how performance will be.  REs are probably slower, but, again: don't guess, MEASURE!\n\nSo here are some numbers for a specific text file on my laptop:\n\n$ python -mtimeit -s\"import re\" \"re.sub('\\n','',open('AV1611Bible.txt').read())\"\n10 loops, best of 3: 53.9 msec per loop\n$ python -mtimeit \"''.join(l[:-1] for l in open('AV1611Bible.txt'))\"\n10 loops, best of 3: 51.3 msec per loop\n$ python -mtimeit \"open('AV1611Bible.txt').read().replace('\\n', '')\"\n10 loops, best of 3: 35.1 msec per loop\n\n\nThe file is a version of the KJ Bible, downloaded and unzipped from here (I do think it's important to run such measurements on one easily fetched file, so others can easily reproduce them!).\n\nOf course, a few milliseconds more or less on a file of 4.3 MB, 34,000 lines, may not matter much to you one way or another; but as the fastest approach is also the simplest one (far from an unusual occurrence, especially in Python;-), I think that's a pretty good recommendation.\n"}
{"prompt": "I know it can be done, but I never remember how.\n\nHow can you reimport a module in python? \nThe scenario is as follows: I import a module interactively and tinker with it, but then I face an error. I fix the error in the .py file and then I want to reimport the fixed module without quitting python. How can I do it ?\n", "completion": "This should work:\n\nreload(my.module)\n\n\nFrom the Python docs\n\n\n  Reload a previously imported module. The argument must be a module object, so it must have been successfully imported before. This is useful if you have edited the module source file using an external editor and want to try out the new version without leaving the Python interpreter.\n\n"}
{"prompt": "Apologies, I am completely new to Django and Python.\n\nI have 2 questions. First, how would I go about getting the last object created (or highest pk) in a list of objects? For example, I know that I could use the following to get the first object:\n\nlist = List.objects.all()[0]\n\n\nIs there a way to get the length of List.objects? I've tried List.objects.length but to no avail.\n\nSecond, is it possible to create simultaneous filters or combine lists? Here is an example:\n\ndef findNumber(request, number)\n    phone_list = Numbers.objects.filter(cell=number)\n\n\nI want something like the above, but more like:\n\ndef findNumber(request, number)\n    phone_list = Numbers.objects.filter(cell=number or home_phone=number)\n\n\nWhat is the correct syntax, if any?\n", "completion": "I haven't tried this yet, but I'd look at the latest() operator on QuerySets:\n\n\n  latest(field_name=None)\n  \n  Returns the latest object in the\n  table, by date, using the field_name\n  provided as the date field.\n  \n  This example returns the latest Entry\n  in the table, according to the\n  pub_date field:\n  \n  Entry.objects.latest('pub_date')\n  \n  If your model's Meta specifies\n  get_latest_by, you can leave off the\n  field_name argument to latest().\n  Django will use the field specified in\n  get_latest_by by default.\n  \n  Like get(), latest() raises\n  DoesNotExist if an object doesn't\n  exist with the given parameters.\n  \n  Note latest() exists purely for\n  convenience and readability.\n\n\nAnd the model docs on get_latest_by:\n\n\n  get_latest_by\n  \n  Options.get_latest_by\n  \n  The name of a DateField or DateTimeField in the model. This specifies the default field to use in your model Manager's latest method.\n  \n  Example:\n  \n  get_latest_by = \"order_date\"\n  \n  See the docs for latest() for more.\n\n\nEdit: Wade has a good answer on Q() operator.\n"}
{"prompt": "I was wondering if there was anything similar like Mechanize or BeautifulSoup for PHP?\n", "completion": "SimpleTest provides you with similar functionality:\n\nhttp://www.simpletest.org/en/browser_documentation.html\n"}
{"prompt": "I am in the final stretch of a project I have been working on.  Everything is running smoothly but I have a bottleneck that I am having trouble working around.\n\nI have a list of tuples.  The list ranges in length from say 40,000 - 1,000,000 records.  Now I have a dictionary where each and every (value, key) is a tuple in the list.\n\nSo, I might have\n\nmyList = [(20000, 11), (16000, 4), (14000, 9)...]\nmyDict = {11:20000, 9:14000, ...}\n\n\nI want to remove each (v, k) tuple from the list.\n\nCurrently I am doing:\n\nfor k, v in myDict.iteritems():\n    myList.remove((v, k))\n\n\nRemoving 838 tuples from the list containing 20,000 tuples takes anywhere from 3 - 4 seconds.  I will most likely be removing more like 10,000 tuples from a list of 1,000,000 so I need this to be faster.\n\nIs there a better way to do this?\n\nI can provide code used to test, plus pickled data from the actual application if needed.\n", "completion": "You'll have to measure, but I can imagine this to be more performant:\n\nmyList = filter(lambda x: myDict.get(x[1], None) != x[0], myList)\n\n\nbecause the lookup happens in the dict, which is more suited for this kind of thing. Note, though, that this will create a new list before removing the old one; so there's a memory tradeoff. If that's an issue, rethinking your container type as jkp suggest might be in order.\n\nEdit: Be careful, though, if None is actually in your list -- you'd have to use a different \"placeholder.\"\n"}
{"prompt": "I've been playing with various ways of doing literate programming in Python.  I like noweb, but I have two main problems with it: first, it is hard to build on Windows, where I spend about half my development time; and second, it requires me to indent each chunk of code as it will be in the final program --- which I don't necessarily know when I write it.  I don't want to use Leo, because I'm very attached to Emacs.\n\nIs there a good literate programming tool that:\n\n\nRuns on Windows\nAllows me to set the indentation of the chunks when they're used, not when they're written\nStill lets me work in Emacs\n\n\nThanks!\n\n\n\nCorrection: noweb does allow me to indent later --- I misread the paper I found on it.  \n\n\n  \n    \n      By default, notangle preserves whitespace and maintains indentation when expanding chunks.  It can therefore be used with languages like Miranda and Haskell, in which indentation is significant\n    \n  \n\n\nThat leaves me with only the \"Runs on Windows\" problem.\n", "completion": "I have written Pweave http://mpastell.com/pweave, that is aimed for dynamic report generation and uses noweb syntax. It is a pure python script so it also runs on Windows. It doesn't fix your indent problem, but maybe you can modify it for that, the code is really quite simple. \n"}
{"prompt": "In this Django Doc explain how to create a formset that allows you to edit books belonging to a particular author.\n\nWhat I want to do is: Create a formset that allows you to ADD new book belonging to a NEW author... Add the Book and their Authors in the same formset.\n\nCan you gime a light? thanks.\n", "completion": "When you're instantiating the form and formset for the initial display, you don't need to provide an instance - so you will just get blank forms.\n\nWhen you pass in the data on POST, you can do the form first, save it, and get an instance. Then you can pass that instance into the formset, so that it correctly saves the related objects.\n"}
{"prompt": "In GTK, how can I scale an image? Right now I load images with PIL and scale them beforehand, but is there a way to do it with GTK?\n", "completion": "Load the image from a file using gtk.gdk.Pixbuf for that:\n\nimport gtk\npixbuf = gtk.gdk.pixbuf_new_from_file('/path/to/the/image.png')\n\n\nthen scale it:\n\npixbuf = pixbuf.scale_simple(width, height, gtk.gdk.INTERP_BILINEAR)\n\n\nThen, if you want use it in a gtk.Image, crate the widget and set the image from the pixbuf.\n\nimage = gkt.Image()\nimage.set_from_pixbuf(pixbuf)\n\n\nOr maybe in a direct way:\n\nimage = gtk.image_new_from_pixbuf(pixbuf)\n\n"}
{"prompt": "Most concise way to check whether a list is empty or contains only None?\n\nI understand that I can test:\n\nif MyList:\n    pass\n\n\nand:\n\nif not MyList:\n    pass\n\n\nbut what if the list has an item (or multiple items), but those item/s are None:\n\nMyList = [None, None, None]\nif ???:\n    pass\n\n", "completion": "One way is to use all and a list comprehension:\n\nif all(e is None for e in myList):\n    print('all empty or None')\n\n\nThis works for empty lists as well. More generally, to test whether the list only contains things that evaluate to False, you can use any:\n\nif not any(myList):\n    print('all empty or evaluating to False')\n\n"}
{"prompt": "I'm a complete Python noob. How can I remove two characters from the beginning of each line in a file? I was trying something like this:\n\n#!/Python26/\n\nimport re\n\nf = open('M:/file.txt')\nlines=f.readlines()\n\ni=0;\nfor line in lines:\n    line = line.strip()     \n    #do something here\n\n", "completion": "You were off to a good start. Try this in your loop:\n\nfor line in lines:\n    line = line[2:]\n    # do something here\n\n\nThe [2:] is called \"slice\" syntax, it essentially says \"give me the part of this sequence which begins at index 2 and continues to the end (since no end point was specified after the colon).\n"}
{"prompt": "I have  generator object returned by multiple yield. Preparation to call this generator is rather time-consuming operation. That is why I want to reuse generator several times.\n\ny = FunctionWithYield()\nfor x in y: print(x)\n#here must be something to reset 'y'\nfor x in y: print(x)\n\n\nOf course, I'm taking in mind copying content into simple list. \n", "completion": "Generators can't be rewound. You have the following options:\n\n\nRun the generator function again, restarting the generation:\n\ny = FunctionWithYield()\nfor x in y: print(x)\ny = FunctionWithYield()\nfor x in y: print(x)\n\nStore the generator results in a data structure on memory or disk which you can iterate over again:\n\ny = list(FunctionWithYield())\nfor x in y: print(x)\n# can iterate again:\nfor x in y: print(x)\n\n\n\nThe downside of option 1 is that it computes the values again. If that's CPU-intensive you end up calculating twice. On the other hand, the downside of 2 is the storage. The entire list of values will be stored on memory. If there are too many values, that can be unpractical.\n\nSo you have the classic memory vs. processing tradeoff. I can't imagine a way of rewinding the generator without either storing the values or calculating them again.\n"}
{"prompt": "I have several buttons in various sizers and they expand in the way that I want them to. However, when I add the parent to a new wx.BoxSizer that is used to add a border around all the elements in the frame, the sizer that has been added functions correctly vertically, but not horizontally.\n\nThe following code demonstrates the problem:\n\n#! /usr/bin/env python\n\nimport wx\nimport webbrowser\n\nclass App(wx.App):\n\n\n    def OnInit(self):\n       frame = MainFrame()\n       frame.Show()\n       self.SetTopWindow(frame)\n       return True\n\n\nclass MainFrame(wx.Frame):\n\n    title = 'Title'\n\n\n    def __init__(self):\n        wx.Frame.__init__(self, None, -1, self.title)\n\n        panel = wx.Panel(self)\n\n        #icon = wx.Icon('icon.png', wx.BITMAP_TYPE_PNG)\n        #self.SetIcon(icon)\n\n        sizer = wx.FlexGridSizer(rows=2, cols=1, vgap=10, hgap=10)\n\n        button1 = wx.Button(panel, -1, 'BUTTON')\n        sizer.Add(button1, 0, wx.EXPAND)\n\n        buttonSizer = wx.FlexGridSizer(rows=1, cols=4, vgap=10, hgap=5)\n\n        buttonDelete = wx.Button(panel, -1, 'Delete')\n        buttonSizer.Add(buttonDelete, 0, 0)\n\n        buttonEdit = wx.Button(panel, -1, 'Edit')\n        buttonSizer.Add(buttonEdit, 0, 0)\n\n        buttonNew = wx.Button(panel, -1, 'New')\n        buttonSizer.Add(buttonNew, 0, 0)\n\n        buttonSizer.AddGrowableCol(0, 0)\n\n        sizer.Add(buttonSizer, 0, wx.EXPAND|wx.HORIZONTAL)\n\n        sizer.AddGrowableCol(0, 0)\n        sizer.AddGrowableRow(0, 0)\n\n        mainSizer = wx.BoxSizer(wx.EXPAND)\n        mainSizer.Add(sizer, 0, wx.EXPAND|wx.ALL, 10)\n\n        #panel.SetSizerAndFit(sizer)\n        #sizer.SetSizeHints(self)\n        panel.SetSizerAndFit(mainSizer)\n        mainSizer.SetSizeHints(self)\n\n\nif __name__ == '__main__':\n    app = App(False)\n    app.MainLoop()\n\n\nCommenting out lines 57 and 58 and uncommenting lines 55 and 56 removes the extra BoxSizer and shows how I expect everything to function (without the whitespace of course).\n\nI am completely stuck with this problem and still have no clue as to how to fix it.\n", "completion": "First of all, you're passing some flags incorrectly.  BoxSizer takes wx.HORIZONTAL or wx.VERTICAL, not wx.EXPAND.  sizer.Add does not take wx.HORIZONTAL.\n\nIf you have a VERTICAL BoxSizer, wx.EXPAND will make the control fill horizontally, while a proportion of 1 or more (second argument to Add) will make the control fill vertically.  It's the opposite for HORIZONTAL BoxSizers.\n\nsizer = wx.BoxSizer(wx.VERTICAL)\nsizer.Add(widget1, 0, wx.EXPAND)\nsizer.Add(widget2, 1)\n\n\nwidget1 will expand horizontally.  widget2 will expand vertically.\n\nIf you put a sizer in another sizer, you need to be sure to have its proportion and EXPAND flags set so that its insides will grow how you want them to.\n\nI'll leave the rest to you.\n"}
{"prompt": "Does anyone know if there's an existing module/function inside Ruby to traverse file system directories and files?  I'm looking for something similar to Python's os.walk. The closest module I've found is Find but requires some extra work to do the traversal.\n\nThe Python code looks like the following:\n\nfor root, dirs, files in os.walk('.'):\n    for name in files:\n        print name\n    for name in dirs:\n        print name\n\n", "completion": "The following will print all files recursively. Then you can use File.directory? to see if the it is a directory or a file.\n\nDir['**/*'].each { |f| print f }\n\n"}
{"prompt": "How can I implement Comet / Server push in Google App Engine in Python?\n", "completion": "We just announced the Channel API to do comet push with App Engine apps: http://googleappengine.blogspot.com/2010/05/app-engine-at-google-io-2010.html\n\nIf you're at Google IO, I'll be talking about this at 1pm tomorrow (on the APIs track): http://code.google.com/events/io/2010/sessions/building-real-time-apps-app-engine-feed-api.html\n\nHere's the YouTube video of the session: http://www.youtube.com/watch?v=oMXe-xK0BWA\n\nHopefully last update! This is now released: code.google.com/appengine/docs/python/channel\n"}
{"prompt": "Is there a recommended package for machine learning in Python?\n\nI have previous experience in implementing a variety of machine learning and statistical algorithms in C++ and MATLAB, but having done some work in Python I'm curious about the available packages for Python.\n", "completion": "There is also scikit-learn (BSD, with only dependencies on numpy & scipy). It includes various supervised learning algorithms such as:\n\n\nSVM based on libsvm and linear with scipy.sparse bindings for wide features datasets\nbayesian methods\nHMMs\nL1 and L1+L2 regularized regression methods aka Lasso and Elastic Net models implemented with algorithms such as LARS and coordinate descent\n\n\nIt also features unsupervised clustering algorithms such as:\n\n\nkmeans++\nmeanshift\naffinity propagation\nspectral clustering\n\n\nAnd also other tools such as:\n\n\nfeature extractors for text content (token and char ngrams + hashing vectorizer)\nunivariate feature selections\na simple pipe line tool\nnumerous implementations of cross validation strategies\nperformance metrics evaluation and ploting (ROC curve, AUC, confusion matrix, ...)\na grid search utility to perform hyper-parameters tuning using parallel cross validation\nintegration with joblib for caching partial results when working in interactive environment (e.g. using ipython)\n\n\nEach algorithm implementation comes with sample programs demonstrating its usage either on toy data or real life datasets.\n\nAlso, the official source repository is hosted on github so please feel free to contribute bugfixes and improvement using the regular pull request feature for interactive code review.\n"}
{"prompt": "I tried IronPython some time ago and it seemed that it implements only python language, and uses .NET for libraries. Is this still the case? Can one use python modules from IronPython?\n", "completion": "The IronPython installer includes the Python standard library. Otherwise, you can use the standard library from a compatible Python install (IPy 2.0 -> CPy 2.5, IPy 2.6 -> CPy 2.6). Either copy the Python Lib directory to the IronPython folder, or set IRONPYTHONPATH.\n\nDo note that only the pure Pyton modules will be available; Python modules that require C extensions have to be re-implemented (and most of them have been).\n"}
{"prompt": "I have a postscript file and want it to be printed on a IPP capable device (or CUPS server). What is the minimal code and dependencies I could get away with to do that.\n\nUsing LPR or libcups gives me lot of cross-plattform dependencies. So my first approach was to implement a minimal subset of IPP (the protocol used by cups and many modern printers) since \"it's only extended HTTP\". But unfortuntely a IPP client is a lot more code than a few lines and so far I found no IPP client implementation meant for just printing and not managing a printserver.\n\nI would prefer a solution in Python, but would also be happy with something in an oter dynamic language.\n", "completion": "you need to add remote printer to CUPS:\n\nlpadmin -p printername -E -v //IPADDRESS/spool -m driver.ppd\n\n\nwhere driver.ppd is the driver to print with\n\nps: this could also work for programatic access, if printer is set before.\n"}
{"prompt": "I'm about to begin my next web development project and wanted to hear about the merits of Lua within the web-development space.\n\nHow does Lua compare to PHP/Python/JSP/etc.. for web development?\n\nAny reason why Lua would be a poor choice for a web application language vs the others?\n", "completion": "In brief:\n\n\nLua gives you a smaller, simpler system that you can understand in its entirety, but it is in a much smaller ecosystem; Kepler is all you get, and you will probably have to build some of your own stuff.  I find this easy and fun (I make heavy use of the Lua bindings to the Expat parser and the Lua Object Model, which are part of Kepler), but others may prefer to use what everyone else is using.\nPHP started out as more of a macro processor than a language, and although it has improved over the years, when people say \"X has really gotten a lot better\", I tend to be wary of\u00a0X.  I\u00a0find PHP offputting, but there is a huge ecosystem for web development.\nPython is a nice language but much bigger than Lua, and in the throes of a major revision (transition from 2.5 to 3.x).  Again you get a big ecosystem; the problem I have with Python is that the language and system are too big for any one person to understand all of.  I\u00a0don't like to be in this situation if I don't have to.\nRuby is a bit of a cleaner language design, and the large Rails ecosystem is a winner.  Ruby is less complex than Python but more complex than Lua.  Rails is a bit of a beast.\n\n\nIt comes down to this question:\n\n\nWould you rather understand all the software in your system, even if you have to build a lot of things yourself?\nOr would you rather have a lot of things already built for you, even if you wind up not understanding exactly how every piece works?\n\n\nIf you want to understand everything, Lua is your game.  If you want a lot of stuff already built for you, I\u00a0can't advise you how to pick among PHP/Python/JSP/Rails and so on.\n\nMore on Lua and Python at Which language is better to use, Lua or Python?\n"}
{"prompt": "Amazon Product API now requires a signature with every request which I'm trying to generate ushing Python.\n\nThe step I get hung up on is this one:\n\n\"Calculate an RFC 2104-compliant HMAC with the SHA256 hash algorithm using the string above with our \"dummy\" Secret Access Key: 1234567890. For more information about this step, see documentation and code samples for your programming language.\" \n\nGiven a string and a secret key (in this case 1234567890) how do I calculate this hash using Python?\n\n----------- UPDATE -------------\n\nThe first solution using HMAC.new looks correct however I'm getting a different result than they are.\n\nhttp://docs.amazonwebservices.com/AWSECommerceService/latest/DG/index.html?rest-signature.html\n\nAccording to Amazon's example when you hash the secret key 1234567890 and the following string\n\nGET\nwebservices.amazon.com\n/onca/xml\nAWSAccessKeyId=00000000000000000000&ItemId=0679722769&Operation=I\ntemLookup&ResponseGroup=ItemAttributes%2COffers%2CImages%2CReview\ns&Service=AWSECommerceService&Timestamp=2009-01-01T12%3A00%3A00Z&\nVersion=2009-01-06\n\n\nYou should get the following signature: 'Nace+U3Az4OhN7tISqgs1vdLBHBEijWcBeCqL5xN9xg='\n\nI am getting this:  '411a59403c9f58b4a434c9c6a14ef6e363acc1d1bb2c6faf9adc30e20898c83b'\n", "completion": "import hmac\nimport hashlib\nimport base64\ndig = hmac.new(b'1234567890', msg=your_bytes_string, digestmod=hashlib.sha256).digest()\nbase64.b64encode(dig).decode()      # py3k-mode\n'Nace+U3Az4OhN7tISqgs1vdLBHBEijWcBeCqL5xN9xg='\n\n"}
{"prompt": "I have the following code which I use to map a nested list in Python to produce a list with the same structure. \n\n>>> nested_list = [['Hello', 'World'], ['Goodbye', 'World']]\n>>> [map(str.upper, x) for x in nested_list]\n[['HELLO', 'WORLD'], ['GOODBYE', 'WORLD']]\n\n\nCan this be done with list comprehension alone (without using the map function)?\n", "completion": "For nested lists you can use nested list comprehensions:\n\nnested_list = [[s.upper() for s in xs] for xs in nested_list]\n\n\nPersonally I find map to be cleaner in this situation, even though I almost always prefer list comprehensions.  So it's really your call, since either will work.\n"}
{"prompt": "I have two PNGs that I am trying to combine into a PDF using ReportLab 2.3 on Python 2.5.  When I use canvas.drawImage(ImageReader) to write either PNG onto the canvas and save, the transparency comes out black.  If I use PIL (1.1.6) to generate a new Image, then paste() either PNG onto the PIL Image, it composits just fine.  I've double checked in Gimp and both images have working alpha channels and are being saved correctly.  I'm not receiving an error and there doesn't seem to be anything my google-fu can turn up.  \n\nHas anybody out there composited a transparent PNG onto a ReportLab canvas, with the transparency working properly?  Thanks!\n", "completion": "Passing the mask parameter with a value of 'auto' to drawImage fixes this for me.\n\ndrawImage(......., mask='auto')\n\n\nMore information on the drawImage-function\n"}
{"prompt": "I have written a function comp(time1, time2) which will return true when time1 is lesser than time2. I have a scenario where time1 should always be lesser than time2. I need time1 to have the least possible value(date). How to find this time and how to form the corresponding object.\n", "completion": "If using the datetime module, date, time, and datetime objects all have a min and max attribute.\n\n>>> from datetime import date, time, datetime\n>>> date.min\ndatetime.date(1, 1, 1)\n>>> date.max\ndatetime.date(9999, 12, 31)\n>>> time.min\ndatetime.time(0, 0)\n>>> time.max\ndatetime.time(23, 59, 59, 999999)\n>>> datetime.min\ndatetime.datetime(1, 1, 1, 0, 0)\n>>> datetime.max\ndatetime.datetime(9999, 12, 31, 23, 59, 59, 999999)\n\n"}
{"prompt": "I am trying to find corresponding keys in two different dictionaries.  Each has about 600k entries.\n\nSay for example:\n\n    myRDP = { 'Actinobacter': 'GATCGA...TCA', 'subtilus sp.': 'ATCGATT...ACT' }\n    myNames = { 'Actinobacter': '8924342' }\n\n\nI want to print out the value for Actinobacter (8924342) since it matches a value in myRDP.\n\nThe following code works, but is very slow:\n\n    for key in myRDP:\n        for jey in myNames:\n            if key == jey:\n                print key, myNames[key]\n\n\nI've tried the following but it always results in a KeyError:\n\n    for key in myRDP:\n        print myNames[key]\n\n\nIs there perhaps a function implemented in C for doing this?  I've googled around but nothing seems to work.\n\nThanks.\n", "completion": "You could do this:\n\nfor key in myRDP:\n    if key in myNames:\n        print key, myNames[key]\n\n\nYour first attempt was slow because you were comparing every key in myRDP with every key in myNames.  In algorithmic jargon, if myRDP has n elements and myNames has m elements, then that algorithm would take O(n\u00c3\u0097m) operations.  For 600k elements each, this is 360,000,000,000 comparisons!\n\nBut testing whether a particular element is a key of a dictionary is fast -- in fact, this is one of the defining characteristics of dictionaries.  In algorithmic terms, the key in dict test is O(1), or constant-time.  So my algorithm will take O(n) time, which is one 600,000th of the time.\n"}
{"prompt": "I need to run a simple script after the modules and programs have been installed.\nI'm having a little trouble finding straight-forward documentation on how to do this. It looks like I need to inherit from distutils.command.install, override some methods and add this object to the setup script. The specifics are a bit hazy though and it seems like a lot of effort for such a simple hook. Does anyone know an easy way to do this? \n", "completion": "I dug through distutils source for a day to learn enough about it to make a bunch of custom commands. It's not pretty, but it does work.\n\nimport distutils.core\nfrom distutils.command.install import install\n...\nclass my_install(install):\n    def run(self):\n        install.run(self)\n        # Custom stuff here\n        # distutils.command.install actually has some nice helper methods\n        # and interfaces. I strongly suggest reading the docstrings.\n...\ndistutils.core.setup(..., cmdclass=dict(install=my_install), ...)\n\n"}
{"prompt": "Folks,\n\nis there a collection of gotchas where Numpy differs from python,\npoints that have puzzled and cost time ?\n\n\n  \"The horror of that moment I shall\n  never never forget !\"\n  \"You will, though,\" the Queen said, \"if you don't\n  make a memorandum of it.\"\n\n\nFor example, NaNs are always trouble, anywhere.\nIf you can explain this without running it, give yourself a point --\n\nfrom numpy import array, NaN, isnan\n\npynan = float(\"nan\")\nprint pynan is pynan, pynan is NaN, NaN is NaN\na = (0, pynan)\nprint a, a[1] is pynan, any([aa is pynan for aa in a])\n\na = array(( 0, NaN ))\nprint a, a[1] is NaN, isnan( a[1] )\n\n\n(I'm not knocking numpy, lots of good work there, just think a FAQ or Wiki of gotchas would be useful.)\n\nEdit: I was hoping to collect half a dozen gotchas (surprises for people learning Numpy).\nThen, if there are common gotchas or, better, common explanations,\nwe could talk about adding them to a community Wiki (where ?)\nIt doesn't look like we have enough so far.\n", "completion": "Because __eq__ does not return a bool, using numpy arrays in any kind of containers prevents equality testing without a container-specific work around.\n\nExample:\n\n>>> import numpy\n>>> a = numpy.array(range(3))\n>>> b = numpy.array(range(3))\n>>> a == b\narray([ True,  True,  True], dtype=bool)\n>>> x = (a, 'banana')\n>>> y = (b, 'banana')\n>>> x == y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\nThis is a horrible problem.  For example, you cannot write unittests for containers which use TestCase.assertEqual() and must instead write custom comparison functions.  Suppose we write a work-around function special_eq_for_numpy_and_tuples.  Now we can do this in a unittest:\n\nx = (array1, 'deserialized')\ny = (array2, 'deserialized')\nself.failUnless( special_eq_for_numpy_and_tuples(x, y) )\n\n\nNow we must do this for every container type we might use to store numpy arrays.  Furthermore, __eq__ might return a bool rather than an array of bools:\n\n>>> a = numpy.array(range(3))\n>>> b = numpy.array(range(5))\n>>> a == b\nFalse\n\n\nNow each of our container-specific equality comparison functions must also handle that special case.\n\nMaybe we can patch over this wart with a subclass?\n\n>>> class SaneEqualityArray (numpy.ndarray):\n...   def __eq__(self, other):\n...     return isinstance(other, SaneEqualityArray) and self.shape == other.shape and (numpy.ndarray.__eq__(self, other)).all()\n... \n>>> a = SaneEqualityArray( (2, 3) )\n>>> a.fill(7)\n>>> b = SaneEqualityArray( (2, 3) )\n>>> b.fill(7)\n>>> a == b\nTrue\n>>> x = (a, 'banana')\n>>> y = (b, 'banana')\n>>> x == y\nTrue\n>>> c = SaneEqualityArray( (7, 7) )\n>>> c.fill(7)\n>>> a == c\nFalse\n\n\nThat seems to do the right thing.  The class should also explicitly export elementwise comparison, since that is often useful.\n"}
{"prompt": "I wonder what is better to do:\n\nd = {'a': 1, 'b': 2}\n'a' in d\nTrue\n\n\nor:\n\nd = {'a': 1, 'b': 2}\nd.has_key('a')\nTrue\n\n", "completion": "in is definitely more pythonic.\n\nIn fact has_key() was removed in Python 3.x.\n"}
{"prompt": "I have the following simple python test script that uses Suds to call a SOAP web service (the service is written in ASP.net):\n\nfrom suds.client import Client\n\nurl = 'http://someURL.asmx?WSDL'\n\nclient = Client( url )\n\nresult = client.service.GetPackageDetails( \"MyPackage\"  )\n\nprint result\n\n\nWhen I run this test script I am getting the following error (used code markup as it doesn't wrap):\n\nNo handlers could be found for logger \"suds.bindings.unmarshaller\"\nTraceback (most recent call last):\n  File \"sudsTest.py\", line 9, in <module>\n    result = client.service.GetPackageDetails( \"t3db\"  )\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 240, in __call__\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 379, in call\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 240, in __call__\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 422, in call\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 480, in invoke\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 505, in send\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/client.py\", line 537, in succeeded\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/binding.py\", line 149, in get_reply\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 303, in process\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 88, in process\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 104, in append\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 181, in append_children\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 104, in append\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 181, in append_children\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 104, in append\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 181, in append_children\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 102, in append\n  File \"build/bdist.cygwin-1.5.25-i686/egg/suds/bindings/unmarshaller.py\", line 324, in start\nsuds.TypeNotFound: Type not found: 'xs:complexType'\n\n\nLooking at the source for the WSDL file's header (reformatted to fit):\n\n<?xml version=\"1.0\" encoding=\"utf-8\" ?> \n<wsdl:definitions xmlns:http=\"http://schemas.xmlsoap.org/wsdl/http/\" \nxmlns:soap=\"http://schemas.xmlsoap.org/wsdl/soap/\" \nxmlns:s=\"http://www.w3.org/2001/XMLSchema\" \nxmlns:soapenc=\"http://schemas.xmlsoap.org/soap/encoding/\" \nxmlns:tns=\"http://http://someInternalURL/webservices.asmx\" \nxmlns:tm=\"http://microsoft.com/wsdl/mime/textMatching/\" \nxmlns:mime=\"http://schemas.xmlsoap.org/wsdl/mime/\" \ntargetNamespace=\"http://someURL.asmx\" \nxmlns:wsdl=\"http://schemas.xmlsoap.org/wsdl/\">\n\n\nI am guessing based on the last line of output:\n\nsuds.TypeNotFound: Type not found: 'xs:complexType'\n\n\nThat I need to use Sud's doctor class to fix the schema but being a SOAP newbie I don't know what exactly needs fixed in my case.  Does anyone here have any experience using Suds to fix/correct schema?\n", "completion": "Ewall's resource is a good one. If you try to search in suds trac tickets, you could see that other people have problems similar to yours, but with different object types. It can be a good way to learn from it's examples and how they import their namespaces.\n\n\n  The problem is that your wsdl contains\n  a schema definition that references\n  the (...) but fails to import\n  the\n  \"http://schemas.xmlsoap.org/soap/encoding/\"\n  namespace (and associated schema)\n  properly. The schema can be patched at\n  runtime using the schema ImportDoctor\n  as discussed here:\n  https://fedorahosted.org/suds/wiki/Documentation#FIXINGBROKENSCHEMAs.\n  \n  This is a fairly common problem.\n  \n  A commonly referenced schema (that is\n  not imported) is the SOAP section 5\n  encoding schema. This can now be fixed\n  as follows:\n\n\n(all emphasis were mine).\n\nYou could try the lines that these documentations provide adding the namespaces presented in your WSDL. This can be a try-and-error aproach.\n\nimp = Import('http://schemas.xmlsoap.org/soap/encoding/')\n# Below is your targetNamespace presented in WSDL. Remember\n# that you can add more namespaces by appending more imp.filter.add\nimp.filter.add('http://someURL.asmx') \ndoctor = ImportDoctor(imp) \nclient = Client(url, doctor=doctor)\n\n\nYou didn't provide the WSDL you're working with,  I suppose you have reasons to not showing to us... so I think you have to try these possibilities by yourself. Good luck!\n"}
{"prompt": "Newbie to Python, so this may seem silly.\n\nI have two dicts:\n\ndefault = {'a': 'alpha', 'b': 'beta', 'g': 'Gamma'}\nuser = {'a': 'NewAlpha', 'b': None}\n\n\nI need to update my defaults with the values that exist in user.  But only for those that have a value not equal to None.  So I need to get back a new dict:\n\nresult = {'a': 'NewAlpha', 'b': 'beta', 'g': 'Gamma'}\n\n", "completion": "result = default.copy()\nresult.update((k, v) for k, v in user.iteritems() if v is not None)\n\n"}
{"prompt": "I have a string that looks like so:\n\n6\u00c3\u0082\u00c2\u00a0918\u00c3\u0082\u00c2\u00a0417\u00c3\u0082\u00c2\u00a0712\n\n\nThe clear cut way to trim this string (as I understand Python) is simply to say the string is in a variable called s, we get:\n\ns.replace('\u00c3\u0082 ', '')\n\n\nThat should do the trick. But of course it complains that the non-ASCII character '\\xc2' in file blabla.py is not encoded.\n\nI never quite could understand how to switch between different encodings.\n\nHere's the code, it really is just the same as above, but now it's in context. The file is saved as UTF-8 in notepad and has the following header:\n\n#!/usr/bin/python2.4\n# -*- coding: utf-8 -*-\n\n\nThe code:\n\nf = urllib.urlopen(url)\n\nsoup = BeautifulSoup(f)\n\ns = soup.find('div', {'id':'main_count'})\n\n#making a print 's' here goes well. it shows 6\u00c3\u0082 918\u00c3\u0082 417\u00c3\u0082 712\n\ns.replace('\u00c3\u0082 ','')\n\nsave_main_count(s)\n\n\nIt gets no further than s.replace...\n", "completion": "def removeNonAscii(s): return \"\".join(filter(lambda x: ord(x)<128, s))\n\n\nedit: my first impulse is always to use a filter, but the generator expression is more memory efficient (and shorter)...\n\ndef removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n\n\nKeep in mind that this is guaranteed to work with UTF-8 encoding (because all bytes in multi-byte characters have the highest bit set to 1).\n"}
{"prompt": "How can I flip the origin of a matplotlib plot to be in the upper-left corner - as opposed to the default lower-left?  I'm using matplotlib.pylab.plot to produce the plot (though if there is another plotting routine that is more flexible, please let me know).\n\nI'm looking for the equivalent of the matlab command:  axis ij;\n\nAlso, I've spent a couple hours surfing matplotlib help and google but haven't come up with an answer.  Some info on where I could have looked up the answer would be helpful as well.\n", "completion": "axis ij just makes the y-axis increase downward instead of upward, right?  If so, then matplotlib.axes.invert_yaxis() might be all you need -- but I can't test that right now.\n\nIf that doesn't work, I found a mailing post suggesting that\n\nsetp(gca(), 'ylim', reversed(getp(gca(), 'ylim')))\n\n\nmight do what you want to resemble axis ij.\n"}
{"prompt": "I'm having trouble debugging a multi-process application (specifically using a process pool in python's multiprocessing module). I have an apparent deadlock and I do not know what is causing it. The stack trace is not sufficient to describe the issue, as it only displays code in the multiprocessing module.\n\nAre there any python tools, or otherwise general techniques used to debug deadlocks?\n", "completion": "Yah, debugging deadlocks is fun.  You can set the logging level to be higher -- see the Python documentation for a description of it, but really quickly:\n\nimport multiprocessing, logging\nlogger = multiprocessing.log_to_stderr()\nlogger.setLevel(multiprocessing.SUBDEBUG)\n\n\nAlso, add logging for anything in your code that deals with a resource or whatnot that might be in contention.  Finally, shot in the dark:  spawning off child processes during an import might cause a problem.\n"}
{"prompt": "Can someone help me to find a solution on how to calculate a cubic root of the negative number using python?\n\n>>> math.pow(-3, float(1)/3)\nnan\n\n\nit does not work. Cubic root of the negative number is negative number. Any solutions?\n", "completion": "A simple use of De Moivre's formula, is sufficient  to show that the cube root of a value, regardless of sign, is a multi-valued function. That means, for any input value, there will be three solutions. Most of the solutions presented to far only return the principle root. A solution that returns all valid roots, and explicitly tests for non-complex special cases, is shown below.\n\nimport numpy\nimport math\ndef cuberoot( z ):\n    z = complex(z)\n    x = z.real\n    y = z.imag\n    mag = abs(z)\n    arg = math.atan2(y,x)\n    return [ mag**(1./3) * numpy.exp( 1j*(arg+2*n*math.pi)/3 ) for n in range(1,4) ]\n\n\nEdit: As requested, in cases where it is inappropriate to have dependency on numpy, the following code does the same thing.\n\ndef cuberoot( z ):\n    z = complex(z) \n    x = z.real\n    y = z.imag\n    mag = abs(z)\n    arg = math.atan2(y,x)\n    resMag = mag**(1./3)\n    resArg = [ (arg+2*math.pi*n)/3. for n in range(1,4) ]\n    return [  resMag*(math.cos(a) + math.sin(a)*1j) for a in resArg ]\n\n"}
{"prompt": "I have a group of objects which I am creating a class for that I want to store each object as its own text file. I would really like to store it as a Python class definition which subclasses the main class I am creating. So, I did some poking around and found a Python Code Generator on effbot.org. I did some experimenting with it and here's what I came up with:\n\n#\n# a Python code generator backend\n#\n# fredrik lundh, march 1998\n#\n# fredrik@pythonware.com\n# http://www.pythonware.com\n#\n# Code taken from http://effbot.org/zone/python-code-generator.htm\n\nimport sys, string\n\nclass CodeGeneratorBackend:\n\n    def begin(self, tab=\"\\t\"):\n        self.code = []\n        self.tab = tab\n        self.level = 0\n\n    def end(self):\n        return string.join(self.code, \"\")\n\n    def write(self, string):\n        self.code.append(self.tab * self.level + string)\n\n    def indent(self):\n        self.level = self.level + 1\n\n    def dedent(self):\n        if self.level == 0:\n            raise SyntaxError, \"internal error in code generator\"\n        self.level = self.level - 1\n\nclass Point():\n    \"\"\"Defines a Point. Has x and y.\"\"\"\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def dump_self(self, filename):\n        self.c = CodeGeneratorBackend()\n        self.c.begin(tab=\"    \")\n        self.c.write(\"class {0}{1}Point()\\n\".format(self.x,self.y))\n        self.c.indent()\n        self.c.write('\"\"\"Defines a Point. Has x and y\"\"\"\\n')\n        self.c.write('def __init__(self, x={0}, y={1}):\\n'.format(self.x, self.y))\n        self.c.indent()\n        self.c.write('self.x = {0}\\n'.format(self.x))\n        self.c.write('self.y = {0}\\n'.format(self.y))\n        self.c.dedent()\n        self.c.dedent()\n        f = open(filename,'w')\n        f.write(self.c.end())\n        f.close()\n\nif __name__ == \"__main__\":\n    p = Point(3,4)\n    p.dump_self('demo.py')\n\n\nThat feels really ugly, is there a cleaner/better/more pythonic way to do this? Please note, this is not the class I actually intend to do this with, this is a small class I can easily mock up in not too many lines. Also, the subclasses don't need to have the generating function in them, if I need that again, I can just call the code generator from the superclass.\n", "completion": "We use Jinja2 to fill in a template.  It's much simpler.\n\nThe template looks a lot like Python code with a few {{something}} replacements in it.\n"}
{"prompt": "I am sending an AJAX request to a Django view that can potentially take a lot of time. It goes through some well-defined steps, however, so I would like to print status indicators to the user letting it know when it is finished doing a certain thing and has moved on to the next.\n\nIf I was using PHP it might look like this, using the flush function:\n\ndo_something();\nprint 'Done doing something!';\nflush();\n\ndo_something_else();\nprint 'Done doing something else!';\nflush();\n\n\nHow would I go about doing the same with Django? Looking at the documentation I see that HttpResponse objects have a flush method, but all it has to say is that \"This method makes an HttpResponse instance a file-like object.\" - I'm not sure that's what I want. I'm having a hard time wrapping my head around how this could be done in Django since I have to return the response and don't really have a control of when the content goes to the browser.\n", "completion": "Most webservers (eg. FCGI/SCGI) do their own buffering, HTTP clients do their own, and so on.  It's very difficult to actually get data flushed out in this way and for the client to actually receive it, because it's not a typical operation.\n\nThe closest to what you're trying to do would be to pass an iterator to HttpResponse, and to do the work in a generator; something like this:\n\ndef index(request):\n    def do_work():\n        step_1()\n        yield \"step 1 complete\"\n        step_2()\n        yield \"step 2 complete\"\n        step_3()\n        yield \"step 3 complete\"\n    return HttpResponse(do_work())\n\n\n... but this won't necessarily flush.  (Not tested code, but you get the idea; see http://docs.djangoproject.com/en/dev/ref/request-response/#passing-iterators.)\n\nMost of the infrastructure is simply not expecting a piecemeal response.  Even if Django isn't buffering, your front-end server might be, and the client probably is, too.  That's why most things use pull updates for this: a separate interface to query the status of a long-running request.\n\n(I'd like to be able to do reliable push updates for this sort of thing, too...)\n"}
{"prompt": "To read contents of a file:\n\ndata = open(filename, \"r\").read()\n\n\nThe open file immediately stops being referenced anywhere, so the file object will eventually close... and it shouldn't affect other programs using it, since the file is only open for reading, not writing.\n\nEDIT: This has actually bitten me in a project I wrote - it prompted me to ask this question. File objects are cleaned up only when you run out of memory, not when you run out of file handles. So if you do this too often, you could end up running out of file descriptors and causing your IO attempts at opening files to throw exceptions.\n", "completion": "Just for the record:\nThis is only slightly longer, and closes the file immediately:\n\nfrom __future__ import with_statement\n\nwith open(filename, \"r\") as f:\n    data = f.read()\n\n"}
{"prompt": "Like if I have a string like str1 = \"IWantToMasterPython\"\n\nIf I want to extract \"Py\" from the above string. I write:\n\nextractedString = foo(\"Master\",\"thon\")\n\n\nI want to do all this because i am trying to extract lyrics from an html page. The lyrics are written like <div class = \"lyricbox\"> ....lyrics goes here....</div>.\n\nAny suggestions on how can I implement.\n", "completion": "The solution is to use a regexp:\n\nimport re\nr = re.compile('Master(.*?)thon')\nm = r.search(str1)\nif m:\n    lyrics = m.group(1)\n\n"}
{"prompt": "One of the things I deal with most in data cleaning is missing values. R deals with this well using its \"NA\" missing data label. In python, it appears that I'll have to deal with masked arrays which seem to be a major pain to set up and don't seem to be well documented. Any suggestions on making this process easier in Python? This is becoming a deal-breaker in moving into Python for data analysis. Thanks\n\nUpdate It's obviously been a while since I've looked at the methods in the numpy.ma module. It appears that at least the basic analysis functions are available for masked arrays, and the examples provided helped me understand how to create masked arrays (thanks to the authors). I would like to see if some of the newer statistical methods in Python (being developed in this year's GSoC) incorporates this aspect, and at least does the complete case analysis.\n", "completion": "If you are willing to consider a library, pandas (http://pandas.pydata.org/) is a library built on top of numpy which amongst many other things provides:\n\n\n  Intelligent data alignment and integrated handling of missing data: gain automatic label-based alignment in computations and easily manipulate messy data into an orderly form\n\n\nI've been using it for almost one year in the financial industry where missing and badly aligned data is the norm and it really made my life easier.\n"}
{"prompt": "I'm having some weird issues with pytz's .localize() function. Sometimes it wouldn't make adjustments to the localized datetime:\n\n.localize behaviour:\n\n>>> tz\n<DstTzInfo 'Africa/Abidjan' LMT-1 day, 23:44:00 STD> \n>>> d\ndatetime.datetime(2009, 9, 2, 14, 45, 42, 91421)\n\n>>> tz.localize(d)\ndatetime.datetime(2009, 9, 2, 14, 45, 42, 91421, \n                  tzinfo=<DstTzInfo 'Africa/Abidjan' GMT0:00:00 STD>)\n>>> tz.normalize(tz.localize(d))\ndatetime.datetime(2009, 9, 2, 14, 45, 42, 91421,\n                  tzinfo=<DstTzInfo 'Africa/Abidjan' GMT0:00:00 STD>)\n\n\nAs you can see, time has not been changed as a result of localize/normalize operations.\nHowever, if .replace is used:\n\n>>> d.replace(tzinfo=tz)\ndatetime.datetime(2009, 9, 2, 14, 45, 42, 91421, \n                  tzinfo=<DstTzInfo 'Africa/Abidjan' LMT-1 day, 23:44:00 STD>)\n>>> tz.normalize(d.replace(tzinfo=tz))\ndatetime.datetime(2009, 9, 2, 15, 1, 42, 91421,\n                  tzinfo=<DstTzInfo 'Africa/Abidjan' GMT0:00:00 STD>)\n\n\nWhich seems to make adjustments into datetime. \n\nQuestion is - which is correct and why other's wrong?\n\nThanks!\n", "completion": "localize just assumes that the naive datetime you pass it is \"right\" (except for not knowing about the timezone!) and so just sets the timezone, no other adjustments.\n\nYou can (and it's advisable...) internally work in UTC (rather than with naive datetimes) and use replace when you need to perform I/O of datetimes in a localized way (normalize will handle DST and the like).\n"}
{"prompt": "I am learning Python and creating a database connection.\nWhile trying to add to the DB, I am thinking of creating tuples out of information and then add them to the DB. \n\nWhat I am Doing:\nI am taking information from the user and store it in variables. \nCan I add these variables into a tuple?  Can you please help me with the syntax?\n\nAlso if there is an efficient way of doing this, please share...\n\nEDIT\nLet me edit this question a bit...I only need the tuple to enter info into the DB.  Once the information is added to the DB, should I delete the tuple?  I mean I don't need the tuple anymore.\n", "completion": "Tuples are immutable; you can't change which variables they contain after construction. However, you can concatenate or slice them to form new tuples:\n\na = (1, 2, 3)\nb = a + (4, 5, 6)\nc = b[1:]\n\n\nAnd, of course, build them from existing values:\n\n   name = \"Joe\"\n   age = 40\n   location = \"New York\"\n   joe = (name, age, location)\n\n"}
{"prompt": "I'm getting about the same bad looking resizing from all the 4 algorithms of PIL\n\n>>> data = utils.fetch(\"http://wavestock.com/images/beta-icon.gif\")\n>>> image = Image.open(StringIO.StringIO(data)); image.save(\"/home/ptarjan/www/tmp/metaward/original.png\")\n>>>\n>>> image = Image.open(StringIO.StringIO(data)); image.resize((36,36), Image.ANTIALIAS).save(\"/home/ptarjan/www/tmp/metaward/antialias.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.resize((36,36), Image.BILINEAR).save(\"/home/ptarjan/www/tmp/metaward/bilinear.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.resize((36,36), Image.BICUBIC).save(\"/home/ptarjan/www/tmp/metaward/bicubic.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.resize((36,36), Image.NEAREST).save(\"/home/ptarjan/www/tmp/metaward/nearest.png\")\n>>>\n>>> image = Image.open(StringIO.StringIO(data)); image.thumbnail((36,36), Image.ANTIALIAS); image.save(\"/home/ptarjan/www/tmp/metaward/antialias-thumb.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.thumbnail((36,36), Image.BILINEAR); image.save(\"/home/ptarjan/www/tmp/metaward/bilinear-thumb.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.thumbnail((36,36), Image.BICUBIC); image.save(\"/home/ptarjan/www/tmp/metaward/bicubic-thumb.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.thumbnail((36,36), Image.NEAREST); image.save(\"/home/ptarjan/www/tmp/metaward/nearest-thumb.png\")\n>>>\n>>> image = Image.open(StringIO.StringIO(data)); image.convert(\"RGB\").resize((36,36), Image.ANTIALIAS).save(\"/home/ptarjan/www/tmp/metaward/antialias-rgb.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.convert(\"RGB\").resize((36,36), Image.BILINEAR).save(\"/home/ptarjan/www/tmp/metaward/bilinear-rgb.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.convert(\"RGB\").resize((36,36), Image.BICUBIC).save(\"/home/ptarjan/www/tmp/metaward/bicubic-rgb.png\")\n>>> image = Image.open(StringIO.StringIO(data)); image.convert(\"RGB\").resize((36,36), Image.NEAREST).save(\"/home/ptarjan/www/tmp/metaward/nearest-rgb.png\")\n\n\nBut the results look much worse that just resizing in firefox.\n\nhttp://paulisageek.com/tmp/metaward/images.html\n\nHow can I get a similar effect to the firefox result using PIL (or another python image library)?\n\nEDIT : Hover your mouse to see what each image is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like the RGB and then ANTIALIS looks the best. Any other recommendations?\n\nFor reference, this is the one that looked the best :\n\n>>> image = Image.open(StringIO.StringIO(data)); \n>>> image.convert(\"RGB\").resize((36,36), Image.ANTIALIAS)\n\n", "completion": "I resized the \"original\" with Python and found the same results as you did. I also resized the \"original\" with GIMP and I got the same (if not inferior) quality. This made me suspect that Firefox cheats. Possibly it converts to RGB (\"original\" mode is indexed color). Thus the following code:\n\nimport Image\nim=Image.open(\"beta-icon.gif\")\nim = im.convert(\"RGB\")\nim=im.resize((36,36), Image.ANTIALIAS)\nim.save(\"q5.png\")\n\n\nThe result is almost as good as that of Firefox.\n"}
{"prompt": "I have a python class that looks like this:\n\nclass Process:\n    def __init__(self, PID, PPID, cmd, FDs, reachable, user):\n\n\nfollowed by:\n\n        self.PID=PID\n        self.PPID=PPID\n        self.cmd=cmd\n        ...\n\n\nIs there any way to autoinitialize these instance variables, like C++'s initialization list? It would spare lots of redundant code.\n", "completion": "Edit: extended the solution to honor default arguments also\n\nHere is the complete solution:\n\nfrom functools import wraps\nimport inspect\n\n\ndef initializer(func):\n    \"\"\"\n    Automatically assigns the parameters.\n\n    >>> class process:\n    ...     @initializer\n    ...     def __init__(self, cmd, reachable=False, user='root'):\n    ...         pass\n    >>> p = process('halt', True)\n    >>> p.cmd, p.reachable, p.user\n    ('halt', True, 'root')\n    \"\"\"\n    names, varargs, keywords, defaults = inspect.getargspec(func)\n\n    @wraps(func)\n    def wrapper(self, *args, **kargs):\n        for name, arg in list(zip(names[1:], args)) + list(kargs.items()):\n            setattr(self, name, arg)\n\n        for name, default in zip(reversed(names), reversed(defaults)):\n            if not hasattr(self, name):\n                setattr(self, name, default)\n\n        func(self, *args, **kargs)\n\n    return wrapper\n\n\n\n\nEdit: Adam asked me to extend the solution to support keyword arguments\n\nfrom functools import wraps\nimport inspect\n\ndef initializer(fun):\n   names, varargs, keywords, defaults = inspect.getargspec(fun)\n   @wraps(fun)\n   def wrapper(self, *args, **kargs):\n       for name, arg in zip(names[1:], args) + kargs.items():\n           setattr(self, name, arg)\n       fun(self, *args, **kargs)\n   return wrapper\n\n\n\n\nYou can use a decorator:\n\nfrom functools import wraps\nimport inspect\n\ndef initializer(fun):\n    names, varargs, keywords, defaults = inspect.getargspec(fun)\n    @wraps(fun)\n    def wrapper(self, *args):\n        for name, arg in zip(names[1:], args):\n            setattr(self, name, arg)\n        fun(self, *args)\n    return wrapper\n\nclass process:\n    @initializer\n    def __init__(self, PID, PPID, cmd, FDs, reachable, user):\n        pass\n\n\nOutput:\n\n>>> c = process(1, 2, 3, 4, 5, 6)\n>>> c.PID\n1\n>>> dir(c)\n['FDs', 'PID', 'PPID', '__doc__', '__init__', '__module__', 'cmd', 'reachable', 'user'\n\n"}
{"prompt": "I'm trying to download file with Python using IE:\n\nfrom win32com.client import DispatchWithEvents\n\nclass EventHandler(object):\n    def OnDownloadBegin(self):\n        pass\n\nie = DispatchWithEvents(\"InternetExplorer.Application\", EventHandler)\n\nie.Visible = 0\n\nie.Navigate('http://website/file.xml')\n\n\nAfter this, I'm getting a window asking the user where to save the file. How can I save this file automatically from python?\n\nI need to use some browser, not urllib or mechanize, because before downloading file I need to interact with some ajax functionality.\n", "completion": "This works for me as long as the IE dialogs are in the foreground and the downloaded file does not already exist in the \"Save As\" directory:\n\nimport time\nimport threading\nimport win32ui, win32gui, win32com, pythoncom, win32con\nfrom win32com.client import Dispatch\n\nclass IeThread(threading.Thread):\n    def run(self):\n        pythoncom.CoInitialize()\n        ie = Dispatch(\"InternetExplorer.Application\")\n        ie.Visible = 0\n        ie.Navigate('http://website/file.xml')\n\ndef PushButton(handle, label):\n    if win32gui.GetWindowText(handle) == label:\n        win32gui.SendMessage(handle, win32con.BM_CLICK, None, None)\n        return True\n\nIeThread().start()\ntime.sleep(3)  # wait until IE is started\nwnd = win32ui.GetForegroundWindow()\nif wnd.GetWindowText() == \"File Download - Security Warning\":\n    win32gui.EnumChildWindows(wnd.GetSafeHwnd(), PushButton, \"&Save\");\n    time.sleep(1)\n    wnd = win32ui.GetForegroundWindow()\nif wnd.GetWindowText() == \"Save As\":\n    win32gui.EnumChildWindows(wnd.GetSafeHwnd(), PushButton, \"&Save\");\n\n"}
{"prompt": "I've got a bit of Django form code that looks like this:\n\nclass GalleryAdminForm(forms.ModelForm):\n    auto_id=False\n    order = forms.CharField(widget=forms.HiddenInput())\n\n\nAnd that makes the form field go away, but it leaves the label \"Order\" in the Django admin page. If I use:\n\norder = forms.CharField(widget=forms.HiddenInput(), label='')\n\n\nI'm still left with the \":\" between where the field and label used to be.\n\nHow do I hide the whole thing?!\n", "completion": "I can't believe several people have suggested using jQuery for this...\n\nIs it a case of: when the only tool you know is a hammer everything looks like a nail?\n\nCome on, if you're going to do it from the client-side (instead of fixing the source of the problem in the back-end code) surely the right place to do it would be in CSS?\n\nIf you're in the admin site then it's a bit harder but if it's a regular page then it's easy to just omit the whole label from the form template, for example\n\nIf you're in the admin site then you could still override the as_table, as_ul, as_p methods of BaseForm (see django/forms/forms.py) in your GalleryAdminForm class to omit the label of any field where the label is blank (or == ':' as the value may be at this stage of rendering)\n\n(...looking at lines 160-170 of forms.py it seems like Django 1.2 should properly omit the ':' if the label is blank so I guess you're on an older version?)\n"}
{"prompt": "What difference it makes when I set python thread as a Deamon, using thread.setDaemon(True) ?\n", "completion": "A daemon thread will not prevent the application from exiting.  The program ends when all non-daemon threads (main thread included) are complete.\n\nSo generally, if you're doing something in the background, you might want to set the thread as daemon so you don't have to explicitly have that thread's function return before the app can exit.\n\nFor example, if you are writing a GUI application and the user closes the main window, the program should quit.  But if you have non-daemon threads hanging around, it won't.\n\nFrom the docs: http://docs.python.org/library/threading.html#threading.Thread.daemon\n\n\n  Its initial value is inherited from\n  the creating thread; the main thread\n  is not a daemon thread and therefore\n  all threads created in the main thread\n  default to daemon = False.\n  \n  The entire Python program exits when\n  no alive non-daemon threads are left.\n\n"}
{"prompt": "What is the easiest way to show a .jpg or .gif image from Python console?\n\nI've got a Python console program that is checking a data set which contains links to images stored locally. How should I write the script so that it would display images pop-up graphical windows?\n", "completion": "Using the awesome Pillow library:\n\n>>> from PIL import Image                                                                                \n>>> img = Image.open('test.png')\n>>> img.show() \n\n\nThis will open the image in your default image viewer.\n"}
{"prompt": "According to the documentation, if DEBUG is set to False and something is provided under the ADMINS setting, Django will send an email whenever the code raises a 500 status code. I have the email settings filled out properly (as I can use send_mail fine) but whenever I intentionally put up erroneous code I get my 500.html template but no error email is sent. What could cause Django to not do this?\n", "completion": "In my case the cause was missing SERVER_EMAIL setting.\n\nThe default for SERVER_EMAIL is root@localhost. But many of email servers including \nmy email provider do not accept emails from such suspicious addresses. They silently drop the emails.\n\nChanging the sender email address to django@my-domain.com solved the problem. In settings.py:\n\nSERVER_EMAIL = 'django@my-domain.com'\n\n"}
{"prompt": "I just installed it with apt-get on debian linux with\n\napt-get install libqt4-opengl\n\n\nthe rest of PyQt4 is available, but I cant get to this new module.\n\nfrom PyQt4 import QtOpenGL\n\n\nraises ImportError. any idea what to do? \n", "completion": "Did you forget to install the Python bindings?\n\napt-get install python-qt4-gl\n\n"}
{"prompt": "I'm starting work on some simulations using MPI and want to do the programming in Python/scipy. The scipy site lists a number of mpi libraries, but I was hoping to get feedback on quality, ease of use, etc from anyone who has used one.\n", "completion": "I have heard good things about mpi4py (but I have never used it myself). That's what a colleague recommended who looked at all the alternatives. He mentioned the completeness as one advantage.\n"}
{"prompt": "I'm reading the not so complete pytz documentation and I'm stuck on understand one part of it.\n\n\n  Converting between timezones also needs special attention. This also needs to use the normalize method to ensure the conversion is correct.\n\n\n>>> utc_dt = utc.localize(datetime.utcfromtimestamp(1143408899))\n>>> utc_dt.strftime(fmt)\n'2006-03-26 21:34:59 UTC+0000'\n>>> au_tz = timezone('Australia/Sydney')\n>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))\n>>> au_dt.strftime(fmt)\n'2006-03-27 08:34:59 EST+1100'\n>>> utc_dt2 = utc.normalize(au_dt.astimezone(utc))\n>>> utc_dt2.strftime(fmt)\n'2006-03-26 21:34:59 UTC+0000'\n\n\nI tried this very example without using normalize and it turned out just the same. In my opinion this example doesn't really explain why we have to use normalize when converting between datetime objects in different timezones.\n\nWould someone please give me an example (like the one above) where the result differs when not using normalize.\n\nThanks\n", "completion": "From the pytz documentation:\n\n\n  In addition, if you perform date arithmetic on local times that cross DST boundaries, the results may be in an incorrect timezone (ie. subtract 1 minute from 2002-10-27 1:00 EST and you get 2002-10-27 0:59 EST instead of the correct 2002-10-27 1:59 EDT). A normalize() method is provided to correct this. Unfortunately these issues cannot be resolved without modifying the Python datetime implementation.\n\n"}
{"prompt": "I know there's a similar topic about python console, but I do not know if they are the same. I tried system(\"clear\") and it didn't work here.\n\nHow do I clear python's IDLE window?\n", "completion": "The \"cls\" and \"clear\" are commands which will clear a terminal (ie a DOS prompt, or terminal window).  From your screenshot, you are using the shell within IDLE, which won't be affected by such things.  Unfortunately, I don't think there is a way to clear the screen in IDLE.  The best you could do is to scroll the screen down lots of lines, eg:\n\nprint \"\\n\" * 100\n\n\nThough you could put this in a function:\n\ndef cls(): print \"\\n\" * 100\n\n\nAnd then call it when needed as cls()\n"}
{"prompt": "I'm working on a library for Python that implements some persistent data structures (mainly as a learning exercise).  However, I'm beginning to learn that explaining persistent data structures to people unfamiliar with them can be difficult.  Can someone help me think of an easy (or at least the least complicated) way to describe persistent data structures to them?\n\nI've had a couple of people tell me that the documentation that I have is somewhat confusing.\n\n(And before anyone asks, no I don't mean persistent data structures as in persisted to the file system.  Google persistent data structures if you're unclear on this.)\n", "completion": "Try this:\n\nPersistent data structures are data structures which, unlike arrays or lists, always preserve their original structure and contents.\n\nThey are effectively immutable - instead of changing such a structure, any computation will yield a new copy leaving the original structure unchanged. Whereas conventional data structures follow an imperative (change-based) philosophy, persistent ones are commonly used in functional programming where, in extreme cases, no change is allowed at all. The advantage of this paradigm is that persistent data are much safer, often easier to handle and even preventing bugs connected to unintended change and multithreading issues. Many problems can be formulated functionally in a much more concise way.\n\nExample for a non-persistent data structure: The list (vector-list)\n\nlist = [1, 2, 3]\n# List is [1, 2, 3]\n\nlist += [4]\n# Added an element at the end\n\nlist[0] = 42\n# We changed the first element to 42\n# List is [42, 2, 3, 4]\n\n\nPersistence example: Linked lists (Haskell)\n\nlist = [1, 2, 3]\n# List is [1, 2, 3]\n\nlist2 = 0 : list\n# List2 combines [0] and [1, 2, 3] to [0, 1, 2, 3]\n# List is still [1, 2, 3]\n\nlist3 = updateAt 0 42 list\n# List3 contains the elements of list2 with the first one set to 42\n# List and List2 still hold their original values\n\n"}
{"prompt": "I have a fresh install (started with a wiped drive) of Snow Leopard with the developer tools installed during the Snow Leopard installation.\n\nI then installed Python 2.6.2, replacing the Snow Leopard default python 2.6.1. I've tried to install PIL by:\n\n\neasy_install\npip\ndownloading source and running python setup.py build manually.\n\n\nAll yield the same error (link to pip log: http://drop.io/gi2bgw6). I've seen others have had success installing PIL using the Snow Leopard default python 2.6.1, so I'm not sure why I'm having so much trouble getting it to work with 2.6.2.\n", "completion": "The problem I ran into was that PIL was being compiled against PowerPC architecture (-arch ppc).\n\nDo this before setup/build/compile:\n\nexport ARCHFLAGS=\"-arch i386\"\n\n\n(Assuming you're on i386)\n"}
{"prompt": "I'm writing a Python script that may or may not (depending on a bunch of things) run for a long time, and I'd like to make sure that multiple instances (started via cron) don't step on each others toes. The logical way to do this seems to be a PID-based lockfile\u00e2\u0080\u00a6 But I don't want to re-invent the wheel if there is already code to do this.\n\nSo, is there a Python module out there which will manage the details of a PID-based lockfile?\n", "completion": "This might be of help to you:  lockfile\n"}
{"prompt": "I'm trying to do this:\n\nmax_title_width = max([len(text) for text in columns])\n\nfor column in columns:\n    print \"%10s, blah\" % column\n\n\nBut I want to replace the 10 with the value of max_title_width.  How do I do this in the most pythonic way possible?\n", "completion": "This is a carryover from the C formatting markup:\n\nprint \"%*s, blah\" % (max_title_width,column)\n\n\nIf you want left-justified text (for entries shorter than max_title_width), put a '-' before the '*'.\n\n>>> text = \"abcdef\"\n>>> print \"<%*s>\" % (len(text)+2,text)\n<  abcdef>\n>>> print \"<%-*s>\" % (len(text)+2,text)\n<abcdef  >\n>>>\n\n\nIf the len field is shorter than the text string, the string just overflows:\n\n>>> print \"<%*s>\" % (len(text)-2,text)\n<abcdef>\n\n\nIf you want to clip at a maximum length, use the '.' precision field of the format placeholder:\n\n>>> print \"<%.*s>\" % (len(text)-2,text)\n<abcd>\n\n\nPut them all together this way:\n\n%\n- if left justified\n* or integer - min width (if '*', insert variable length in data tuple)\n.* or .integer - max width (if '*', insert variable length in data tuple)\n\n"}
{"prompt": "I am trying to install MySQLdb package. I found the source code here.\n\nI did the following:\n\ngunzip MySQL-python-1.2.3c1.tar.gz\ntar xvf MySQL-python-1.2.3c1.tar\ncd MySQL-python-1.2.3c1\npython setup.py build\n\n\nAs the result I got the following:\n\nTraceback (most recent call last):\n  File \"setup.py\", line 5, in ?\n    from setuptools import setup, Extension\nImportError: No module named setuptools\n\n\nDoes anybody knows how to solve this problem?\nBy the way, if I am able to do the described step, I will need to do the following:\n\nsudo python setup.py install\n\n\nAnd I have no system-administrator-rights. Do I still have a chance to install MySQLdb?\n\nThank you.\n", "completion": "After trying many suggestions, simply using sudo apt-get install python-mysqldb worked for me.\n\nMore info: \nGetting \"Error loading MySQLdb module: No module named MySQLdb\" - have tried previously posted solutions\n"}
{"prompt": "I'm new to Python and I'm trying to create a simple GUI using Tkinter. \n\nSo often in many user interfaces, hitting the tab button will change the focus from one Text widget to another. Whenever I'm in a Text widget, tab only indents the text cursor.\n\nDoes anyone know if this is configurable? If not, can PyQt or any other Python UI framework offer this functionality?\n", "completion": "This is very easy to do with Tkinter.\n\nThere are a couple of things that have to happen to make this work. First, you need to make sure that the standard behavior doesn't happen. That is, you don't want tab to both insert a tab and move focus to the next widget. By default events are processed by a specific widget prior to where the standard behavior occurs (typically in class bindings). Tk has a simple built-in mechanism to stop events from further processing. \n\nSecond, you need to make sure you send focus to the appropriate widget. There is built-in support for determining what the next widget is.\n\nFor example:\n\ndef focus_next_window(event):\n    event.widget.tk_focusNext().focus()\n    return(\"break\")\n\ntext_widget=Text(...)\ntext_widget.bind(\"<Tab>\", focus_next_window)\n\n\nImportant points about this code:\n\n\nThe method tk_focusNext() returns the next widget in the keyboard traversal hierarchy.\nthe method focus() sets the focus to that widget\nreturning \"break\" is critical in that it prevents the class binding from firing. It is this class binding that inserts the tab character, which you don't want.\n\n\nIf you want this behavior for all text widgets in an application you can use the bind_class() method instead of bind() to make this binding affect all text widgets. \n\nYou can also have the binding send focus to a very specific widget but I recommend sticking with the default traversal order, then make sure the traversal order is correct.\n"}
{"prompt": "class gpagelet:\n    \"\"\"\n    Holds   1) the pagelet xpath, which is a string\n            2) the list of pagelet shingles, list\n    \"\"\"\n    def __init__(self, parent):\n        if not isinstance( parent, gwebpage):\n            raise Exception(\"Parent must be an instance of gwebpage\")\n        self.parent = parent    # This must be a gwebpage instance\n        self.xpath = None       # String\n        self.visibleShingles = [] # list of tuples\n        self.invisibleShingles = [] # list of tuples\n        self.urls = [] # list of string\n\nclass gwebpage:\n    \"\"\"\n    Holds all the datastructure after the results have been parsed\n    holds:  1) lists of gpagelets\n            2) loc, string, location of the file that represents it\n    \"\"\"\n    def __init__(self, url):\n        self.url = url              # Str\n        self.netloc = False         # Str\n        self.gpagelets = []         # gpagelets instance\n        self.page_key = \"\"          # str\n\n\nIs there a way for me to make my class json serializable? The thing that I am worried is the recursive reference.\n", "completion": "Write your own encoder and decoder, which can be very simple like return __dict__\n\ne.g. here is a encoder to dump totally recursive tree structure, you can enhance it or use as it is for your own purpose\n\nimport json\n\nclass Tree(object):\n    def __init__(self, name, childTrees=None):\n        self.name = name\n        if childTrees is None:\n            childTrees = []\n        self.childTrees = childTrees\n\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if not isinstance(obj, Tree):\n            return super(MyEncoder, self).default(obj)\n\n        return obj.__dict__\n\nc1 = Tree(\"c1\")\nc2 = Tree(\"c2\") \nt = Tree(\"t\",[c1,c2])\n\nprint json.dumps(t, cls=MyEncoder)\n\n\nit prints \n\n{\"childTrees\": [{\"childTrees\": [], \"name\": \"c1\"}, {\"childTrees\": [], \"name\": \"c2\"}], \"name\": \"t\"}\n\n\nyou can similarly write a decoder but there you will somehow need to identify is it is your object or not, so may be you can put a type too if needed.\n"}
{"prompt": "I just read about zip bombs, i.e. zip files that contain very large amount of highly compressible data (00000000000000000...).\n\nWhen opened they fill the server's disk.\n\nHow can I detect a zip file is a zip bomb before unzipping it?\n\nUPDATE Can you tell me how is this done in Python or Java?\n", "completion": "Try this in Python:\n\nimport zipfile\nz = zipfile.ZipFile('c:/a_zip_file')\nprint 'total files size=', sum(e.file_size for e in z.infolist())\nz.close()\n\n"}
{"prompt": "I've followed the basic CherryPy tutorial (http://www.cherrypy.org/wiki/CherryPyTutorial).  One thing not discussed is deployment.\n\nHow can I launch a CherryPy app as a daemon and \"forget about it\"?  What happens if the server reboots?\n\nIs there a standard recipe?  Maybe something that will create a service script (/etc/init.d/cherrypy...)\n\nThanks!\n", "completion": "There is a Daemonizer plugin for CherryPy included by default which is useful for getting it to start but by far the easiest way for simple cases is to use the cherryd script:\n\n> cherryd -h\nUsage: cherryd [options]\n\nOptions:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config=CONFIG\n                        specify config file(s)\n  -d                    run the server as a daemon\n  -e ENVIRONMENT, --environment=ENVIRONMENT\n                        apply the given config environment\n  -f                    start a fastcgi server instead of the default HTTP\n                        server\n  -s                    start a scgi server instead of the default HTTP server\n  -i IMPORTS, --import=IMPORTS\n                        specify modules to import\n  -p PIDFILE, --pidfile=PIDFILE\n                        store the process id in the given file\n\n\nAs far as an init.d script goes I think there are examples that can be Googled.\n\nAnd the cherryd is found in your:\n\n\n  virtualenv/lib/python2.7/site-packages/cherrypy/cherryd\n\n\nor in: https://bitbucket.org/cherrypy/cherrypy/src/default/cherrypy/cherryd\n"}
{"prompt": "How do you do jinja2 aware syntax highlighting for vim?\n", "completion": "There appears to be a syntax highlighting file here.\n"}
{"prompt": "In the python built-in open function, what is the exact difference between the modes w, a, w+, a+, and r+?\n\nIn particular, the documentation implies that all of these will allow writing to the file, and says that it opens the files for \"appending\", \"writing\", and \"updating\" specifically, but does not define what these terms mean.\n", "completion": "The opening modes are exactly the same that C fopen() std library function.\n\nThe BSD fopen manpage defines them as follows:\n\n The argument mode points to a string beginning with one of the following\n sequences (Additional characters may follow these sequences.):\n\n ``r''   Open text file for reading.  The stream is positioned at the\n         beginning of the file.\n\n ``r+''  Open for reading and writing.  The stream is positioned at the\n         beginning of the file.\n\n ``w''   Truncate file to zero length or create text file for writing.\n         The stream is positioned at the beginning of the file.\n\n ``w+''  Open for reading and writing.  The file is created if it does not\n         exist, otherwise it is truncated.  The stream is positioned at\n         the beginning of the file.\n\n ``a''   Open for writing.  The file is created if it does not exist.  The\n         stream is positioned at the end of the file.  Subsequent writes\n         to the file will always end up at the then current end of file,\n         irrespective of any intervening fseek(3) or similar.\n\n ``a+''  Open for reading and writing.  The file is created if it does not\n         exist.  The stream is positioned at the end of the file.  Subse-\n         quent writes to the file will always end up at the then current\n         end of file, irrespective of any intervening fseek(3) or similar.\n\n"}
{"prompt": "I've started using Eclipe+PyDev as an environment for developing my first app for Google App Engine. Eclipse is configured according to this tutorial.\n\nEverything was working until I start to use memcache. PyDev reports the errors and I don't know how to fix it:\n\n\n\nError: Undefined variable from import: get\n\nHow to fix this?\nSure, it is only PyDev checker problem. Code is correct and run on GAE.\n\nUPDATE:\n\n\nI'm using PyDev 1.5.0 but experienced the same with 1.4.8.\nMy PYTHONPATH includes (set in Project Properties/PyDev - PYTHONPATH):\n\nC:\\Program Files\\Google\\google_appengine\nC:\\Program Files\\Google\\google_appengine\\lib\\django\nC:\\Program Files\\Google\\google_appengine\\lib\\webob\nC:\\Program Files\\Google\\google_appengine\\lib\\yaml\\lib\n\n\n\nUPDATE 2:\n\nI took a look at C:\\Program Files\\Google\\google_appengine\\google\\appengine\\api\\memcache\\__init__.py and found get() is not declared as memcache module function. They use the following trick to do that (I didn't hear about such possibility):\n\n_CLIENT = None\n\n\ndef setup_client(client_obj):\n  \"\"\"Sets the Client object instance to use for all module-level methods.\n\n  Use this method if you want to have customer persistent_id() or\n  persistent_load() functions associated with your client.\n\n  Args:\n    client_obj: Instance of the memcache.Client object.\n  \"\"\"\n  global _CLIENT\n  var_dict = globals()\n\n  _CLIENT = client_obj\n  var_dict['set_servers'] = _CLIENT.set_servers\n  var_dict['disconnect_all'] = _CLIENT.disconnect_all\n  var_dict['forget_dead_hosts'] = _CLIENT.forget_dead_hosts\n  var_dict['debuglog'] = _CLIENT.debuglog\n  var_dict['get'] = _CLIENT.get\n  var_dict['get_multi'] = _CLIENT.get_multi\n  var_dict['set'] = _CLIENT.set\n  var_dict['set_multi'] = _CLIENT.set_multi\n  var_dict['add'] = _CLIENT.add\n  var_dict['add_multi'] = _CLIENT.add_multi\n  var_dict['replace'] = _CLIENT.replace\n  var_dict['replace_multi'] = _CLIENT.replace_multi\n  var_dict['delete'] = _CLIENT.delete\n  var_dict['delete_multi'] = _CLIENT.delete_multi\n  var_dict['incr'] = _CLIENT.incr\n  var_dict['decr'] = _CLIENT.decr\n  var_dict['flush_all'] = _CLIENT.flush_all\n  var_dict['get_stats'] = _CLIENT.get_stats\n\n\nsetup_client(Client())\n\n\nHmm... Any idea how to force PyDev to recognize that?\n", "completion": "There is a cleaner solution: Try adding GAE's memcache to your forced builtins.\n\nIn your PyDev->Interpreter-Python->ForcedBuiltins window, add the \"google.appengine.api.memcache\" entry and apply.\n\nDouble-click on the memcache errors to check them back, they disappear!\n\nPlease make sure that system pythonpath includes google APE install directory.\n"}
{"prompt": "So, I've got an application that uses Twisted + Stomper as a STOMP client which farms out work to a multiprocessing.Pool of workers.\n\nThis appears to work ok when I just use a python script to fire this up, which (simplified) looks something like this:\n\n# stompclient.py\n\nlogging.config.fileConfig(config_path)\nlogger = logging.getLogger(__name__)\n\n# Add observer to make Twisted log via python\ntwisted.python.log.PythonLoggingObserver().start() \n\n# initialize the process pool.  (child processes get forked off immediately)\npool = multiprocessing.Pool(processes=processes)\n\nStompClientFactory.username = username\nStompClientFactory.password = password\nStompClientFactory.destination = destination\nreactor.connectTCP(host, port, StompClientFactory())\nreactor.run()\n\n\nAs this gets packaged for deployment, I thought I would take advantage of the twistd script and run this from a tac file.\n\nHere's my very-similar-looking tac file:\n\n# stompclient.tac\n\nlogging.config.fileConfig(config_path)\nlogger = logging.getLogger(__name__)\n\n# Add observer to make Twisted log via python\ntwisted.python.log.PythonLoggingObserver().start() \n\n# initialize the process pool.  (child processes get forked off immediately)\npool = multiprocessing.Pool(processes=processes)\n\nStompClientFactory.username = username\nStompClientFactory.password = password\nStompClientFactory.destination = destination\n\napplication = service.Application('myapp')\n\nservice = internet.TCPClient(host, port, StompClientFactory())\nservice.setServiceParent(application)\n\n\nFor the sake of illustration, I have collapsed or changed a few details; hopefully they were not the essence of the problem.  For example, my app has a plugin system, the pool is initialized by a separate method, and then work is delegated to the pool using pool.apply_async() passing one of my plugin's process() methods.\n\nSo, if I run the script (stompclient.py), everything works as expected.\n\nIt also appears to work OK if I run twist in non-daemon mode (-n):\n\ntwistd -noy stompclient.tac\n\n\nhowever, it does not work when I run in daemon mode:\n\ntwistd -oy stompclient.tac\n\n\nThe application appears to start up OK, but when it attempts to fork off work, it just hangs.  By \"hangs\", I mean that it appears that the child process is never asked to do anything and the parent (that called pool.apply_async()) just sits there waiting for the response to return.\n\nI'm sure that I'm doing something stupid with Twisted + multiprocessing, but I'm really hoping that someone can explain to my the flaw in my approach.\n\nThanks in advance!\n", "completion": "Since the difference between your working invocation and your non-working invocation is only the \"-n\" option, it seems most likely that the problem is caused by the daemonization process (which \"-n\" prevents from happening).\n\nOn POSIX, one of the steps involved in daemonization is forking and having the parent exit.  Among of things, this has the consequence of having your code run in a different process than the one in which the .tac file was evaluated.  This also re-arranges the child/parent relationship of processes which were started in the .tac file - as your pool of multiprocessing processes were.\n\nThe multiprocessing pool's processes start off with a parent of the twistd process you start.  However, when that process exits as part of daemonization, their parent becomes the system init process.  This may cause some problems, although probably not the hanging problem you described.  There are probably other similarly low-level implementation details which normally allow the multiprocessing module to work but which are disrupted by the daemonization process.\n\nFortunately, avoiding this strange interaction should be straightforward.  Twisted's service APIs allow you to run code after daemonization has completed.  If you use these APIs, then you can delay the initialization of the multiprocessing module's process pool until after daemonization and hopefully avoid the problem.  Here's an example of what that might look like:\n\nfrom twisted.application.service import Service\n\nclass MultiprocessingService(Service):\n    def startService(self):\n        self.pool = multiprocessing.Pool(processes=processes)\n\nMultiprocessingService().setServiceParent(application)\n\n\nNow, separately, you may also run into problems relating to clean up of the multiprocessing module's child processes, or possibly issues with processes created with Twisted's process creation API, reactor.spawnProcess.  This is because part of dealing with child processes correctly generally involves handling the SIGCHLD signal.  Twisted and multiprocessing aren't going to be cooperating in this regard, though, so one of them is going to get notified of all children exiting and the other will never be notified.  If you don't use Twisted's API for creating child processes at all, then this may be okay for you - but you might want to check to make sure any signal handler the multiprocessing module tries to install actually \"wins\" and doesn't get replaced by Twisted's own handler.\n"}
{"prompt": "I need to emulate \"tail -f\" in python, but I don't want to use time.sleep in the reading loop. I want something more elegant like some kind of blocking read, or select.select with timeout, but python 2.6 \"select\" documentation specifically says: \"it cannot be used on regular files to determine whether a file has grown since it was last read.\"\nAny other way?\nIn a few days if no solution is given I will read tail's C source code to try to figure it out. I hope they don't use sleep, hehe\nThanks.\n\nMarioR\n", "completion": "(update)\nEither use FS monitors tools \n\n\nFor linux\nFor Windows\nFor Mac\n\n\nOr a single sleep usage (which I would you consider as much more elegant).\n\nimport time\ndef follow(thefile):\n    thefile.seek(0,2)      # Go to the end of the file\n    while True:\n         line = thefile.readline()\n         if not line:\n             time.sleep(0.1)    # Sleep briefly\n             continue\n         yield line\n\nlogfile = open(\"access-log\")\nloglines = follow(logfile)\nfor line in loglines:\n    print line\n\n"}
{"prompt": "I'm creating a chat daemon in python and twisted framework.  And I'm wondering if I have to delete every variable create in my functions to save memory in the long run when multiple users are connected, or are those variable automatically clear?.  Here's a strip down version of my code to illustrate my point:\n\nclass Chat(LineOnlyReceiver):\n\n    LineOnlyReceiver.MAX_LENGTH = 500\n\n    def lineReceived(self, data):\n\n            self.sendMessage(data)\n\n    def sendMessage(self, data):\n\n            try:\n                message = data.split(None,1)[1]\n            except IndexError:\n                return\n\n            self.factory.sendAll(message)\n\n            #QUESTION : do i have to delete message and date??????????????????\n\n            del message\n            del data\n\n\nclass ChatFactory(Factory):\n    protocol = Chat\n\n    def __init__(self):\n        self.clients = []\n\n    def addClient(self, newclient):\n        self.clients.append(newclient)\n\n    def delClient(self, client):\n        self.clients.remove(client)\n\n    def sendAll(self, message):\n        for client in self.clients:\n            client.transport.write(message + \"\\n\")\n\n", "completion": "C Python (the reference implementation) uses reference counting and garbage collection. When count of references to object decrease to 0, it is automatically reclaimed. The garbage collection normally reclaims only those objects that refer to each other (or other objects from them) and thus cannot be reclaimed by reference counting. \n\nThus, in most cases, local variables are reclaimed at the end of the function, because at the exit from the function, the objects cease being referenced from anywhere. So your \"del\" statements are completely unnecessary, because Python does that anyway.\n"}
{"prompt": "I am working on embedding python in to c++. In some peculiar case I require two separate instances of the interpreter in same thread.\n\nCan I wrap Python interpreter in to a c++ class and get services from two or more class instances?\n", "completion": "I have used Py_NewInterpreter for different interpreters in different threads, but this should also work for several interpreters within one thread:\n\nIn the main thread:\n\nPy_Initialize();\nPyEval_InitThreads();\nmainThreadState = PyEval_SaveThread();\n\n\nFor each interpreter instance (in any thread):\n\n// initialize interpreter\nPyEval_AcquireLock();                // get the GIL\nmyThreadState = Py_NewInterpreter();\n... // call python code\nPyEval_ReleaseThread(myThreadState); // swap out thread state + release the GIL\n\n... // any other code\n\n// continue with interpreter\nPyEval_AcquireThread(myThreadState); // get GIL + swap in thread state\n... // call python code\nPyEval_ReleaseThread(myThreadState);\n\n... // any other code\n\n// finish with interpreter\nPyEval_AcquireThread(myThreadState);\n... // call python code\nPy_EndInterpreter(myThreadState);\nPyEval_ReleaseLock();                // release the GIL\n\n\nNote that you need a variable myThreadState for each interpreter instance!\n\nFinally the finish in the main thread:\n\nPyEval_RestoreThread(mainThreadState);\nPy_Finalize();\n\n\nThere are some restrictions with using several interpreter instances (they seem not to be totally independent), but in most cases this does not seem to cause problems.\n"}
{"prompt": "xrange function doesn't work for large integers:\n\n>>> N = 10**100\n>>> xrange(N)\nTraceback (most recent call last):\n...\nOverflowError: long int too large to convert to int\n>>> xrange(N, N+10)\nTraceback (most recent call last):\n...\nOverflowError: long int too large to convert to int\n\n\nPython 3.x:\n\n>>> N = 10**100\n>>> r = range(N)\n>>> r = range(N, N+10)\n>>> len(r)\n10\n\n\nIs there a backport of py3k builtin range() function for Python 2.x?\n\nEdit\n\nI'm looking for a complete implementation of \"lazy\" range(), not just a partial implementation of some of its functionality. \n", "completion": "I believe there is no backport (Py 3's completely removed the int/long distinction, after all, but in 2.* it's here to stay;-) but it's not hard to hack your own, e.g....:\n\nimport operator\n\ndef wowrange(start, stop, step=1):\n  if step == 0:\n    raise ValueError('step must be != 0')\n  elif step < 0:\n    proceed = operator.gt\n  else:\n    proceed = operator.lt\n  while proceed(start, stop):\n    yield start\n    start += step\n\n\nEdit it appears the OP doesn't just want looping (the normal purpose of xrange, and \nrange in Py3), but also len and the in operator (the latter does work on the above generator, but slowly -- optimizations are possible). For such richness a class \nis better...:\n\nimport operator\n\nclass wowrange(object):\n  def __init__(self, start, stop=None, step=1):\n    if step == 0: raise ValueError('step must be != 0')\n    if stop is None: start, stop = 0, start\n    if step < 0:\n      self.proceed = operator.gt\n      self.l = (stop-start+step+1)//step\n    else:\n      self.proceed = operator.lt\n      self.l = (stop-start+step-1)//step\n    self.lo = min(start, stop)\n    self.start, self.stop, self.step = start, stop, step\n  def __iter__(self):\n    start = self.start\n    while self.proceed(start, self.stop):\n      yield start\n      start += self.step\n  def __len__(self):\n    return self.l\n  def __contains__(self, x):\n    if x == self.stop:\n      return False\n    if self.proceed(x, self.start):\n      return False\n    if self.proceed(self.stop, x):\n      return False\n    return (x-self.lo) % self.step == 0\n\n\nI wouldn't be surprised if there's an off-by-one or similar glitch lurking here, but, I hope this helps!\n\nEdit again: I see indexing is ALSO required. Is it just too hard to write your own __getitem__?  I guess it is, so here it, too, is, served on a silver plate...:\n\n def __getitem__(self, i):\n   if i < 0:\n     i += self.l\n     if i < 0: raise IndexError\n   elif if i >= self.l:\n     raise IndexError\n   return self.start + i * self.step\n\n\nI don't know if 3.0 range supports slicing (xrange in recent 2.* releases doesn't -- it used to, but that was removed because the complication was ridiculous and prone to bugs), but I guess I do have to draw a line in the sand somewhere, so I'm not going to add it;-).\n"}
{"prompt": "I have been programming Python for a while and I have a very good understanding of its features, but I would like to improve my coding style. I think reading the source code of the Python Modules would be a good idea. Can anyone recommend any ones in particular?\n\nRelated Threads:\n\n\nBeginner looking for beautiful and instructional Python code: This thread actually inspired this question\n\n", "completion": "Queue.py shows you how to make a class thread-safe, and the proper use of the Template Method design pattern.\n\nsched.py is a great example of the Dependency Injection pattern.\n\nheapq.py is a really well-crafted implementation of the Heap data structure.\n\nIf I had to pick my three favorite modules in the Python standard library, this triplet would probably be my choice. (It doesn't hurt that they're all so very useful... but I'm picking in terms of quality of code, comments and design, first and foremost).\n"}
{"prompt": "Let's say you are building a Python-based web app that requires some workflow management such as that in jBPM or Windows Workflow Foundation. Is there a library that offers this in the Python world?\n", "completion": "Oh yes, tons. But most of them depend on a specific framework. DCWorkflow is integrated with Zopes CMF, for example. hurry.workflow is for Zope 3, etc. SpiffWorkflow presumes sql-alchemy, etc. This is because you need to have something to apply the workflow to, and that means you need to make some basic assumptions on the objects you use.\n\nHurry.workflow is probably one of the more independent ones, but it still assumes both that you use the Persistence library (and therefore in practice ZODB), and zope3's security model.\n\nSo you probably need to expand a bit on your requirements here...\n"}
{"prompt": "I have a very large collection of (p, q) tuples that I would like to convert into a dictionary of lists where the first item in each tuple is a key that indexes a list that contains q.\n\nExample:  \n\nOriginal List: (1, 2), (1, 3), (2, 3)  \nResultant Dictionary: {1:[2, 3], 2:[3]}\n\n\nFurthermore, I would like to efficiently combine these dictionaries.\n\nExample:  \n\nOriginal Dictionaries: {1:[2, 3], 2:[3]}, {1:[4], 3:[1]}  \nResultant Dictionary: {1:[2, 3, 4], 2:[3], 3:[1]}\n\n\nThese operations reside within an inner loop, so I would prefer that they be as fast as possible.\n\nThanks in advance\n", "completion": "If the list of tuples is sorted, itertools.groupby, as suggested by @gnibbler, is not a bad alternative to defaultdict, but it needs to be used differently than he suggested:\n\nimport itertools\nimport operator\n\ndef lot_to_dict(lot):\n  key = operator.itemgetter(0)\n  # if lot's not sorted, you also need...:\n  # lot = sorted(lot, key=key)\n  # NOT in-place lot.sort to avoid changing it!\n  grob = itertools.groupby(lot, key)\n  return dict((k, [v[1] for v in itr]) for k, itr in grob)\n\n\nFor \"merging\" dicts of lists into a new d.o.l...:\n\ndef merge_dols(dol1, dol2):\n  keys = set(dol1).union(dol2)\n  no = []\n  return dict((k, dol1.get(k, no) + dol2.get(k, no)) for k in keys)\n\n\nI'm giving [] a nickname no to avoid uselessly constructing a lot of empty lists, given that performance is important. If the sets of the dols' keys overlap only modestly, faster would be:\n\ndef merge_dols(dol1, dol2):\n  result = dict(dol1, **dol2)\n  result.update((k, dol1[k] + dol2[k])\n                for k in set(dol1).intersection(dol2))\n  return result\n\n\nsince this uses list-catenation only for overlapping keys -- so, if those are few, it will be faster.\n"}
{"prompt": "I am attempting to use BeautifulSoup to parse through a DOM tree and extract the names of authors. Below is a snippet of HTML to show the structure of the code I'm going to scrape. \n\n<html>\n<body>\n<div class=\"list-authors\">\n<span class=\"descriptor\">Authors:</span> \n<a href=\"/find/astro-ph/1/au:+Lin_D/0/1/0/all/0/1\">Dacheng Lin</a>, \n<a href=\"/find/astro-ph/1/au:+Remillard_R/0/1/0/all/0/1\">Ronald A. Remillard</a>, \n<a href=\"/find/astro-ph/1/au:+Homan_J/0/1/0/all/0/1\">Jeroen Homan</a> \n</div>\n<div class=\"list-authors\">\n<span class=\"descriptor\">Authors:</span> \n<a href=\"/find/astro-ph/1/au:+Kosovichev_A/0/1/0/all/0/1\">A.G. Kosovichev</a>\n</div>\n\n<!--There are many other div tags with this structure-->\n</body>\n</html>\n\n\nMy point of confusion is that when I do soup.find, it finds the first occurrence of the div tag that I'm searching for. After that, I search for all 'a' link tags. At this stage, how do I extract the authors names from each of the link tags and print them out? Is there a way to do it using BeautifulSoup or do I need to use Regex? How do I continue iterating over every other other div tag and extract the authors names?\n\nimport re\nimport urllib2,sys\nfrom BeautifulSoup import BeautifulSoup, NavigableString\nhtml = urllib2.urlopen(address).read()\n    soup = BeautifulSoup(html)\n\n    try:\n\n        authordiv = soup.find('div', attrs={'class': 'list-authors'})\n        links=tds.findAll('a')\n\n\n        for link in links:\n            print ''.join(link[0].contents)\n\n        #Iterate through entire page and print authors\n\n\n    except IOError: \n        print 'IO error'\n\n", "completion": "just use findAll for the divs link you do for the links\n\nfor authordiv in soup.findAll('div', attrs={'class': 'list-authors'}):\n"}
{"prompt": "To learn from good examples, what are the best open source Google App Engine applications out there?\n\nI don't care if it is Java or Python based.\n\nPlease one app per answer. Feel free to add a link to the live app (if there is) and to the project page.\n", "completion": "Rietveld of course\n"}
{"prompt": "I need to make an export like this in Python :\n\n# export MY_DATA=\"my_export\"\n\n\nI've tried to do :\n\n# -*- python-mode -*-\n# -*- coding: utf-8 -*-\nimport os\nos.system('export MY_DATA=\"my_export\"')\n\n\nBut when I list export, \"MY_DATA\" not appear :\n\n# export\n\n\nHow I can do an export with Python without saving \"my_export\" into a file ?\n", "completion": "export is a command that you give directly to the shell (e.g. bash), to tell it to add or modify one of its environment variables. You can't change your shell's environment from a child process (such as Python), it's just not possible.\n\nHere's what's happening with you try os.system('export MY_DATA=\"my_export\"')...\n\n/bin/bash process, command `python yourscript.py` forks python subprocess\n |_\n   /usr/bin/python process, command `os.system()` forks /bin/sh subprocess\n    |_\n      /bin/sh process, command `export ...` changes local environment\n\n\nWhen the bottom-most /bin/sh subprocess finishes running your export ... command, then it's discarded, along with the environment that you have just changed.\n"}
{"prompt": "I need an Object Pool, and rather than implement it myself, I thought I would look around for a ready-made and tested Python library.\n\nWhat I found was plenty of other people looking, but not getting many straight answers, so I have brought it over here to Stack Overflow.\n\nIn my case, I have a large number of threads (using the threading module), which need to occasionally call a remote SOAP-based server. They could each establish their own connection to the server, but setting up a socket and completing the authentication process is expensive (it is throttled by the server), so I want to share a pool of connections, creating more only as needed.\n\nIf the items to pool were worker subprocesses, I might have chosen multiprocessing.pool, but they are not. If they were worker threads, I might have chosen this implementation, but they are not.\n\nIf they were MySQL connections, I might have chosen pysqlpool, but they are not. Similarly the SQLAlchemy Pool is out.\n\nIf there was one thread, using a variable number of connections/objects, I would consider this implementation, but I need it to be thread-safe.\n\nI know I could implement this again fairly quickly, but given there are many people looking for it, I thought a canonical answer on Stack Overflow would be nice.\n", "completion": "It seems to me, from your description, that what you need is a pool of connections, not of objects. For simple thread-safety, just keep the reusable connections in a Queue.Queue instance, call it pool.  When a thread instantiates a connection-wrapping object, the object gets its connection via pool.get() (which automaticaly enqueues it to wait if there are no connections currently availabe and dequeues it when a connection's ready for it); when the object's done using its connection, it puts it back in the pool via pool.put.\n\nThere's so little universally-required, general-purpose functionality in this, beyond what Queue.Queue already gives you, that it's not surprising no module providing it is well known or popular -- hard to make a module widespread when it has about 6 lines of functional code in all (e.g. to call a user-supplied connection factory to populate the queue either in advance or just-in-time up to some maximum number -- not a big added value generally, anyway).  \"Thick glue\", thickly wrapping the underlying functionality from a standard library module without substantial added value, is an architectural minus, after all;-).\n"}
{"prompt": "I'm making a 2D list and I would like to initialize it with a list comprehension.  I would like it to do something like this:\n\n[[x for i in range(3) if j <= 1: x=1 else x=2] for j in range(3)]\n\n\nso it should return something like:\n\n[[1,1,1],\n [1,1,1],\n [2,2,2]]\n\n\nHow might I go about doing this?\n\nThanks for your help.\n", "completion": "It appears as though you're looking for something like this:\n\n[[1 if j <= 1 else 2 for i in range(3)] for j in range(3)]\n\n\nThe Python conditional expression is a bit different from what you might be used to if you're coming from something like C or Java:\n\n\n  The expression x if C else y first evaluates C (not x); if C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned.\n\n\nA slightly shorter way to do the same thing is:\n\n[[1 if j <= 1 else 2]*3 for j in range(3)]\n\n"}
{"prompt": "In PHP, I can do this:\n\necho '<pre>'\nprint_r($array);\necho '</pre>'\n\n\nIn Python, I currently just do this:\n\nprint the_list\n\n\nHowever, this will cause a big jumbo of data. Is there any way to print it nicely into a readable tree? (with indents)?\n", "completion": "from pprint import pprint\npprint(the_list)\n\n"}
{"prompt": "Anyone know of a good feed parser for python 3.1?\nI was using feedparser for 2.5 but it doesn't seem to be ported to 3.1 yet, and it's apparently more complicated than just running 2to3.py on it.\nAny help?\n", "completion": "You may take a look at the Atom Models blog post by Ian Bicking. He proposes not to use any special \"feed parsing\" library because Atom and RSS are just XML so your model is really an XML tree, not some fancy class. You could try his code under Python 3.\n"}
{"prompt": "I have a float: 1.2333333\n\nHow do I change it into a two decimal number with a comma as a decimal point separator, eg 1,23?\n", "completion": "The locale module can help you with reading and writing numbers in the locale's format.\n\n>>> import locale\n>>> locale.setlocale(locale.LC_ALL, \"\")\n'sv_SE.UTF-8'\n>>> locale.format(\"%f\", 2.2)\n'2,200000'\n>>> locale.format(\"%g\", 2.2)\n'2,2'\n>>> locale.atof(\"3,1415926\")\n3.1415926000000001\n\n"}
{"prompt": "I have a series of large text files (up to 1 gig) that are output from an experiment that need to be analysed in Python. They would be best loaded into a 2D numpy array, which presents the first question:\n\n\nAs the number of rows is unknown at the beginning of the loading, how can\na very large numpy array be most efficiently built, row by row?\n\n\nSimply adding the row to the array would be inefficient in memory terms, as two large arrays would momentarily co-exist. The same problem would seem to be occur if you use numpy.append. The stack functions are promising, but ideally I would want to grow the array in place.\n\nThis leads to the second question:\n\n\nWhat is the best way to observe the memory usage of a Python program that heavily\nuses numpy arrays?\n\n\nTo study the above problem, I've used the usual memory profiling tools - heapy and pympler - but am only getting the size of the outer array objects (80 bytes) and not the data they are containing. Asides from a crude measuring of how much memory the Python process is using, how can I get at the \"full\" size of the arrays as they grow?\n\nLocal details: OSX 10.6, Python 2.6, but general solutions are welcome.\n", "completion": "There's no way to ensure you can grow the array in place other than creating an empty array (numpy.empty) of the maximum possible size and then using a view of that at the end.  You can't start small because there's no guarantee that you can expand whatever memory the map is without clobbering some other data.  (And all of this is much lower level than python allows you to get from inside the interpreter.)\n\nYour best bet is probably numpy.fromiter.  Looking at the source, as the number of items increase, the array is expanded by a little over 50% each time.  If you can easily get the number of rows (say from counting the lines), you can even pass it a count.\n"}
{"prompt": "Most likely it's a dumb question for those who knows the answer, but I'm a beginner, and here it goes:\n\nI have a Python script which I run in a command-line with some parameter, and it prints me some results. Let's say results are some HTML code.\n\nI never done any Python programming for web, and couldn't figure it out... I need to have a page (OK, I know how to upload files to a server, Apache is running, Python is installed on the server...) with an edit field, which will accept that parameter, and Submit button, and I need it to \"print\" the results on a web page after the user submitted a proper parameter, or show any output that in a command-line situation are printed.\n\nI've read Dive Into Python's chapters about \"HTML Processing\" and \"HTTP Web Services\", but they do not describe what I'm looking for.\n\nIf the answer isn't short, I would very much appreciate links to the more relevant stuff to read or maybe the key words to google for it.\n", "completion": "For such a simple task, you probably don't need more than CGI. Luckily Python has a built-in cgi module which should do what you want.\n\nOr you could look into some of the minimal web frameworks, such as web.py.\n"}
{"prompt": "I have a Debian system currently running with python 2.5.4. I got virtualenv properly installed, everything is working fine. Is there a possibility that I can use a virtualenv with a different version of Python?\n\nI compiled Python 2.6.2 and would like to use it with some virtualenv. Is it enough to overwrite the binary file? Or do I have to change something in respect to the libraries?\n", "completion": "Just use the -p flag when creating your virtualenv instance to specify the Python executable you want to use, e.g.:\n\nvirtualenv -p /usr/bin/python2.6 <path/to/new/virtualenv/>\n\n"}
{"prompt": "Looking at comprehensions in Python and Javascript, so far I can't see some of the main features that I consider most powerful in comprehensions in languages like Haskell.  \n\nDo they allow things like multiple generators?  Or are they just a basic map-filter form?\n\nIf they don't allow multiple generators, I find them quite disappointing - why have such things been left out?\n", "completion": "Python allows multiple generators:\n\n>>> [(x,y,x*y) for x in range(1,5) for y in range(1,5)]\n[(1, 1, 1), (1, 2, 2), (1, 3, 3), (1, 4, 4), \n (2, 1, 2), (2, 2, 4), (2, 3, 6), (2, 4, 8), \n (3, 1, 3), (3, 2, 6), (3, 3, 9), (3, 4, 12),\n (4, 1, 4), (4, 2, 8), (4, 3, 12), (4, 4, 16)]\n\n\nAnd also restrictions:\n\n>>> [(x,y,x*y) for x in range(1,5) for y in range(1,5) if x*y > 8]\n[(3, 3, 9), (3, 4, 12), (4, 3, 12), (4, 4, 16)]\n\n\nUpdate: Javascript's syntax is similar (results from using the javascript shell on firefox):\n\nvar nums = [1, 2, 3, 21, 22, 30];\nvar s = eval('[[i,j] for each (i in nums) for each (j in [3,4]) if (i%2 == 0)]');\ns.toSource();\n[[2, 3], [2, 4], [22, 3], [22, 4], [30, 3], [30, 4]]\n\n\n(For some reason, something about the context stuff is evaluated in in the javascript shell requires the eval indirection to have list comprehensions work.  Javascript inside a <script> tag doesn't require that, of course)\n"}
{"prompt": "A python script is controlling an external application on Linux, passing in input via a pipe to the external applications stdin, and reading output via a pipe from the external applications stdout. \n\nThe problem is that writes to pipes are buffered by block, and not by line, and therefore delays occur before the controlling script receives data output by, for example, printf in the external application.\n\nThe external application cannot be altered to add explicit fflush(0) calls.\n\nHow can the pty module of the python standard library be used with the subprocess module to achieve this?\n", "completion": "Doing this is possible, but the only solution I can think of is fairly convoluted, non-portable, and probably fraught with problematic details.  You can use LD_PRELOAD to cause the external application to load a dynamic library which contains a constructor that invokes setvbuf to unbuffer stdout.  You will probably also want to wrap setvbuf in the library to prevent the application from explicitly buffering its own stdout.  And you'll want to wrap fwrite and printf so that they flush on each call.  Writing the .so to be preloaded will take you outside of python.  \n"}
{"prompt": "I'm writting a small python script notify me when certain condition met. I used smtplib which does the emailing for me, but I also want the script to call my cell phone as well.\n\nI can't find a free library for phone callings. Does anyone know any?\n", "completion": "Make the calls using Skype, and use the Skype4Py API.\n\nIf you want other suggestions, please specify how you want to make the call (modem? Some software bridge? What?).\n\nAlso, might I suggest that you send an SMS instead of placing a call? You can do that via Skype too, btw.\n"}
{"prompt": "I'm not sure if I need a lambda, or something else. But still, I need the following:\n\nI have an array = [1,2,3,4,5]. I need to put this array, for instance, into another array. But write it all in one line.\n\nfor item in array:\n    array2.append(item)\n\n\nI know that this is completely possible to iterate through the items and make it one-line. But googling and reading manuals didn't help me that much... if you can just give me a hint or name this thing so that I could find what that is, I would really appreciate it.\n\nUpdate: let's say this: array2 = SOME FANCY EXPRESSION THAT IS GOING TO GET ALL THE DATA FROM THE FIRST ONE\n\n(the example is NOT real. I'm just trying to iterate through different chunks of data, but that's the best I could come up with) \n", "completion": "The keyword you're looking for is list comprehensions:\n\n>>> x = [1, 2, 3, 4, 5]\n>>> y = [2*a for a in x if a % 2 == 1]\n>>> print(y)\n[2, 6, 10]\n\n"}
{"prompt": "Summary: Building Python 3.1 on RHEL 5.3 64 bit with --enable-shared fails to compile all extensions.  Building \"normal\" works fine without any problems.\n\nPlease note that this question may seem to blur the line between programming and system administration.  However, I believe that because it has to deal directly with getting language support in place, and it very much has to do with supporting the process of programming, that I would cross-post it here.  Also at: http://serverfault.com/questions/73196/python-3-1-1-with-enable-shared-will-not-build-any-extensions.  Thank you!\n\nProblem:\n\nBuilding Python 3.1 on RHEL 5.3 64 bit with --enable-shared fails to compile all extensions.  Building \"normal\" works fine without any problems.\n\nI can build python 3.1 just fine, but when built as a shared library, it emits many warnings (see below), and refuses to build any of the c based modules.  Despite this failure, I can still build mod_wsgi 3.0c5 against it, and run it under apache.  Needless to say, the functionality of Python is greatly reduced...\n\nInteresting to note that Python 3.2a0 (from svn) compiles fine with --enable-shared, and mod_wsgi compiles fine against it.  But when starting apache, I get:\n\nCannot load /etc/httpd/modules/mod_wsgi.so into server: /etc/httpd/modules/mod_wsgi.so: undefined symbol: PyCObject_FromVoidPtr\n\nThe project that this is for is a long-term project, so I'm okay with alpha quality software if needed.  Here are some more details on the problem.\n\nHost:\n\n\nDell PowerEdge\nIntel Xenon \nRHEL 5.3 64bit\nNothing \"special\"\n\n\nBuild:\n\n\nPython 3.1.1 source distribution\nWorks fine with ./configure\nDoes not work fine with ./configure --enable-shared\n\n\n(export CFLAGS=\"-fPIC\" has been done)\n\nmake output\n\n\n\ngcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes  -I. -IInclude -I./Include  -fPIC -DPy_BUILD_CORE  -c ./Modules/_weakref.c -o Modules/_weakref.o\n\n\n\nbuilding 'bz2' extension\ngcc -pthread -fPIC -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I. -I./Include -I/usr/local/include -IInclude -I/home/build/RPMBUILD/BUILD/Python-3.1.1 -c /home/build/RPMBUILD/BUILD/Python-3.1.1/Modules/bz2module.c -o build/temp.linux-x86_64-3.1/home/build/RPMBUILD/BUILD/Python-3.1.1/Modules/bz2module.o\ngcc -pthread -shared -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes build/temp.linux-x86_64-3.1/home/build/RPMBUILD/BUILD/Python-3.1.1/Modules/bz2module.o -L/usr/local/lib -L. -lbz2 -lpython3.1 -o build/lib.linux-x86_64-3.1/bz2.so\n/usr/bin/ld: /usr/local/lib/libpython3.1.a(abstract.o): relocation R_X86_64_32 against 'a local symbol' can not be used when making a shared object; recompile with -fPIC\n\n\n\nFailed to build these modules:\n_bisect            _codecs_cn         _codecs_hk\n_codecs_iso2022    _codecs_jp         _codecs_kr\n_codecs_tw         _collections       _csv\n_ctypes            _ctypes_test       _curses\n_curses_panel      _dbm               _elementtree\n_gdbm              _hashlib           _heapq\n_json              _lsprof            _multibytecodec\n_multiprocessing   _pickle            _random\n_socket            _sqlite3           _ssl\n_struct            _testcapi          array\natexit             audioop            binascii\nbz2                cmath              crypt\ndatetime           fcntl              grp\nitertools          math               mmap\nnis                operator           ossaudiodev\nparser             pyexpat            readline\nresource           select             spwd\nsyslog             termios            time\nunicodedata        zlib\n\n", "completion": "Something is wrong with your build environment. It is picking up a libpython3.1.a from /usr/local/lib; this confuses the error messages. It tries linking with that library, which fails - however, it shouldn't have tried that in the first place, since it should have used the libpython that it just built. I recommend taking the Python 3.1 installation in /usr/local out of the way.\n\nYou don't show in your output whether a libpython3.1.so.1.0 was created in the build tree; it would be important to find out whether it exists, how it was linked, and what symbols it has exported.\n"}
{"prompt": "Sometimes it is useful to \"clone\" a row or column vector to a matrix. By cloning I mean converting a row vector such as\n\n[1,2,3]\n\n\nInto a matrix\n\n[[1,2,3]\n [1,2,3]\n [1,2,3]\n]\n\n\nor a column vector such as\n\n[1\n 2\n 3\n]\n\n\ninto\n\n[[1,1,1]\n [2,2,2]\n [3,3,3]\n]\n\n\nIn matlab or octave this is done pretty easily:\n\n x = [1,2,3]\n a = ones(3,1) * x\n a =\n\n    1   2   3\n    1   2   3\n    1   2   3\n\n b = (x') * ones(1,3)\n b =\n\n    1   1   1\n    2   2   2\n    3   3   3\n\n\nI want to repeat this in numpy, but unsuccessfully\n\nIn [14]: x = array([1,2,3])\nIn [14]: ones((3,1)) * x\nOut[14]:\narray([[ 1.,  2.,  3.],\n       [ 1.,  2.,  3.],\n       [ 1.,  2.,  3.]])\n# so far so good\nIn [16]: x.transpose() * ones((1,3))\nOut[16]: array([[ 1.,  2.,  3.]])\n# DAMN\n# I end up with \nIn [17]: (ones((3,1)) * x).transpose()\nOut[17]:\narray([[ 1.,  1.,  1.],\n       [ 2.,  2.,  2.],\n       [ 3.,  3.,  3.]])\n\n\nWhy wasn't the first method (In[16]) working? Is there a way to achieve this task in python in a more elegant way?\n", "completion": "Use numpy.tile:\n\n>>> tile(array([1,2,3]), (3, 1))\narray([[1, 2, 3],\n       [1, 2, 3],\n       [1, 2, 3]])\n\n\nor for repeating columns:\n\n>>> tile(array([[1,2,3]]).transpose(), (1, 3))\narray([[1, 1, 1],\n       [2, 2, 2],\n       [3, 3, 3]])\n\n"}
{"prompt": "From what I remember from my C++ class, the professor said that operator overloading is cool, but since it takes relatively a lot of thought and code to cover all end-cases (e.g. when overloading + you probably also want to overload ++ and +=, and also make sure to handle end cases like adding an object to itself etc.), you should only consider it in those cases where this feature will have a major impact on your code, like overloading the operators for the matrix class in a math application.\n\nDoes the same apply to python? Would you recommend overriding operator behavior in python? And what rules of thumb can you give me?\n", "completion": "Operator overloading is mostly useful when you're making a new class that falls into an existing \"Abstract Base Class\" (ABC) -- indeed, many of the ABCs in standard library module collections rely on the presence of certain special methods (and special methods, one with names starting and ending with double underscores AKA \"dunders\", are exactly the way you perform operator overloading in Python). This provides good starting guidance.\n\nFor example, a Container class must override special method __contains__, i.e., the membership check operator item in container (as in, if item in container: -- don't confuse with the for statement, for item in container:, which relies on __iter__!-).\nSimilarly, a Hashable must override __hash__, a Sized must override __len__, a Sequence or a Mapping must override __getitem__, and so forth.  (Moreover, the ABCs can provide your class with mixin functionality -- e.g., both Sequence and Mapping can provide __contains__ on the basis of your supplied __getitem__ override, and thereby automatically make your class a Container).\n\nBeyond the collections, you'll want to override special methods (i.e. provide for operator overloading) mostly if your new class \"is a number\". Other special cases exist, but resist the temptation of overloading operators \"just for coolness\", with no semantic connection to the \"normal\" meanings, as C++'s streams do for << and >> and Python strings (in Python 2.*, fortunately not in 3.* any more;-) do for % -- when such operators do not any more mean \"bit-shifting\" or \"division remainder\", you're just engendering confusion.  A language's standard library can get away with it (though it shouldn't;-), but unless your library gets as widespread as the language's standard one, the confusion will hurt!-)\n"}
{"prompt": "Python 2.6.2 (r262:71605, Apr 14 2009, 22:40:02) [MSC v.1500 32 bit (Intel)] on\nwin32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sqlite3\n>>> sqlite3.version\n'2.4.1'\n\n\nQuestions:\n\n\nWhy is the version of the sqlite**3** module '2.4.1'\nWhats the reason behind bundling such an old sqlite with Python? The sqlite releaselog says 2002 Mar 13 (2.4.1).\n\n", "completion": "Python 2.5.1\n>>> import sqlite3\n>>> sqlite3.version\n'2.3.2'\n>>> sqlite3.sqlite_version\n'3.3.4'\n\n\nversion - pysqlite version\nsqlite_version - sqlite version\n"}
{"prompt": "I just documented loads of my code and learnt how to use sphinx to generate the documentation. I want to include that into my github project page but I do not know how to. Does anyone know existing tutorial or simple step to do so?\n\nThanks.\n", "completion": "github will serve static content for you using their github pages feature.  Essentially, you create a branch called gh-pages, into which you commit your static pages. The pages are then served at you.github.com/yourproject.\n\nSee the instructions at http://pages.github.com/.\n\nYou will likely run into an issue using Sphinx on github, because Sphinx uses directories with leading underscores.  You can fix this by adding a file called .nojekyll in the the directory with the generated sphinx html.\n"}
{"prompt": "How can I save a model, such that signals arent sent. (post_save and pre_save)\n", "completion": "It's a bit of a hack, but you can do something like this:\n\nuse a unique identifier with a filter and then use the update method of the queryset (which does not trigger the signals)\n\nuser_id = 142187\nUser.objects.filter(id=user_id).update(name='tom')\n\n"}
{"prompt": "Is is possible to do something similar to this with a list, dictionary or something else even?\n\ndata_dict = {\n    'title' : 'awesome title',\n    'body' : 'great body of text',\n}\nModel.objects.create(data_dict)\n\n\nEven better if I can extend it\n\nModel.objects.create(data_dict, extra='hello', extra2='world)\n\n", "completion": "If title and body are fields in your model, then you can deliver the keyword arguments in your dictionary using the ** operator.\n\nAssuming your model is called MyModel:\n\n# create instance of model\nm = MyModel(**data_dict)\n# don't forget to save to database!\nm.save()\n\n\nAs for your second question, the dictionary has to be the final argument. Again, extra and extra2 should be fields in the model.\n\nm2 =MyModel(extra='hello', extra2='world', **data_dict)\nm2.save()\n\n"}
{"prompt": "I have an array in python that contains a set of values, some of them are\n\n2.32313e+07\n\n2.1155e+07\n\n1.923e+07\n\n11856\n\n112.32\n\nHow do I convert the exponential formats to the decimal format\n\nAdditional: Is there a way I can convert the exponent directly to decimal when printing out in UNIX with awk?\n", "completion": "I imagine you have a list rather than an array, but here it doesn't make much of a difference; in 2.6 and earlier versions of Python, something like:\n\n>>> L = [2.32313e+07, 2.1155e+07, 1.923e+07, 11856, 112.32]\n>>> for x in L: print '%f' % x\n... \n23231300.000000\n21155000.000000\n19230000.000000\n11856.000000\n112.320000\n\n\nand in 2.6 or later, the .format method.  I imagine you are aware that the numbers per se, as numbers, aren't in any \"format\" -- it's the strings you obtain by formatting the numbers, e.g. for output, that are in some format.  BTW, variants on that %f can let you control number of decimals, width, alignment, etc -- hard to suggest exactly what you may want without further specs from you.\n\nIn awk, you can use printf.\n"}
{"prompt": "Sphinx is a Python library to generate nice documentation from a set of ReST formatted text files.\n\nI wonder if any one has written Sphinx plugins to make it generate personal websites and blogs.\n\nEspecially for blogs, there needs to be a way to automatically list posts chronologically and generate a RSS feed. One needs to write a Sphinx plugin to do such special page/xml generation.\n\nHas anyone tried this before?\n", "completion": "I've done it at http://reinout.vanrees.org/weblog. The key trick is to add a preprocessor step.  I've got my blog entries in a weblog/yyyy/mm/dd/ folder structure.\n\nA script iterates through that folder structure, creating index.txt files in every directory, listing the sub-items.  The normal Sphinx process then renders those index.txt files.\n\nI added a custom Sphinx processor for tags.  So \".. tags:: python, buildout\" somewhere at the top of my weblog entry generates the tags.  And the preprocessor again collects those entries and writes out a weblog/tags/TAGNAME.txt file which Sphinx again renders normally.\n\nThe preprocessor also creates the root weblog/index.txt with the latest 10 entries.  And an weblog/atom.xml in (hardcoded) the output directory for the rss feed.\n\nSo: you need some custom stuff, but it is pretty much plain text, so for me it was a nice exercise.  And you get to write some helper scripts to make life easy, for instance one that copies a textfile from somewhere to today's weblog directory (including creation of missing directories and an \"svn add\").\n"}
{"prompt": "I have the following data structure:\n\n  data = [\n      {'site': 'Stackoverflow', 'id': 1},\n      {'site': 'Superuser', 'id': 2}, \n      {'site': 'Serverfault', 'id': 3}\n  ]\n\n\nI want to search the above list to see if it has any site with a specific value. For instance, search the above to see if the list contain a dictionary with site = 'Superuser' and return True/False.  I can do the above the usual way of looping over each item and comparing them.  Is there an alternative way to achieve a search?\n", "completion": "any(d['site'] == 'Superuser' for d in data)\n\n"}
{"prompt": "I need to update a text file whenever my IP address changes, and then run a few commands from the shell afterwards.\n\n\nCreate variable LASTKNOWN = \"212.171.135.53\" \nThis is the ip address we have while writing this script.\nGet the current IP address. It will change on a daily basis.\nCreate variable CURRENT for the new IP.\nCompare (as strings) CURRENT to LASTKNOWN\nIf they are the same, exit()\nIf they differ, \n\nA. \"Copy\" the old config file (/etc/ipf.conf) containing LASTKNOWN IP address into /tmp\nB. Replace LASTKNOWN with CURRENT in the /tmp/ipf.conf file.\nC. Using subprocess \"mv /tmp/ipf.conf /etc/ipf.conf\"\nD. Using subprocess execute, \"ipf -Fa -f /etc/ipf.conf\"\nE. Using subprocess execute, \"ipnat -CF -f /etc/ipnat.conf\"  \nexit()\n\n\nI know how to do steps 1 through 6. I fall down on the \"file editing\" part, A -> C. I can't tell what module to use or whether I should be editing the file in place. There are so many ways to do this, I can't decide on the best approach. I guess I want the most conservative one.\n\nI know how to use subprocess, so you don't need to comment on that.  \n\nI don't want to replace entire lines; just a specific dotted quad.   \n\nThanks!\n", "completion": "Another way to simply edit files in place is to use the fileinput module:\n\nimport fileinput, sys\nfor line in fileinput.input([\"test.txt\"], inplace=True):\n    line = line.replace(\"car\", \"truck\")\n    # sys.stdout is redirected to the file\n    sys.stdout.write(line)\n\n"}
{"prompt": "As I work on my first django powered site, I am constantly learning new things and making all sorts of changes and additions to my apps as I go.  I try to follow DRY and pythonic principles and be smart in my coding but eventually I will have to take the site live and am certain that not long after I do, something new and exiting will come down the pipe and I will want to implement it.  \n\nPreparing for the future:\n\nWith this in mind, do folks have any suggestions about how I can prepare my code now to be as future-ready as possible for these currently unforseen/unknown upgrades/additions to my code base?  \n\nHindsight is 20/20:\n\nWhat do you wish you had done at the start that would have made your life easier now that your site is up and running ?  \n\nLittle Things I've Learned (examples):\n\n\nuse UTC as the default timezone (and use datetime.datetime.utcnow())\nuse South to aid future database changes (haven't done it yet, but it seems wise)\nnot hard code links in my templates (use get_absolute_url() and reverse lookups)\ncreate a separate tools app to contain small re-usable templatetags and utility functions that I may want to use in future projects (no need to decouple them later)\n\n\nThese are small tips, and some straight from the django-docs, but I think they help .  \n\nHow about you?  What are your best practices for a new app or project that prepare you for the future?\n", "completion": "\nDeploy into a pure environment using virtualenv.\nDocument requirements using a pip requirements file.\n\n\nI'm sure others will suggest their deployment strategies, but making these changes were big positives for me.\n"}
{"prompt": "I'm starting to experiment with CouchDB because it looks like the perfect solution for certain problems we have.  Given that all work will be on a brand new project with no legacy dependencies, which client library would you suggest that I use, and why?\n\nThis would be easier if there was any overlap on the OSes we use.  FreeBSD only has py-simplecouchdb already available in its ports collection, but that library's project website says to use CouchDBKit instead.  Neither of those come with Ubuntu, which only ships with CouchDB.  Since those two OSes don't have an libraries in common, I'll probably be installing something from source (and hopefully submitting packages to the Ubuntu and FreeBSD folks if I have time).\n\nFor those interested, I'd like to use CouchDB as a convenient intermediate storage place for data passed between various services - think of a message bus system but with less formality.  For example, we have daemons that download and parse web pages, then send interesting bits to other daemons for further processing.  A lot of those objects are ill-defined until runtime (\"here's some HTML, plus a set of metadata, and some actions to run on it\").  Rather than serialize it to an ad-hoc local network protocol or stick it in PostgreSQL, I'd much rather use something designed for the purpose.  We're currently using NetWorkSpaces in this role, but it doesn't have nearly the breadth of support or the user community of CouchDB.\n", "completion": "I have been using couchdb-python with quite a lot of success and as far as I know the guys of desktopcouch use it in ubuntu. The prerequisites are very basic and you should have not problems:\n\n\nhttplib2\nsimplejson or cjson\nPython\nCouchDB 0.9.x (earlier or later versions are unlikely to work as the interface is still changing) \n\n\nFor me some of the advantages are:\n\n\nPythonic interface. You can work with the database like if it was a dict.\nInterface for design documents.\na CouchDB view server that allows writing view functions in Python \n\n\nIt also provides a couple of command-line tools: \n\n\ncouchdb-dump: Writes a snapshot of a CouchDB database\ncouchdb-load: Reads a MIME multipart file as generated by couchdb-dump and loads all the documents, attachments, and design documents into a CouchDB database. \ncouchdb-replicate: Can be used as an update-notification script to trigger replication between databases when data is changed. \n\n"}
{"prompt": "I wrote a desktop application and was using datetime.datetime.utcnow() for timestamping, however I've recently noticed that some people using the application get wildly different results than I do when we run the program at the same time.  Is there any way to get the UTC time locally without using urllib to fetch it from a website?\n", "completion": "Python depends on the underlying operating system to provide an accurate time-of-day clock.  If it isn't doing that, you don't have much choice other than to bypass the o/s.  There's a pure-Python implementation of an NTP client here.  A very simple-minded approach:\n\n>>> import ntplib,datetime\n>>> x = ntplib.NTPClient()\n>>> datetime.datetime.utcfromtimestamp(x.request('europe.pool.ntp.org').tx_time)\ndatetime.datetime(2009, 10, 21, 7, 1, 54, 716657)\n\n\nHowever, it would not be very nice to be continually hitting on other NTP servers out there.  A good net citizen would use the ntp client library to keep track of the offset between the o/s system clock and that obtained from the server and only periodically poll to adjust the time.\n"}
{"prompt": "I use popen to execute commands in a Python script, and I call it via cron.\nCron calls out this script but the behavior isn't the same if I call it by hand.\n\nSource:\n\nfrom subprocess import Popen, PIPE\n\npp = Popen('/usr/bin/which iptables', shell=True, stdout=PIPE)\n\ndata = ''\nfor ln in pp.stdout:\n    data = data+ln\n\nif data == '':\n    print 'ko'\nelse:\n    print 'ok : '+data\n\n\nBy hand:\n\n# python /home/user/test.py\n> : /sbin/iptables\n\n\nBy cron (in /tmp/err_cron):\n\n* * * * * /usr/bin/python /home/user/test.py >> /tmp/err_cron\nko\nko\nko\n\n\nWhy does cron not run this script normally?\n", "completion": "Normally when processes are run from cron, the PATH is set to a very restrictive value (the man page for my crontab says /usr/bin:/bin). You  may need to add:\n\n\nPATH=/usr/bin:/bin:/sbin\n\n\nto the top of your crontab file.\n"}
{"prompt": "I'm working on a Python library used by third-party developers to write extensions for our core application.\n\nI'd like to know if it's possible to modify the traceback when raising exceptions, so the last stack frame is the call to the library function in the developer's code, rather than the line in the library that raised the exception. There are also a few frames at the bottom of the stack containing references to functions used when first loading the code that I'd ideally like to remove too.\n\nThanks in advance for any advice!\n", "completion": "You can remove the top of the traceback easily with by raising with the tb_next element of the traceback:\n\nexcept:\n    ei = sys.exc_info()\n    raise ei[0], ei[1], ei[2].tb_next\n\n\ntb_next is a read_only attribute, so I don't know of a way to remove stuff from the bottom.  You might be able to screw with the properties mechanism to allow access to the property, but I don't know how to do that.\n"}
{"prompt": "I want to grab the color of a pixel with known coordinates on my Linux desktop.\n\nUntil now, I've used \"import -window SomeWindow -crop 1x1+X+Y /tmp/grab.jpg\"\nthen extracting the pixel value using Python and PIL.\n\nThis does the job, but since import grabs the whole window before cropping, it's very slow :(\n\nAre there any clever way to grab the color of only one pixel? I know both relative (window) and absolute coordinates. \n\nA Python or shell script would be preferable, but if you know some clever C/X11 functions, also please let me know :)\n", "completion": "This does the trick, but requires python-gtk:\n\nimport gtk.gdk\nimport sys\n\ndef PixelAt(x, y):\n    w = gtk.gdk.get_default_root_window()\n    sz = w.get_size()\n    pb = gtk.gdk.Pixbuf(gtk.gdk.COLORSPACE_RGB,False,8,sz[0],sz[1])\n    pb = pb.get_from_drawable(w,w.get_colormap(),0,0,0,0,sz[0],sz[1])\n    pixel_array = pb.get_pixels_array()\n    return pixel_array[y][x]\n\nprint PixelAt(int(sys.argv[1]), int(sys.argv[2]))\n\n\nOn Ubuntu 9.10, this also requires python-numpy or it segfaults the python interpreter on the get_pixels_array line. Ubuntu 10.04 it still has this requirement, or it causes an ImportError regarding numpy.core.multiarray.\n"}
{"prompt": "I tried these two methods:\n\nos.system(\"python test.py\")\n\nsubprocess.Popen(\"python test.py\", shell=True)\n\n\nBoth approaches need to wait until test.py finishes which blocks main process. I know \"nohup\" can do the job. Is there a Python way to launch test.py or any other shell scripts and leave it running in background?\n\nSuppose test.py is like this:\n\nfor i in range(0, 1000000):\n    print i\n\n\nBoth os.system() or subprocess.Popen() will block main program until 1000000 lines of output displayed. What I want is let test.py runs silently and display main program output only. Main program may quie while test.py is still running.\n", "completion": "subprocess.Popen([\"python\", \"test.py\"]) should work.\n\nNote that the job might still die when your main script exits. In this case, try subprocess.Popen([\"nohup\", \"python\", \"test.py\"])\n"}
